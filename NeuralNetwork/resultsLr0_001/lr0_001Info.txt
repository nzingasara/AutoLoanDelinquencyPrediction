Using TensorFlow backend.
filename:
mortgage_data_small_50_50_2.csv
df shape before dropping Nans:
729053
df shape after dropping Nans:
729053
unique labels:
[0. 1.]
unique labels count:
2
0.0: 368780
1.0: 360273
df after loading (729053 rows):
        loan_amnt state  annual_inc  int_rate  delinquent  credit_score  disbursal_timestamp
160789    99000.0    AZ     25030.0     6.500         0.0         749.0         1.009861e+09
316459   152000.0    AZ     23966.0     4.375         0.0         788.0         1.385874e+09
92840    397000.0    CA     60546.0     3.875         0.0         749.0         1.448946e+09
258376   240000.0    NJ     64169.0     6.375         0.0         620.0         1.196485e+09
141796   260000.0    NY     61153.0     6.125         0.0         761.0         1.133413e+09
<hash> df rows: 729053
hashing col state
new cols df rows: 729053
BEFORE concatting for col state
<hash> df rows: 729053
<hash> new cols rows: 729053
concatting for col state
<hash> df rows: 729053
df after hashing (729053 rows):
   loan_amnt  annual_inc  int_rate  delinquent  credit_score  ...  state_1  state_2  state_3  state_4  state_5
0    99000.0     25030.0     6.500         0.0         749.0  ...        0        1        0        0        0
1   152000.0     23966.0     4.375         0.0         788.0  ...        0        1        0        0        0
2   397000.0     60546.0     3.875         0.0         749.0  ...        0        0        1        0        0
3   240000.0     64169.0     6.375         0.0         620.0  ...        0        0        1        0        0
4   260000.0     61153.0     6.125         0.0         761.0  ...        0        1        0        0        0

[5 rows x 12 columns]
df_train_y in get_train_test_split:
0    0.0
1    0.0
2    0.0
3    0.0
4    0.0
Name: delinquent, dtype: float64
df train X:
   loan_amnt  annual_inc  int_rate  credit_score  disbursal_timestamp  state_0  state_1  state_2  state_3  state_4  state_5
0    99000.0     25030.0     6.500         749.0         1.009861e+09        0        0        1        0        0        0
1   152000.0     23966.0     4.375         788.0         1.385874e+09        0        0        1        0        0        0
2   397000.0     60546.0     3.875         749.0         1.448946e+09        0        0        0        1        0        0
3   240000.0     64169.0     6.375         620.0         1.196485e+09        0        0        0        1        0        0
4   260000.0     61153.0     6.125         761.0         1.133413e+09        0        0        1        0        0        0
df train X rows:
583242
df train y head:
0    0
1    0
2    0
3    0
4    0
Name: delinquent, dtype: int32
df test y head:
583242    0
583243    0
583244    0
583245    0
583246    0
Name: delinquent, dtype: int32
df_train_y label counts:
count0:
294998
count1:
288244
df_test_y label counts:
count0:
73782
count1:
72029
df test X:
        loan_amnt  annual_inc  int_rate  credit_score  disbursal_timestamp  state_0  state_1  state_2  state_3  state_4  state_5
583242   379000.0     63076.0     4.375         764.0         1.385874e+09        0        0        1        0        0        0
583243   105000.0     20843.0     4.040         708.0         1.417410e+09        1        0        0        0        0        0
583244   102000.0     14481.0     5.875         673.0         1.133413e+09        0        0        0        0        1        0
583245   128000.0     46664.0     6.125         794.0         1.196485e+09        0        0        0        0        1        0
583246   165000.0     31890.0     4.375         698.0         1.448946e+09        0        1        0        0        0        0
...           ...         ...       ...           ...                  ...      ...      ...      ...      ...      ...      ...
729048    91000.0     15869.0     5.250         691.0         1.322716e+09        0        0        0        0        0        1
729049   240000.0    167811.0     7.500         727.0         9.467028e+08        0        0        1        0        0        0
729050   193000.0     33270.0     6.500         770.0         1.136092e+09        0        0        1        0        0        0
729051    87000.0     36327.0     5.875         727.0         1.038719e+09        0        0        0        0        0        1
729052   251000.0     37119.0     5.875         627.0         1.196485e+09        0        1        0        0        0        0

[145811 rows x 11 columns]
df test X rows:
145811
df_train_X shape:
(583242, 11)
df_test_X shape:
(145811, 11)
df_train_y.values type:
<class 'numpy.ndarray'>
df_train_y.values[:10]:
[0 0 0 0 0 1 1 1 0 1]
train_y one hot:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]]
train_X AFTER scaling:
[[-0.85800292 -0.45741021  0.59116916 ... -0.54492263 -0.34356544
  -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396 ... -0.54492263 -0.34356544
  -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351 ...  1.83512292 -0.34356544
  -0.39504551]
 ...
 [-0.63498576  0.10087032 -0.11516507 ... -0.54492263 -0.34356544
   2.53135392]
 [-0.66407495 -0.50693106 -0.01426018 ... -0.54492263 -0.34356544
  -0.39504551]
 [ 1.41095433  1.78797372  0.28845449 ... -0.54492263 -0.34356544
  -0.39504551]]
test_X AFTER scaling:
[[ 1.85698866  0.5436899  -1.12421396 ... -0.54492263 -0.34356544
  -0.39504551]
 [-0.79982453 -0.56758227 -1.39463906 ... -0.54492263 -0.34356544
  -0.39504551]
 [-0.82891373 -0.73498486  0.08664471 ... -0.54492263  2.91065364
  -0.39504551]
 ...
 [ 0.05345854 -0.24059202  0.59116916 ... -0.54492263 -0.34356544
  -0.39504551]
 [-0.97435971 -0.16015352  0.08664471 ... -0.54492263 -0.34356544
   2.53135392]
 [ 0.61584965 -0.13931371  0.08664471 ... -0.54492263 -0.34356544
  -0.39504551]]
Before getting predictions, test X shape:
(145811, 11)
Before getting predictions, test y shape:
(145811, 2)
After getting predictions, test X shape:
(144811, 11)
After getting predictions, test y shape:
(144811, 2)
pred X shape:
(1000, 11)
pred y shape:
(1000, 2)
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
2019-11-25 23:06:17.582900: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-11-25 23:06:17.615335: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9e00cd2cc0 executing computations on platform Host. Devices:
2019-11-25 23:06:17.615351: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 5)                 60        
_________________________________________________________________
dense_2 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_4 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_5 (Dense)              (None, 2)                 12        
=================================================================
Total params: 162
Trainable params: 162
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.85800292 -0.45741021  0.59116916  0.03276942 -1.06987407 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396  0.12540677  1.48684084 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351  0.03276942  1.91570157 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.50918927  0.57244989  0.49026427 -0.27364646  0.1990837  -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.70311724  0.49309022  0.28845449  0.06127322 -0.22977703 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]]
train_y BEFORE fitting:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]]
test_X BEFORE fitting:
[[ 1.26550835  0.51166712 -0.01426018 -0.15725593 -0.44420739 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.32470101 -0.34831698  0.38935938 -0.06224326 -0.01534666 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 1.02309839  0.64562603 -0.41787973 -0.17150783 -0.8736556   2.63932675
  -0.52714789 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 2.2254518   2.05128678 -0.7205944   0.14440931  0.62853191 -0.3788845
   1.89700087 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 0.46070727  0.71288177  0.08664471  0.00901625 -0.64101334 -0.3788845
  -0.52714789 -0.48036317 -0.54492263 -0.34356544  2.53135392]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288244, 294998]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71536, 73275]))
Epoch 1/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5090 - accuracy: 0.7529 - precision: 0.7245 - recall: 0.7763
Epoch 2/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.5019 - accuracy: 0.7573 - precision: 0.7364 - recall: 0.7824
Epoch 3/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.5004 - accuracy: 0.7584 - precision: 0.7386 - recall: 0.7829
Epoch 4/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4999 - accuracy: 0.7585 - precision: 0.7392 - recall: 0.7838
Epoch 5/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4999 - accuracy: 0.7592 - precision: 0.7398 - recall: 0.7843
Epoch 6/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4996 - accuracy: 0.7584 - precision: 0.7397 - recall: 0.7852
Epoch 7/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4995 - accuracy: 0.7589 - precision: 0.7396 - recall: 0.7859
Epoch 8/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4993 - accuracy: 0.7597 - precision: 0.7395 - recall: 0.7868
Epoch 9/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4993 - accuracy: 0.7591 - precision: 0.7393 - recall: 0.7879
Epoch 10/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4992 - accuracy: 0.7591 - precision: 0.7393 - recall: 0.7881
Epoch 11/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4991 - accuracy: 0.7593 - precision: 0.7393 - recall: 0.7888
Epoch 12/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4992 - accuracy: 0.7589 - precision: 0.7393 - recall: 0.7890
Epoch 13/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4992 - accuracy: 0.7592 - precision: 0.7393 - recall: 0.7891
Epoch 14/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4991 - accuracy: 0.7596 - precision: 0.7393 - recall: 0.7897
Epoch 15/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4989 - accuracy: 0.7593 - precision: 0.7393 - recall: 0.7898
Epoch 16/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4990 - accuracy: 0.7597 - precision: 0.7393 - recall: 0.7899
Epoch 17/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4989 - accuracy: 0.7592 - precision: 0.7393 - recall: 0.7900
Epoch 18/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4987 - accuracy: 0.7594 - precision: 0.7393 - recall: 0.7907
Epoch 19/50
583242/583242 [==============================] - 18s 32us/step - loss: 0.4989 - accuracy: 0.7595 - precision: 0.7392 - recall: 0.7909
Epoch 20/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4987 - accuracy: 0.7594 - precision: 0.7390 - recall: 0.7912
Epoch 21/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4986 - accuracy: 0.7598 - precision: 0.7390 - recall: 0.7917
Epoch 22/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4987 - accuracy: 0.7595 - precision: 0.7390 - recall: 0.7919
Epoch 23/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4987 - accuracy: 0.7597 - precision: 0.7389 - recall: 0.7920
Epoch 24/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7597 - precision: 0.7388 - recall: 0.7922
Epoch 25/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4987 - accuracy: 0.7597 - precision: 0.7386 - recall: 0.7927
Epoch 26/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7598 - precision: 0.7386 - recall: 0.7927
Epoch 27/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4986 - accuracy: 0.7599 - precision: 0.7386 - recall: 0.7929
Epoch 28/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7598 - precision: 0.7386 - recall: 0.7930
Epoch 29/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7599 - precision: 0.7385 - recall: 0.7930
Epoch 30/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7600 - precision: 0.7385 - recall: 0.7930
Epoch 31/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7932
Epoch 32/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4983 - accuracy: 0.7596 - precision: 0.7385 - recall: 0.7932
Epoch 33/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7937
Epoch 34/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7599 - precision: 0.7385 - recall: 0.7937
Epoch 35/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4984 - accuracy: 0.7598 - precision: 0.7385 - recall: 0.7937
Epoch 36/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7598 - precision: 0.7385 - recall: 0.7939
Epoch 37/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7598 - precision: 0.7385 - recall: 0.7939
Epoch 38/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7601 - precision: 0.7385 - recall: 0.7939
Epoch 39/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7598 - precision: 0.7385 - recall: 0.7939
Epoch 40/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7939
Epoch 41/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4983 - accuracy: 0.7603 - precision: 0.7384 - recall: 0.7940
Epoch 42/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7597 - precision: 0.7384 - recall: 0.7940
Epoch 43/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4983 - accuracy: 0.7599 - precision: 0.7384 - recall: 0.7940
Epoch 44/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4984 - accuracy: 0.7599 - precision: 0.7384 - recall: 0.7942
Epoch 45/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4985 - accuracy: 0.7599 - precision: 0.7384 - recall: 0.7942
Epoch 46/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4983 - accuracy: 0.7597 - precision: 0.7384 - recall: 0.7943
Epoch 47/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.4983 - accuracy: 0.7599 - precision: 0.7383 - recall: 0.7946
Epoch 48/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4984 - accuracy: 0.7596 - precision: 0.7383 - recall: 0.7947
Epoch 49/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4984 - accuracy: 0.7603 - precision: 0.7383 - recall: 0.7947
Epoch 50/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4986 - accuracy: 0.7598 - precision: 0.7383 - recall: 0.7949
history.history:
{'loss': [0.5090262290586238, 0.5018811665449845, 0.5004227558201519, 0.4999210867743247, 0.49986147227525646, 0.4995933521465993, 0.4994734085878585, 0.4993105358606022, 0.4992780534794929, 0.499244131404142, 0.4990623447542525, 0.4991817786841953, 0.4991841951084424, 0.4991186167039778, 0.49887660354616303, 0.4989880088839929, 0.4988741808438486, 0.49872870600952274, 0.49887397198661515, 0.49868032526103867, 0.49858532060219146, 0.4986724432542147, 0.4986989252534286, 0.4984744770174083, 0.498683402138642, 0.4984267397480346, 0.49862874245602, 0.49842101333420286, 0.4984995148107607, 0.4984665627012046, 0.4983792380460907, 0.49831532162740066, 0.4984422055366738, 0.49848223434381017, 0.4984032823958921, 0.4984436224794107, 0.49849528707078833, 0.49847323689094125, 0.4984032571399361, 0.4984236286753421, 0.49830488078684654, 0.49845660919715884, 0.4982529727231233, 0.4984396773252307, 0.4984695486828539, 0.4982985099562618, 0.49833329396635206, 0.4984296753647289, 0.4984438974312073, 0.4985686707261839], 'accuracy': [0.7528573, 0.75731856, 0.7584416, 0.7584536, 0.7591531, 0.75839186, 0.7588771, 0.75967264, 0.7591137, 0.75905025, 0.75925946, 0.7588994, 0.759184, 0.75963837, 0.7593332, 0.7596744, 0.75921315, 0.75940174, 0.75948405, 0.7594309, 0.7598167, 0.7594875, 0.7597464, 0.75968295, 0.7596761, 0.75982696, 0.75991267, 0.7598167, 0.75989896, 0.76002926, 0.7597104, 0.7596092, 0.7596761, 0.759887, 0.7597755, 0.7597961, 0.7598321, 0.76013047, 0.7598115, 0.75970864, 0.7602899, 0.759707, 0.7599144, 0.7599093, 0.7599487, 0.7597498, 0.7599195, 0.7596092, 0.76029503, 0.75980467], 'precision': [0.7245166, 0.73642474, 0.7385854, 0.7392455, 0.7397728, 0.7397241, 0.7396359, 0.73953694, 0.739335, 0.7393266, 0.73931706, 0.7392781, 0.7393124, 0.73927057, 0.7392754, 0.7392761, 0.73929393, 0.73925674, 0.7392015, 0.7390344, 0.73902154, 0.7389758, 0.73890483, 0.7388025, 0.73857737, 0.73856336, 0.7385632, 0.73856056, 0.7385281, 0.7385227, 0.7385226, 0.7385216, 0.73851913, 0.73850846, 0.73850775, 0.73850524, 0.73850507, 0.73850447, 0.73850554, 0.73850465, 0.7383555, 0.7383581, 0.7383546, 0.73835444, 0.7383536, 0.7383512, 0.7383414, 0.73833823, 0.73833716, 0.7383001], 'recall': [0.77625525, 0.7823826, 0.7829392, 0.7838074, 0.7843351, 0.7851932, 0.78589255, 0.78683746, 0.78791106, 0.78811264, 0.78875965, 0.78898346, 0.78906405, 0.7896952, 0.78976494, 0.78994256, 0.7900362, 0.7907204, 0.7909469, 0.79121923, 0.7917109, 0.79193157, 0.79199356, 0.79219615, 0.7926744, 0.7927438, 0.7928999, 0.7929523, 0.792969, 0.792992, 0.7931567, 0.7931902, 0.79366386, 0.7936728, 0.7937223, 0.7938753, 0.79388046, 0.79392797, 0.7939369, 0.7939454, 0.7939591, 0.7939978, 0.79401237, 0.7941558, 0.7941678, 0.79434097, 0.79464567, 0.79465896, 0.7946989, 0.7948519]}
144811/144811 [==============================] - 3s 19us/step
results evaluated:
[0.5026206840119477, 0.7587130665779114, 0.738311231136322, 0.7948489189147949]
predictions:
[[0.8485268  0.15147321]
 [0.74918395 0.25081608]
 [0.25827947 0.74172056]
 ...
 [0.33406797 0.66593206]
 [0.17673571 0.8232643 ]
 [0.31161478 0.6883853 ]]
pred_y:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 10)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_8 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_9 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_10 (Dense)             (None, 2)                 22        
=================================================================
Total params: 472
Trainable params: 472
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.85800292 -0.45741021  0.59116916  0.03276942 -1.06987407 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396  0.12540677  1.48684084 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351  0.03276942  1.91570157 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.50918927  0.57244989  0.49026427 -0.27364646  0.1990837  -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.70311724  0.49309022  0.28845449  0.06127322 -0.22977703 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]]
train_y BEFORE fitting:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]]
test_X BEFORE fitting:
[[ 1.26550835  0.51166712 -0.01426018 -0.15725593 -0.44420739 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.32470101 -0.34831698  0.38935938 -0.06224326 -0.01534666 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 1.02309839  0.64562603 -0.41787973 -0.17150783 -0.8736556   2.63932675
  -0.52714789 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 2.2254518   2.05128678 -0.7205944   0.14440931  0.62853191 -0.3788845
   1.89700087 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 0.46070727  0.71288177  0.08664471  0.00901625 -0.64101334 -0.3788845
  -0.52714789 -0.48036317 -0.54492263 -0.34356544  2.53135392]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288244, 294998]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71536, 73275]))
Epoch 1/50
583242/583242 [==============================] - 24s 40us/step - loss: 0.5076 - accuracy: 0.7537 - precision_1: 0.7292 - recall_1: 0.7710
Epoch 2/50
583242/583242 [==============================] - 25s 42us/step - loss: 0.4987 - accuracy: 0.7589 - precision_1: 0.7354 - recall_1: 0.7880
Epoch 3/50
583242/583242 [==============================] - 29s 50us/step - loss: 0.4974 - accuracy: 0.7601 - precision_1: 0.7361 - recall_1: 0.7923
Epoch 4/50
583242/583242 [==============================] - 25s 42us/step - loss: 0.4965 - accuracy: 0.7603 - precision_1: 0.7367 - recall_1: 0.7939
Epoch 5/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4963 - accuracy: 0.7603 - precision_1: 0.7372 - recall_1: 0.7946
Epoch 6/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4959 - accuracy: 0.7609 - precision_1: 0.7378 - recall_1: 0.7947
Epoch 7/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4957 - accuracy: 0.7616 - precision_1: 0.7382 - recall_1: 0.7948
Epoch 8/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4956 - accuracy: 0.7614 - precision_1: 0.7383 - recall_1: 0.7949
Epoch 9/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4955 - accuracy: 0.7615 - precision_1: 0.7390 - recall_1: 0.7948
Epoch 10/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4953 - accuracy: 0.7613 - precision_1: 0.7392 - recall_1: 0.7945
Epoch 11/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4953 - accuracy: 0.7614 - precision_1: 0.7393 - recall_1: 0.7946
Epoch 12/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4951 - accuracy: 0.7618 - precision_1: 0.7395 - recall_1: 0.7947
Epoch 13/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4952 - accuracy: 0.7619 - precision_1: 0.7399 - recall_1: 0.7947
Epoch 14/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4948 - accuracy: 0.7617 - precision_1: 0.7400 - recall_1: 0.7948
Epoch 15/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4950 - accuracy: 0.7618 - precision_1: 0.7402 - recall_1: 0.7947
Epoch 16/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4949 - accuracy: 0.7618 - precision_1: 0.7402 - recall_1: 0.7947
Epoch 17/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4948 - accuracy: 0.7618 - precision_1: 0.7402 - recall_1: 0.7947
Epoch 18/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4949 - accuracy: 0.7619 - precision_1: 0.7402 - recall_1: 0.7949
Epoch 19/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4949 - accuracy: 0.7618 - precision_1: 0.7402 - recall_1: 0.7949
Epoch 20/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4948 - accuracy: 0.7618 - precision_1: 0.7403 - recall_1: 0.7949
Epoch 21/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4947 - accuracy: 0.7620 - precision_1: 0.7403 - recall_1: 0.7949
Epoch 22/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4947 - accuracy: 0.7620 - precision_1: 0.7403 - recall_1: 0.7949
Epoch 23/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.4947 - accuracy: 0.7618 - precision_1: 0.7403 - recall_1: 0.7950
Epoch 24/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4946 - accuracy: 0.7620 - precision_1: 0.7405 - recall_1: 0.7950
Epoch 25/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4947 - accuracy: 0.7615 - precision_1: 0.7406 - recall_1: 0.7949
Epoch 26/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4947 - accuracy: 0.7618 - precision_1: 0.7407 - recall_1: 0.7949
Epoch 27/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4946 - accuracy: 0.7617 - precision_1: 0.7409 - recall_1: 0.7949
Epoch 28/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4947 - accuracy: 0.7618 - precision_1: 0.7409 - recall_1: 0.7949
Epoch 29/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.4948 - accuracy: 0.7620 - precision_1: 0.7410 - recall_1: 0.7949
Epoch 30/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4944 - accuracy: 0.7621 - precision_1: 0.7410 - recall_1: 0.7949
Epoch 31/50
583242/583242 [==============================] - 37s 64us/step - loss: 0.4944 - accuracy: 0.7620 - precision_1: 0.7410 - recall_1: 0.7948
Epoch 32/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4943 - accuracy: 0.7621 - precision_1: 0.7411 - recall_1: 0.7947
Epoch 33/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4942 - accuracy: 0.7621 - precision_1: 0.7412 - recall_1: 0.7947
Epoch 34/50
583242/583242 [==============================] - 34s 58us/step - loss: 0.4946 - accuracy: 0.7620 - precision_1: 0.7412 - recall_1: 0.7947
Epoch 35/50
583242/583242 [==============================] - 15s 27us/step - loss: 0.4942 - accuracy: 0.7624 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 36/50
583242/583242 [==============================] - 15s 27us/step - loss: 0.4944 - accuracy: 0.7624 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 37/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.4942 - accuracy: 0.7621 - precision_1: 0.7412 - recall_1: 0.7948
Epoch 38/50
583242/583242 [==============================] - 17s 28us/step - loss: 0.4941 - accuracy: 0.7626 - precision_1: 0.7412 - recall_1: 0.7947
Epoch 39/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4941 - accuracy: 0.7622 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 40/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4942 - accuracy: 0.7621 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 41/50
583242/583242 [==============================] - 27s 47us/step - loss: 0.4941 - accuracy: 0.7624 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 42/50
583242/583242 [==============================] - 31s 53us/step - loss: 0.4943 - accuracy: 0.7618 - precision_1: 0.7412 - recall_1: 0.7949
Epoch 43/50
583242/583242 [==============================] - 32s 54us/step - loss: 0.4943 - accuracy: 0.7622 - precision_1: 0.7413 - recall_1: 0.7947
Epoch 44/50
583242/583242 [==============================] - 32s 55us/step - loss: 0.4942 - accuracy: 0.7623 - precision_1: 0.7413 - recall_1: 0.7949
Epoch 45/50
583242/583242 [==============================] - 31s 53us/step - loss: 0.4943 - accuracy: 0.7622 - precision_1: 0.7413 - recall_1: 0.7949
Epoch 46/50
583242/583242 [==============================] - 31s 53us/step - loss: 0.4942 - accuracy: 0.7621 - precision_1: 0.7413 - recall_1: 0.7949
Epoch 47/50
583242/583242 [==============================] - 30s 52us/step - loss: 0.4944 - accuracy: 0.7620 - precision_1: 0.7413 - recall_1: 0.7948
Epoch 48/50
583242/583242 [==============================] - 30s 51us/step - loss: 0.4941 - accuracy: 0.7622 - precision_1: 0.7413 - recall_1: 0.7949
Epoch 49/50
583242/583242 [==============================] - 31s 53us/step - loss: 0.4945 - accuracy: 0.7622 - precision_1: 0.7413 - recall_1: 0.7949
Epoch 50/50
583242/583242 [==============================] - 30s 51us/step - loss: 0.4943 - accuracy: 0.7619 - precision_1: 0.7413 - recall_1: 0.7949
history.history:
{'loss': [0.5076170854942419, 0.49872388913069354, 0.4974216871023179, 0.49650814715930386, 0.4963257178350928, 0.4959124138057327, 0.4957435432933659, 0.4955763028449189, 0.4954735604658558, 0.4952693832771257, 0.4953197148869966, 0.4951224494106353, 0.4951951737852646, 0.49482446587341905, 0.495012431322007, 0.49488960319460096, 0.49476628414536555, 0.49486117533462515, 0.4949125332147398, 0.4948045714852321, 0.4946709450815306, 0.49466461901252745, 0.4946862701588165, 0.49456334096170074, 0.4947266754688506, 0.49469460912037855, 0.49460092152968266, 0.4947119696117373, 0.49476803736791586, 0.49444018711499543, 0.4943790416970712, 0.49434072973874355, 0.4941722357157335, 0.49463480634714857, 0.4942409765051969, 0.4944481830974785, 0.4941917389570874, 0.49414284275620085, 0.49413173776087566, 0.49419651476849397, 0.49412756358866183, 0.4942565228637122, 0.494257705706494, 0.4942039735287079, 0.4942844152917799, 0.49422248783056827, 0.4944262861584975, 0.494067684272514, 0.49449926362134344, 0.49427351418678767], 'accuracy': [0.7536717, 0.7589491, 0.76006013, 0.7603396, 0.76025563, 0.7609106, 0.7615604, 0.7614301, 0.76150894, 0.7613444, 0.76142496, 0.761797, 0.76193243, 0.7616821, 0.7617867, 0.76177984, 0.761785, 0.76187414, 0.761821, 0.7618364, 0.7620199, 0.76201475, 0.7617507, 0.76196504, 0.7614935, 0.761785, 0.76166666, 0.7617816, 0.76200104, 0.76211935, 0.761977, 0.76214504, 0.762121, 0.7619805, 0.7624194, 0.7623525, 0.7621468, 0.76257885, 0.76221704, 0.76209533, 0.7624314, 0.7618278, 0.7621708, 0.76227707, 0.7621982, 0.76205415, 0.7620439, 0.76217765, 0.762193, 0.76187074], 'precision_1': [0.72924536, 0.73538965, 0.73611724, 0.7367104, 0.73720366, 0.737798, 0.7381979, 0.7383207, 0.73899156, 0.73923105, 0.73931104, 0.73948616, 0.73993003, 0.73995316, 0.74015903, 0.7401764, 0.7402344, 0.74023396, 0.74024916, 0.74029875, 0.74034494, 0.7403085, 0.7403141, 0.7404572, 0.74061763, 0.7407138, 0.7409149, 0.7409188, 0.7409515, 0.7409704, 0.74100184, 0.7411403, 0.7411517, 0.74119025, 0.7411545, 0.74119234, 0.7412071, 0.7412075, 0.7412137, 0.7412144, 0.7412155, 0.7412257, 0.7412665, 0.74126744, 0.74126995, 0.74127173, 0.74128115, 0.7412836, 0.7412838, 0.74128467], 'recall_1': [0.77099115, 0.78802174, 0.79231083, 0.7938574, 0.7946053, 0.7947441, 0.794803, 0.79487884, 0.7948224, 0.7944853, 0.7946453, 0.79465026, 0.7947106, 0.79484797, 0.79469854, 0.79470867, 0.794672, 0.79486454, 0.79488736, 0.7948581, 0.79487044, 0.79491735, 0.7949783, 0.79497755, 0.79493123, 0.7948658, 0.79490566, 0.7949041, 0.7948701, 0.79486793, 0.7947975, 0.79470205, 0.7947009, 0.794701, 0.79485327, 0.7948668, 0.7948252, 0.79471546, 0.79485244, 0.79485637, 0.7948613, 0.79486185, 0.79471445, 0.7948526, 0.794856, 0.79485285, 0.79484755, 0.79485303, 0.79485637, 0.7948563]}
144811/144811 [==============================] - 4s 27us/step
results evaluated:
[0.49619093230398253, 0.7621589303016663, 0.741454005241394, 0.7946953773498535]
predictions:
[[0.8104868  0.18951318]
 [0.79694885 0.2030511 ]
 [0.357315   0.642685  ]
 ...
 [0.4269218  0.57307816]
 [0.2372295  0.7627705 ]
 [0.37755364 0.6224463 ]]
pred_y:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_12 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_13 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_14 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_15 (Dense)             (None, 2)                 42        
=================================================================
Total params: 1,542
Trainable params: 1,542
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.85800292 -0.45741021  0.59116916  0.03276942 -1.06987407 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396  0.12540677  1.48684084 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351  0.03276942  1.91570157 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.50918927  0.57244989  0.49026427 -0.27364646  0.1990837  -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.70311724  0.49309022  0.28845449  0.06127322 -0.22977703 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]]
train_y BEFORE fitting:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]]
test_X BEFORE fitting:
[[ 1.26550835  0.51166712 -0.01426018 -0.15725593 -0.44420739 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.32470101 -0.34831698  0.38935938 -0.06224326 -0.01534666 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 1.02309839  0.64562603 -0.41787973 -0.17150783 -0.8736556   2.63932675
  -0.52714789 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 2.2254518   2.05128678 -0.7205944   0.14440931  0.62853191 -0.3788845
   1.89700087 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 0.46070727  0.71288177  0.08664471  0.00901625 -0.64101334 -0.3788845
  -0.52714789 -0.48036317 -0.54492263 -0.34356544  2.53135392]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288244, 294998]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71536, 73275]))
Epoch 1/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.5063 - accuracy: 0.7557 - precision_2: 0.7295 - recall_2: 0.7824
Epoch 2/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5001 - accuracy: 0.7600 - precision_2: 0.7349 - recall_2: 0.7946
Epoch 3/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4981 - accuracy: 0.7608 - precision_2: 0.7352 - recall_2: 0.7984
Epoch 4/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.4978 - accuracy: 0.7610 - precision_2: 0.7363 - recall_2: 0.7989
Epoch 5/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4969 - accuracy: 0.7611 - precision_2: 0.7365 - recall_2: 0.7995
Epoch 6/50
583242/583242 [==============================] - 25s 43us/step - loss: 0.4976 - accuracy: 0.7612 - precision_2: 0.7364 - recall_2: 0.8005
Epoch 7/50
583242/583242 [==============================] - 29s 50us/step - loss: 0.4963 - accuracy: 0.7616 - precision_2: 0.7365 - recall_2: 0.8008
Epoch 8/50
583242/583242 [==============================] - 32s 55us/step - loss: 0.4964 - accuracy: 0.7612 - precision_2: 0.7371 - recall_2: 0.8008
Epoch 9/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4965 - accuracy: 0.7614 - precision_2: 0.7372 - recall_2: 0.8008
Epoch 10/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4961 - accuracy: 0.7615 - precision_2: 0.7373 - recall_2: 0.8008
Epoch 11/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4960 - accuracy: 0.7615 - precision_2: 0.7373 - recall_2: 0.8008
Epoch 12/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4958 - accuracy: 0.7616 - precision_2: 0.7375 - recall_2: 0.8008
Epoch 13/50
583242/583242 [==============================] - 32s 54us/step - loss: 0.4956 - accuracy: 0.7613 - precision_2: 0.7375 - recall_2: 0.8008
Epoch 14/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4958 - accuracy: 0.7614 - precision_2: 0.7375 - recall_2: 0.8008
Epoch 15/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4951 - accuracy: 0.7618 - precision_2: 0.7380 - recall_2: 0.8008
Epoch 16/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4952 - accuracy: 0.7615 - precision_2: 0.7380 - recall_2: 0.8007
Epoch 17/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4951 - accuracy: 0.7618 - precision_2: 0.7380 - recall_2: 0.8007
Epoch 18/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4949 - accuracy: 0.7616 - precision_2: 0.7381 - recall_2: 0.8007
Epoch 19/50
583242/583242 [==============================] - 25s 43us/step - loss: 0.4947 - accuracy: 0.7620 - precision_2: 0.7380 - recall_2: 0.8008
Epoch 20/50
583242/583242 [==============================] - 30s 52us/step - loss: 0.4948 - accuracy: 0.7615 - precision_2: 0.7382 - recall_2: 0.8007
Epoch 21/50
583242/583242 [==============================] - 34s 59us/step - loss: 0.4951 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8006
Epoch 22/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4948 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 23/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4945 - accuracy: 0.7621 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 24/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4950 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8006
Epoch 25/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4948 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8006
Epoch 26/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.4944 - accuracy: 0.7622 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 27/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4953 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 28/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4948 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 29/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4953 - accuracy: 0.7617 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 30/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4945 - accuracy: 0.7617 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 31/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4949 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 32/50
583242/583242 [==============================] - 24s 41us/step - loss: 0.4947 - accuracy: 0.7615 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 33/50
583242/583242 [==============================] - 30s 52us/step - loss: 0.4946 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 34/50
583242/583242 [==============================] - 35s 61us/step - loss: 0.4944 - accuracy: 0.7623 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 35/50
583242/583242 [==============================] - 36s 63us/step - loss: 0.4951 - accuracy: 0.7617 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 36/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4947 - accuracy: 0.7617 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 37/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4944 - accuracy: 0.7621 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 38/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4948 - accuracy: 0.7617 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 39/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.4947 - accuracy: 0.7617 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 40/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4946 - accuracy: 0.7621 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 41/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4945 - accuracy: 0.7619 - precision_2: 0.7384 - recall_2: 0.8008
Epoch 42/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4949 - accuracy: 0.7611 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 43/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4964 - accuracy: 0.7616 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4947 - accuracy: 0.7619 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 45/50
583242/583242 [==============================] - 26s 45us/step - loss: 0.4947 - accuracy: 0.7622 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 46/50
583242/583242 [==============================] - 33s 57us/step - loss: 0.4948 - accuracy: 0.7620 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 47/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4944 - accuracy: 0.7621 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 48/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4947 - accuracy: 0.7616 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 49/50
583242/583242 [==============================] - 35s 59us/step - loss: 0.4944 - accuracy: 0.7618 - precision_2: 0.7385 - recall_2: 0.8008
Epoch 50/50
583242/583242 [==============================] - 37s 63us/step - loss: 0.4944 - accuracy: 0.7621 - precision_2: 0.7385 - recall_2: 0.8008
history.history:
{'loss': [0.5062611277260126, 0.5000599952169638, 0.4981230377211151, 0.4978247805456196, 0.49689119372990076, 0.4976391102366053, 0.4962575680563992, 0.4964498100950722, 0.49649748949518113, 0.4961139870193928, 0.49601471841427885, 0.4958003736015135, 0.4955918605182015, 0.49577606601447105, 0.49513849861943054, 0.4951906394838478, 0.4951162112280341, 0.49489139269283633, 0.4947422399826839, 0.4948141549306683, 0.49508820828770916, 0.49479202180076337, 0.4944626492695052, 0.49500382912964225, 0.49477596035100413, 0.4944090506211379, 0.4952784837679876, 0.49480153957033096, 0.4953126945888559, 0.49453439462444115, 0.49485991207347185, 0.49467304345235924, 0.4946181824780287, 0.494388840650106, 0.4950518788453821, 0.4946736889744577, 0.4943825329502136, 0.49480431423081594, 0.4947061890696053, 0.4945821822498789, 0.4944743253319651, 0.49485046436518154, 0.49641064437416177, 0.4946983464445932, 0.4946943586334434, 0.49477916828888346, 0.49442451512451674, 0.49472004633001077, 0.49436919604237745, 0.4944499856443281], 'accuracy': [0.7557343, 0.75998986, 0.76083857, 0.7610323, 0.76114374, 0.76122946, 0.76155525, 0.76120716, 0.7614335, 0.7614798, 0.76154155, 0.76156384, 0.76128435, 0.76142496, 0.7618176, 0.7615175, 0.76180387, 0.7616478, 0.76203704, 0.76153296, 0.7618244, 0.7618673, 0.7621262, 0.7618673, 0.76186216, 0.7621896, 0.7618467, 0.7617987, 0.76167357, 0.76171124, 0.7619359, 0.76149863, 0.7617627, 0.76226336, 0.7617164, 0.76169753, 0.7621279, 0.76170784, 0.76167357, 0.7621399, 0.76188445, 0.7611386, 0.76160496, 0.76187414, 0.7621536, 0.76204216, 0.76213473, 0.7616153, 0.7617644, 0.7621056], 'precision_2': [0.72954065, 0.7348863, 0.73524684, 0.7362502, 0.7364953, 0.7363991, 0.73645395, 0.73708516, 0.737232, 0.73728955, 0.73730946, 0.7374746, 0.73745024, 0.7374522, 0.73795134, 0.7380203, 0.7380276, 0.7380513, 0.7380407, 0.7382144, 0.73826617, 0.738267, 0.738281, 0.73829526, 0.7382987, 0.7382955, 0.73829955, 0.7382955, 0.73828596, 0.73829514, 0.73828554, 0.73829544, 0.7382996, 0.7383381, 0.7383509, 0.73835117, 0.73835516, 0.73835754, 0.7383675, 0.7383504, 0.73843133, 0.738505, 0.738504, 0.73850405, 0.7385057, 0.73851585, 0.7385217, 0.7385183, 0.7385084, 0.7385189], 'recall_2': [0.78239477, 0.7945555, 0.79838824, 0.7989269, 0.79951304, 0.80054253, 0.8007916, 0.8008022, 0.8007736, 0.8007802, 0.80077225, 0.8007894, 0.8007641, 0.8008477, 0.8007849, 0.80066735, 0.8006934, 0.8007323, 0.8007819, 0.80073017, 0.80056626, 0.80071247, 0.8006522, 0.8005623, 0.800609, 0.8007259, 0.8007633, 0.8007816, 0.80083203, 0.80083597, 0.8008489, 0.8008465, 0.80083305, 0.8007974, 0.8007952, 0.80079776, 0.8007841, 0.80079436, 0.80079377, 0.80084944, 0.8008423, 0.8008115, 0.80083686, 0.8008382, 0.8008329, 0.80079824, 0.80078524, 0.80083585, 0.8008489, 0.8008366]}
144811/144811 [==============================] - 5s 31us/step
results evaluated:
[0.4939631501951042, 0.7633329033851624, 0.7386957406997681, 0.8008362054824829]
predictions:
[[0.91398466 0.08601532]
 [0.835979   0.16402103]
 [0.22556539 0.7744346 ]
 ...
 [0.36978304 0.6302169 ]
 [0.11556955 0.88443047]
 [0.24504876 0.7549513 ]]
pred_y:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_17 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_18 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_19 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_20 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_21 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_22 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_23 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_24 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_25 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_26 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_27 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_28 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_29 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_30 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_31 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_32 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_33 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_34 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_35 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_36 (Dense)             (None, 2)                 42        
=================================================================
Total params: 8,262
Trainable params: 8,262
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.85800292 -0.45741021  0.59116916  0.03276942 -1.06987407 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396  0.12540677  1.48684084 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351  0.03276942  1.91570157 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.50918927  0.57244989  0.49026427 -0.27364646  0.1990837  -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.70311724  0.49309022  0.28845449  0.06127322 -0.22977703 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]]
train_y BEFORE fitting:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]]
test_X BEFORE fitting:
[[ 1.26550835  0.51166712 -0.01426018 -0.15725593 -0.44420739 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.32470101 -0.34831698  0.38935938 -0.06224326 -0.01534666 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 1.02309839  0.64562603 -0.41787973 -0.17150783 -0.8736556   2.63932675
  -0.52714789 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 2.2254518   2.05128678 -0.7205944   0.14440931  0.62853191 -0.3788845
   1.89700087 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 0.46070727  0.71288177  0.08664471  0.00901625 -0.64101334 -0.3788845
  -0.52714789 -0.48036317 -0.54492263 -0.34356544  2.53135392]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288244, 294998]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71536, 73275]))
Epoch 1/50
583242/583242 [==============================] - 74s 127us/step - loss: 3.6718 - accuracy: 0.6788 - precision_3: 0.6443 - recall_3: 0.7845
Epoch 2/50
583242/583242 [==============================] - 36s 62us/step - loss: 31.8835 - accuracy: 0.6512 - precision_3: 0.6606 - recall_3: 0.7285
Epoch 3/50
583242/583242 [==============================] - 42s 71us/step - loss: 970.6843 - accuracy: 0.5551 - precision_3: 0.6363 - recall_3: 0.6595
Epoch 4/50
583242/583242 [==============================] - 61s 104us/step - loss: 19114.2972 - accuracy: 0.5197 - precision_3: 0.6071 - recall_3: 0.6223
Epoch 5/50
583242/583242 [==============================] - 82s 141us/step - loss: 1181.5971 - accuracy: 0.5086 - precision_3: 0.5849 - recall_3: 0.5967
Epoch 6/50
583242/583242 [==============================] - 83s 142us/step - loss: 8441.1247 - accuracy: 0.5127 - precision_3: 0.5702 - recall_3: 0.5789
Epoch 7/50
583242/583242 [==============================] - 65s 112us/step - loss: 36950.4952 - accuracy: 0.5130 - precision_3: 0.5611 - recall_3: 0.5661
Epoch 8/50
583242/583242 [==============================] - 36s 62us/step - loss: 116483.5832 - accuracy: 0.5308 - precision_3: 0.5554 - recall_3: 0.5538
Epoch 9/50
583242/583242 [==============================] - 46s 78us/step - loss: 66430.4569 - accuracy: 0.5421 - precision_3: 0.5532 - recall_3: 0.5481
Epoch 10/50
583242/583242 [==============================] - 45s 77us/step - loss: 40108.3860 - accuracy: 0.5425 - precision_3: 0.5513 - recall_3: 0.5481
Epoch 11/50
583242/583242 [==============================] - 40s 69us/step - loss: 727337.5998 - accuracy: 0.5216 - precision_3: 0.5491 - recall_3: 0.5432
Epoch 12/50
583242/583242 [==============================] - 39s 67us/step - loss: 1282.8581 - accuracy: 0.5722 - precision_3: 0.5484 - recall_3: 0.5449
Epoch 13/50
583242/583242 [==============================] - 39s 66us/step - loss: 103536.1553 - accuracy: 0.5164 - precision_3: 0.5472 - recall_3: 0.5478
Epoch 14/50
583242/583242 [==============================] - 38s 66us/step - loss: 1276537.5464 - accuracy: 0.5383 - precision_3: 0.5453 - recall_3: 0.5466
Epoch 15/50
583242/583242 [==============================] - 38s 64us/step - loss: 20254.7469 - accuracy: 0.5217 - precision_3: 0.5438 - recall_3: 0.5447
Epoch 16/50
583242/583242 [==============================] - 37s 63us/step - loss: 50449.2668 - accuracy: 0.5219 - precision_3: 0.5422 - recall_3: 0.5436
Epoch 17/50
583242/583242 [==============================] - 39s 66us/step - loss: 28359.7490 - accuracy: 0.5331 - precision_3: 0.5407 - recall_3: 0.5425
Epoch 18/50
583242/583242 [==============================] - 38s 65us/step - loss: 12998870.5452 - accuracy: 0.5296 - precision_3: 0.5399 - recall_3: 0.5405
Epoch 19/50
583242/583242 [==============================] - 37s 64us/step - loss: 100591711.4419 - accuracy: 0.5282 - precision_3: 0.5395 - recall_3: 0.5371
Epoch 20/50
583242/583242 [==============================] - 36s 62us/step - loss: 251706.7473 - accuracy: 0.5357 - precision_3: 0.5391 - recall_3: 0.5346
Epoch 21/50
583242/583242 [==============================] - 36s 63us/step - loss: 18993536904.6273 - accuracy: 0.5367 - precision_3: 0.5385 - recall_3: 0.5319
Epoch 22/50
583242/583242 [==============================] - 36s 62us/step - loss: 2527883.2410 - accuracy: 0.5480 - precision_3: 0.5386 - recall_3: 0.5306
Epoch 23/50
583242/583242 [==============================] - 36s 62us/step - loss: 96780325.8956 - accuracy: 0.5346 - precision_3: 0.5385 - recall_3: 0.5309
Epoch 24/50
583242/583242 [==============================] - 36s 62us/step - loss: 110343198327.4809 - accuracy: 0.5236 - precision_3: 0.5380 - recall_3: 0.5307
Epoch 25/50
583242/583242 [==============================] - 37s 63us/step - loss: 4751495.8074 - accuracy: 0.5003 - precision_3: 0.5366 - recall_3: 0.5301
Epoch 26/50
583242/583242 [==============================] - 37s 63us/step - loss: 115199929.4821 - accuracy: 0.5117 - precision_3: 0.5352 - recall_3: 0.5286
Epoch 27/50
583242/583242 [==============================] - 38s 65us/step - loss: 3632382.2934 - accuracy: 0.5237 - precision_3: 0.5343 - recall_3: 0.5263
Epoch 28/50
583242/583242 [==============================] - 49s 85us/step - loss: 4335126321.6809 - accuracy: 0.5203 - precision_3: 0.5338 - recall_3: 0.5241
Epoch 29/50
583242/583242 [==============================] - 65s 111us/step - loss: 16086693.3498 - accuracy: 0.5221 - precision_3: 0.5333 - recall_3: 0.5224
Epoch 30/50
583242/583242 [==============================] - 83s 143us/step - loss: 17250581.5588 - accuracy: 0.5189 - precision_3: 0.5326 - recall_3: 0.5216
Epoch 31/50
583242/583242 [==============================] - 85s 146us/step - loss: 111899124.3596 - accuracy: 0.5140 - precision_3: 0.5320 - recall_3: 0.5214
Epoch 32/50
583242/583242 [==============================] - 77s 131us/step - loss: 12071200.9545 - accuracy: 0.5163 - precision_3: 0.5314 - recall_3: 0.5200
Epoch 33/50
583242/583242 [==============================] - 41s 70us/step - loss: 89912.3199 - accuracy: 0.5140 - precision_3: 0.5307 - recall_3: 0.5184
Epoch 34/50
583242/583242 [==============================] - 49s 83us/step - loss: 5019474988.7828 - accuracy: 0.5200 - precision_3: 0.5300 - recall_3: 0.5177
Epoch 35/50
583242/583242 [==============================] - 65s 111us/step - loss: 2194486.6746 - accuracy: 0.5123 - precision_3: 0.5294 - recall_3: 0.5171
Epoch 36/50
583242/583242 [==============================] - 74s 126us/step - loss: 243921003.2174 - accuracy: 0.5123 - precision_3: 0.5289 - recall_3: 0.5163
Epoch 37/50
583242/583242 [==============================] - 71s 122us/step - loss: 337973255.7395 - accuracy: 0.5142 - precision_3: 0.5284 - recall_3: 0.5155
Epoch 38/50
583242/583242 [==============================] - 82s 140us/step - loss: 11438510.1172 - accuracy: 0.5131 - precision_3: 0.5278 - recall_3: 0.5148
Epoch 39/50
583242/583242 [==============================] - 54s 92us/step - loss: 55743827927.7452 - accuracy: 0.5329 - precision_3: 0.5276 - recall_3: 0.5149
Epoch 40/50
583242/583242 [==============================] - 36s 62us/step - loss: 1868671204.3497 - accuracy: 0.5138 - precision_3: 0.5274 - recall_3: 0.5144
Epoch 41/50
583242/583242 [==============================] - 49s 84us/step - loss: 811181217.1609 - accuracy: 0.5112 - precision_3: 0.5268 - recall_3: 0.5138
Epoch 42/50
583242/583242 [==============================] - 75s 129us/step - loss: 14365408.8403 - accuracy: 0.5131 - precision_3: 0.5264 - recall_3: 0.5128
Epoch 43/50
583242/583242 [==============================] - 83s 142us/step - loss: 39625109.2388 - accuracy: 0.5104 - precision_3: 0.5258 - recall_3: 0.5123
Epoch 44/50
583242/583242 [==============================] - 80s 138us/step - loss: 865381415.7334 - accuracy: 0.5069 - precision_3: 0.5254 - recall_3: 0.5116
Epoch 45/50
583242/583242 [==============================] - 45s 77us/step - loss: 27334960.3853 - accuracy: 0.5109 - precision_3: 0.5249 - recall_3: 0.5102
Epoch 46/50
583242/583242 [==============================] - 38s 65us/step - loss: 390138492.4089 - accuracy: 0.5294 - precision_3: 0.5247 - recall_3: 0.5093
Epoch 47/50
583242/583242 [==============================] - 57s 99us/step - loss: 774499647.6348 - accuracy: 0.5235 - precision_3: 0.5247 - recall_3: 0.5094
Epoch 48/50
583242/583242 [==============================] - 82s 141us/step - loss: 2688848614.9629 - accuracy: 0.5231 - precision_3: 0.5245 - recall_3: 0.5093
Epoch 49/50
583242/583242 [==============================] - 70s 119us/step - loss: 44206730.9636 - accuracy: 0.5332 - precision_3: 0.5244 - recall_3: 0.5091
Epoch 50/50
583242/583242 [==============================] - 82s 140us/step - loss: 40974965.0748 - accuracy: 0.5139 - precision_3: 0.5244 - recall_3: 0.5090
history.history:
{'loss': [3.6717688435238847, 31.88347330961178, 970.6843226477176, 19114.29721726703, 1181.5970651867303, 8441.124685059935, 36950.49515049321, 116483.58315119214, 66430.45691532917, 40108.38600440279, 727337.599806795, 1282.8580632496398, 103536.15532560239, 1276537.5464364071, 20254.746900359052, 50449.26680390539, 28359.748995075417, 12998870.545236068, 100591711.44186695, 251706.74725549438, 18993536904.627293, 2527883.2410014085, 96780325.89558546, 110343198327.48088, 4751495.807433627, 115199929.48209143, 3632382.293412664, 4335126321.68091, 16086693.34976749, 17250581.558772035, 111899124.3595799, 12071200.954499938, 89912.31985188853, 5019474988.782777, 2194486.674615152, 243921003.2174246, 337973255.73953503, 11438510.11717041, 55743827927.7452, 1868671204.3496773, 811181217.1609424, 14365408.840265436, 39625109.238775946, 865381415.7333674, 27334960.385289952, 390138492.40889776, 774499647.6348025, 2688848614.962888, 44206730.96355405, 40974965.07475409], 'accuracy': [0.6788177, 0.65120655, 0.5551229, 0.5197037, 0.50859165, 0.512674, 0.51301, 0.5308328, 0.5420769, 0.5425398, 0.5215622, 0.5722359, 0.5163568, 0.5382826, 0.5216514, 0.521876, 0.5330772, 0.5296052, 0.5282147, 0.5356627, 0.5367309, 0.54804355, 0.53461, 0.5236471, 0.50031376, 0.51165897, 0.5237226, 0.520254, 0.5220663, 0.5189047, 0.51404047, 0.5163431, 0.51397705, 0.51996255, 0.51226765, 0.5123465, 0.5141862, 0.51305634, 0.5329074, 0.51375073, 0.5112183, 0.5131472, 0.5104262, 0.50693196, 0.5109286, 0.5294406, 0.5235271, 0.5230779, 0.5331818, 0.5138519], 'precision_3': [0.64425987, 0.66064996, 0.63626057, 0.60706437, 0.5849102, 0.57018083, 0.5610942, 0.5553938, 0.5532214, 0.55129766, 0.5491006, 0.54836845, 0.5472149, 0.5453398, 0.54375166, 0.5422222, 0.540734, 0.53990895, 0.53946155, 0.5390548, 0.5385246, 0.5386137, 0.5384629, 0.5380209, 0.5366103, 0.5352094, 0.5342978, 0.5337694, 0.533314, 0.5326327, 0.53199095, 0.5313548, 0.53069246, 0.5300007, 0.5294373, 0.52885455, 0.5283757, 0.52784765, 0.5276191, 0.52737504, 0.52681893, 0.5263995, 0.5258461, 0.52543443, 0.5248831, 0.5247407, 0.5246575, 0.5244812, 0.524426, 0.52444065], 'recall_3': [0.78454095, 0.72854275, 0.6594887, 0.62233675, 0.5966734, 0.5788544, 0.56612456, 0.5538447, 0.548115, 0.5480589, 0.54318047, 0.54491735, 0.54780936, 0.5466233, 0.54473305, 0.54359114, 0.54249275, 0.54052186, 0.5371138, 0.5346066, 0.5319002, 0.530562, 0.5308916, 0.5307254, 0.53008187, 0.528621, 0.52625793, 0.5240948, 0.5223988, 0.5215986, 0.5213593, 0.5199994, 0.51844186, 0.5177219, 0.5171429, 0.5162939, 0.51552016, 0.514753, 0.51492375, 0.51436996, 0.51379716, 0.51280594, 0.5122596, 0.51158893, 0.510239, 0.5093229, 0.5093804, 0.5093098, 0.50911397, 0.509041]}
144811/144811 [==============================] - 3s 24us/step
results evaluated:
[5359000.140486565, 0.49399563670158386, 0.5240612030029297, 0.5101418495178223]
predictions:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
pred_y:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_37 (Dense)             (None, 40)                480       
_________________________________________________________________
dense_38 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_39 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_40 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_41 (Dense)             (None, 2)                 82        
=================================================================
Total params: 5,482
Trainable params: 5,482
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.85800292 -0.45741021  0.59116916  0.03276942 -1.06987407 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.3440938  -0.48540712 -1.12421396  0.12540677  1.48684084 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [ 2.03152383  0.4771183  -1.52783351  0.03276942  1.91570157 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.50918927  0.57244989  0.49026427 -0.27364646  0.1990837  -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 0.70311724  0.49309022  0.28845449  0.06127322 -0.22977703 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]]
train_y BEFORE fitting:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]
 [1. 0.]]
test_X BEFORE fitting:
[[ 1.26550835  0.51166712 -0.01426018 -0.15725593 -0.44420739 -0.3788845
  -0.52714789  2.08175825 -0.54492263 -0.34356544 -0.39504551]
 [-0.32470101 -0.34831698  0.38935938 -0.06224326 -0.01534666 -0.3788845
  -0.52714789 -0.48036317  1.83512292 -0.34356544 -0.39504551]
 [ 1.02309839  0.64562603 -0.41787973 -0.17150783 -0.8736556   2.63932675
  -0.52714789 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 2.2254518   2.05128678 -0.7205944   0.14440931  0.62853191 -0.3788845
   1.89700087 -0.48036317 -0.54492263 -0.34356544 -0.39504551]
 [ 0.46070727  0.71288177  0.08664471  0.00901625 -0.64101334 -0.3788845
  -0.52714789 -0.48036317 -0.54492263 -0.34356544  2.53135392]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288244, 294998]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71536, 73275]))
Epoch 1/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5243 - accuracy: 0.7527 - precision_4: 0.7283 - recall_4: 0.7825
Epoch 2/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5042 - accuracy: 0.7580 - precision_4: 0.7332 - recall_4: 0.7901
Epoch 3/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5045 - accuracy: 0.7584 - precision_4: 0.7352 - recall_4: 0.7913
Epoch 4/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.5019 - accuracy: 0.7594 - precision_4: 0.7353 - recall_4: 0.7935
Epoch 5/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.5250 - accuracy: 0.7575 - precision_4: 0.7352 - recall_4: 0.7949
Epoch 6/50
583242/583242 [==============================] - 27s 46us/step - loss: 0.5032 - accuracy: 0.7590 - precision_4: 0.7352 - recall_4: 0.7955
Epoch 7/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.5037 - accuracy: 0.7585 - precision_4: 0.7352 - recall_4: 0.7965
Epoch 8/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.5033 - accuracy: 0.7589 - precision_4: 0.7353 - recall_4: 0.7969
Epoch 9/50
583242/583242 [==============================] - 35s 61us/step - loss: 0.4993 - accuracy: 0.7598 - precision_4: 0.7353 - recall_4: 0.7969
Epoch 10/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.5011 - accuracy: 0.7601 - precision_4: 0.7355 - recall_4: 0.7969
Epoch 11/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.5041 - accuracy: 0.7601 - precision_4: 0.7360 - recall_4: 0.7969
Epoch 12/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.5004 - accuracy: 0.7598 - precision_4: 0.7363 - recall_4: 0.7969
Epoch 13/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.5050 - accuracy: 0.7597 - precision_4: 0.7363 - recall_4: 0.7969
Epoch 14/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5015 - accuracy: 0.7602 - precision_4: 0.7363 - recall_4: 0.7969
Epoch 15/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4994 - accuracy: 0.7607 - precision_4: 0.7365 - recall_4: 0.7969
Epoch 16/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4982 - accuracy: 0.7609 - precision_4: 0.7369 - recall_4: 0.7969
Epoch 17/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4991 - accuracy: 0.7606 - precision_4: 0.7370 - recall_4: 0.7969
Epoch 18/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4991 - accuracy: 0.7603 - precision_4: 0.7372 - recall_4: 0.7969
Epoch 19/50
583242/583242 [==============================] - 27s 47us/step - loss: 0.4984 - accuracy: 0.7603 - precision_4: 0.7373 - recall_4: 0.7969
Epoch 20/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4987 - accuracy: 0.7610 - precision_4: 0.7373 - recall_4: 0.7968
Epoch 21/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4980 - accuracy: 0.7605 - precision_4: 0.7373 - recall_4: 0.7968
Epoch 22/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4980 - accuracy: 0.7609 - precision_4: 0.7375 - recall_4: 0.7967
Epoch 23/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4999 - accuracy: 0.7606 - precision_4: 0.7376 - recall_4: 0.7967
Epoch 24/50
583242/583242 [==============================] - 34s 59us/step - loss: 0.4999 - accuracy: 0.7603 - precision_4: 0.7378 - recall_4: 0.7966
Epoch 25/50
583242/583242 [==============================] - 33s 57us/step - loss: 0.4982 - accuracy: 0.7605 - precision_4: 0.7376 - recall_4: 0.7966
Epoch 26/50
583242/583242 [==============================] - 34s 58us/step - loss: 0.4982 - accuracy: 0.7613 - precision_4: 0.7380 - recall_4: 0.7967
Epoch 27/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4976 - accuracy: 0.7608 - precision_4: 0.7380 - recall_4: 0.7968
Epoch 28/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.4977 - accuracy: 0.7608 - precision_4: 0.7380 - recall_4: 0.7967
Epoch 29/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5003 - accuracy: 0.7604 - precision_4: 0.7382 - recall_4: 0.7966
Epoch 30/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4984 - accuracy: 0.7608 - precision_4: 0.7382 - recall_4: 0.7964
Epoch 31/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4983 - accuracy: 0.7601 - precision_4: 0.7383 - recall_4: 0.7962
Epoch 32/50
583242/583242 [==============================] - 26s 44us/step - loss: 0.4975 - accuracy: 0.7612 - precision_4: 0.7383 - recall_4: 0.7962
Epoch 33/50
583242/583242 [==============================] - 34s 59us/step - loss: 0.4978 - accuracy: 0.7610 - precision_4: 0.7383 - recall_4: 0.7966
Epoch 34/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4987 - accuracy: 0.7606 - precision_4: 0.7383 - recall_4: 0.7966
Epoch 35/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4975 - accuracy: 0.7612 - precision_4: 0.7383 - recall_4: 0.7966
Epoch 36/50
583242/583242 [==============================] - 34s 59us/step - loss: 0.4983 - accuracy: 0.7605 - precision_4: 0.7383 - recall_4: 0.7966
Epoch 37/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4977 - accuracy: 0.7610 - precision_4: 0.7383 - recall_4: 0.7962
Epoch 38/50
583242/583242 [==============================] - 34s 58us/step - loss: 0.5034 - accuracy: 0.7602 - precision_4: 0.7383 - recall_4: 0.7962
Epoch 39/50
583242/583242 [==============================] - 33s 56us/step - loss: 0.4989 - accuracy: 0.7607 - precision_4: 0.7383 - recall_4: 0.7962
Epoch 40/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4986 - accuracy: 0.7607 - precision_4: 0.7384 - recall_4: 0.7962
Epoch 41/50
583242/583242 [==============================] - 15s 27us/step - loss: 0.4989 - accuracy: 0.7609 - precision_4: 0.7385 - recall_4: 0.7962
Epoch 42/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4978 - accuracy: 0.7609 - precision_4: 0.7385 - recall_4: 0.7961
Epoch 43/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4975 - accuracy: 0.7613 - precision_4: 0.7385 - recall_4: 0.7961
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5008 - accuracy: 0.7610 - precision_4: 0.7385 - recall_4: 0.7962
Epoch 45/50
583242/583242 [==============================] - 26s 44us/step - loss: 0.4998 - accuracy: 0.7607 - precision_4: 0.7385 - recall_4: 0.7962
Epoch 46/50
583242/583242 [==============================] - 32s 55us/step - loss: 0.4980 - accuracy: 0.7608 - precision_4: 0.7386 - recall_4: 0.7962
Epoch 47/50
583242/583242 [==============================] - 36s 61us/step - loss: 0.4972 - accuracy: 0.7607 - precision_4: 0.7386 - recall_4: 0.7962
Epoch 48/50
583242/583242 [==============================] - 35s 59us/step - loss: 0.4979 - accuracy: 0.7611 - precision_4: 0.7386 - recall_4: 0.7962
Epoch 49/50
583242/583242 [==============================] - 35s 60us/step - loss: 0.4987 - accuracy: 0.7611 - precision_4: 0.7386 - recall_4: 0.7965
Epoch 50/50
583242/583242 [==============================] - 36s 62us/step - loss: 0.4968 - accuracy: 0.7619 - precision_4: 0.7387 - recall_4: 0.7966
history.history:
{'loss': [0.5243189873807004, 0.5042443606507501, 0.5044905004412196, 0.5019236487311927, 0.5249967656427927, 0.5032085489035311, 0.5037339301715962, 0.5032541689152435, 0.49929456480021445, 0.5011344966626367, 0.5040984573425199, 0.5004331658830308, 0.505022425710983, 0.5015025800355533, 0.499371527791114, 0.4981677794088088, 0.49909112172349784, 0.49907106946141677, 0.4984159584759282, 0.49865285799646, 0.4979792318987015, 0.4980071782824615, 0.4998557865207812, 0.49985392119673305, 0.4982476055111737, 0.4982195318657405, 0.4976051391246351, 0.4977326683993922, 0.5002504151377565, 0.4983541120038493, 0.4983364961759753, 0.4975358020302718, 0.4977595293515872, 0.49869193518264093, 0.4974600203283242, 0.4983328601194228, 0.4977206979220233, 0.5034224926514006, 0.49890084468126505, 0.49862764871535653, 0.4988975672736154, 0.4977796672800797, 0.49748059355900837, 0.500819722670147, 0.49982660805098106, 0.4979584386251917, 0.49719095788457285, 0.497942757820667, 0.49869085016598924, 0.49683795471611075], 'accuracy': [0.75271326, 0.75801814, 0.75842273, 0.7594429, 0.75746775, 0.7589765, 0.75853246, 0.7589457, 0.7597824, 0.76006013, 0.76010644, 0.7597721, 0.75974464, 0.76016814, 0.76072365, 0.7608574, 0.7605642, 0.7603122, 0.76034135, 0.76097226, 0.76050425, 0.76091397, 0.76061225, 0.7603345, 0.76051795, 0.76134604, 0.76076823, 0.760818, 0.76044077, 0.76081115, 0.7601013, 0.7611866, 0.76100314, 0.7606054, 0.7612089, 0.76051277, 0.7609603, 0.7601733, 0.7607048, 0.76069283, 0.760938, 0.7608677, 0.7612586, 0.7609603, 0.7607374, 0.7608454, 0.7606825, 0.76105285, 0.76107, 0.76193756], 'precision_4': [0.72831225, 0.73324066, 0.7351939, 0.7353373, 0.73515934, 0.7352422, 0.73521096, 0.7352821, 0.7352896, 0.7355034, 0.73603463, 0.7362532, 0.73630995, 0.73633766, 0.7365413, 0.7368858, 0.7370208, 0.73722917, 0.73727053, 0.7373, 0.7373228, 0.73752594, 0.73756295, 0.7378258, 0.73760307, 0.7379835, 0.73798823, 0.73803985, 0.73820686, 0.7382208, 0.73826283, 0.7382658, 0.7382779, 0.73828125, 0.73828113, 0.7382848, 0.73831666, 0.73834074, 0.738344, 0.73837197, 0.7385043, 0.7385123, 0.7385205, 0.7385226, 0.73853874, 0.7385608, 0.738563, 0.738567, 0.73857456, 0.73871374], 'recall_4': [0.7824548, 0.7900586, 0.7912965, 0.7935405, 0.7948944, 0.7955491, 0.7965092, 0.79685956, 0.7969285, 0.7969311, 0.7968779, 0.7968736, 0.7968877, 0.7968815, 0.79689395, 0.79687953, 0.796875, 0.7968694, 0.7968522, 0.7968232, 0.7968102, 0.7966555, 0.7966509, 0.79661363, 0.7965997, 0.79673654, 0.7968095, 0.79666764, 0.7966164, 0.79640704, 0.796174, 0.796173, 0.7965994, 0.79660267, 0.79664046, 0.7966167, 0.796226, 0.79617316, 0.79617065, 0.79617, 0.79615784, 0.7961224, 0.7961221, 0.79615766, 0.7961697, 0.79616964, 0.7961729, 0.79617053, 0.79645723, 0.79659945]}
144811/144811 [==============================] - 4s 30us/step
results evaluated:
[0.4926805767079419, 0.7634226679801941, 0.73876953125, 0.7963996529579163]
predictions:
[[0.8990202  0.10097981]
 [0.83864343 0.16135652]
 [0.31140986 0.6885901 ]
 ...
 [0.3169428  0.68305725]
 [0.18770143 0.81229854]
 [0.30126408 0.69873595]]
pred_y:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [0. 1.]
 [0. 1.]
 [0. 1.]]
