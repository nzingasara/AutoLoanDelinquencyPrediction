Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 5)                 60        
_________________________________________________________________
dense_2 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_4 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_5 (Dense)              (None, 2)                 12        
=================================================================
Total params: 162
Trainable params: 162
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.12072835  0.06224739 -0.1153661  -0.03172075 -0.65880084  2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]
 [ 0.51020808  0.44357032  0.89385305 -0.10968772 -1.73149673 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-1.08169307 -0.52091018 -0.1153661   0.09586157 -0.4437917  -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 1.67501381  0.71605772 -2.23472631  0.15965274  1.93423291 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 0.66551551 -0.0068908  -0.8218195  -0.24435795  1.70160007 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]]
train_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.2177955  -0.21637795 -0.61997567 -0.07424819  0.4144825  -0.37939622
  -0.52680264 -0.48076322 -0.54421604 -0.34394896  2.53222775]
 [-1.2370005  -0.76834096  0.89385305 -0.24435795 -1.08764421 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-0.22750221 -0.31272502  0.28832156  0.03207041 -0.22937001 -0.37939622
  -0.52680264 -0.48076322 -0.54421604  2.90740811 -0.39490919]
 [ 1.17026466 -0.07302646 -0.61997567  0.09586157  0.43269366 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [-0.47987678 -0.61967523 -1.32642908 -0.02699548  2.1310309   2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288332, 294910]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71470, 73341]))
Epoch 1/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5091 - accuracy: 0.7522 - precision: 0.7241 - recall: 0.7778 - f1_score: 2.6746e-05       
Epoch 2/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5022 - accuracy: 0.7569 - precision: 0.7331 - recall: 0.7886 - f1_score: 4.6997e-06
Epoch 3/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4996 - accuracy: 0.7583 - precision: 0.7356 - recall: 0.7888 - f1_score: 2.7490e-06
Epoch 4/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4991 - accuracy: 0.7591 - precision: 0.7364 - recall: 0.7897 - f1_score: 1.9512e-06
Epoch 5/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4988 - accuracy: 0.7590 - precision: 0.7372 - recall: 0.7900 - f1_score: 1.5130e-06
Epoch 6/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4987 - accuracy: 0.7598 - precision: 0.7379 - recall: 0.7900 - f1_score: 1.2364e-06
Epoch 7/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4983 - accuracy: 0.7595 - precision: 0.7381 - recall: 0.7902 - f1_score: 1.0453e-06
Epoch 8/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4981 - accuracy: 0.7595 - precision: 0.7384 - recall: 0.7900 - f1_score: 9.0555e-07
Epoch 9/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4982 - accuracy: 0.7597 - precision: 0.7390 - recall: 0.7899 - f1_score: 7.9874e-07
Epoch 10/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4980 - accuracy: 0.7597 - precision: 0.7392 - recall: 0.7900 - f1_score: 7.1457e-07
Epoch 11/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4980 - accuracy: 0.7595 - precision: 0.7393 - recall: 0.7900 - f1_score: 6.4635e-07
Epoch 12/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4979 - accuracy: 0.7601 - precision: 0.7394 - recall: 0.7900 - f1_score: 5.9013e-07
Epoch 13/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4979 - accuracy: 0.7601 - precision: 0.7399 - recall: 0.7898 - f1_score: 5.4283e-07
Epoch 14/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4977 - accuracy: 0.7602 - precision: 0.7399 - recall: 0.7898 - f1_score: 5.0259e-07
Epoch 15/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4978 - accuracy: 0.7598 - precision: 0.7401 - recall: 0.7900 - f1_score: 4.6792e-07
Epoch 16/50
583242/583242 [==============================] - 18s 30us/step - loss: 0.4980 - accuracy: 0.7600 - precision: 0.7400 - recall: 0.7900 - f1_score: 4.3768e-07
Epoch 17/50
583242/583242 [==============================] - 18s 32us/step - loss: 0.4979 - accuracy: 0.7598 - precision: 0.7402 - recall: 0.7900 - f1_score: 4.1114e-07
Epoch 18/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4979 - accuracy: 0.7601 - precision: 0.7402 - recall: 0.7900 - f1_score: 3.8763e-07
Epoch 19/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4978 - accuracy: 0.7598 - precision: 0.7402 - recall: 0.7900 - f1_score: 3.6668e-07
Epoch 20/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4978 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7901 - f1_score: 3.4787e-07
Epoch 21/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4977 - accuracy: 0.7598 - precision: 0.7402 - recall: 0.7901 - f1_score: 3.3088e-07
Epoch 22/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4976 - accuracy: 0.7603 - precision: 0.7402 - recall: 0.7902 - f1_score: 3.1549e-07
Epoch 23/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4977 - accuracy: 0.7599 - precision: 0.7402 - recall: 0.7903 - f1_score: 3.0146e-07
Epoch 24/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4977 - accuracy: 0.7595 - precision: 0.7402 - recall: 0.7907 - f1_score: 2.8863e-07
Epoch 25/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4975 - accuracy: 0.7604 - precision: 0.7402 - recall: 0.7908 - f1_score: 2.7684e-07
Epoch 26/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4977 - accuracy: 0.7605 - precision: 0.7402 - recall: 0.7909 - f1_score: 2.6598e-07
Epoch 27/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4976 - accuracy: 0.7597 - precision: 0.7402 - recall: 0.7910 - f1_score: 2.5595e-07
Epoch 28/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4976 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7910 - f1_score: 2.4663e-07
Epoch 29/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4977 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7910 - f1_score: 2.3798e-07
Epoch 30/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4976 - accuracy: 0.7602 - precision: 0.7402 - recall: 0.7910 - f1_score: 2.2991e-07
Epoch 31/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4977 - accuracy: 0.7599 - precision: 0.7402 - recall: 0.7911 - f1_score: 2.2237e-07
Epoch 32/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4975 - accuracy: 0.7604 - precision: 0.7402 - recall: 0.7911 - f1_score: 2.1531e-07
Epoch 33/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4975 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7912 - f1_score: 2.0868e-07
Epoch 34/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4974 - accuracy: 0.7601 - precision: 0.7402 - recall: 0.7912 - f1_score: 2.0245e-07
Epoch 35/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4976 - accuracy: 0.7598 - precision: 0.7402 - recall: 0.7912 - f1_score: 1.9659e-07
Epoch 36/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4976 - accuracy: 0.7602 - precision: 0.7402 - recall: 0.7912 - f1_score: 1.9105e-07
Epoch 37/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4975 - accuracy: 0.7604 - precision: 0.7402 - recall: 0.7913 - f1_score: 1.8581e-07
Epoch 38/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4975 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7913 - f1_score: 1.8086e-07
Epoch 39/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4976 - accuracy: 0.7599 - precision: 0.7402 - recall: 0.7913 - f1_score: 1.7616e-07
Epoch 40/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4975 - accuracy: 0.7603 - precision: 0.7402 - recall: 0.7913 - f1_score: 1.7170e-07
Epoch 41/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4976 - accuracy: 0.7602 - precision: 0.7402 - recall: 0.7917 - f1_score: 1.6746e-07
Epoch 42/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4974 - accuracy: 0.7602 - precision: 0.7402 - recall: 0.7917 - f1_score: 1.6343e-07
Epoch 43/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4976 - accuracy: 0.7603 - precision: 0.7402 - recall: 0.7917 - f1_score: 1.5958e-07
Epoch 44/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4976 - accuracy: 0.7601 - precision: 0.7402 - recall: 0.7918 - f1_score: 1.5591e-07
Epoch 45/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4975 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7918 - f1_score: 1.5241e-07
Epoch 46/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4975 - accuracy: 0.7603 - precision: 0.7402 - recall: 0.7919 - f1_score: 1.4905e-07
Epoch 47/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4974 - accuracy: 0.7604 - precision: 0.7402 - recall: 0.7919 - f1_score: 1.4585e-07
Epoch 48/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4975 - accuracy: 0.7602 - precision: 0.7402 - recall: 0.7919 - f1_score: 1.4278e-07
Epoch 49/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4976 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7919 - f1_score: 1.3984e-07
Epoch 50/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4977 - accuracy: 0.7600 - precision: 0.7402 - recall: 0.7919 - f1_score: 1.3701e-07
history.history:
{'loss': [0.50913593528248, 0.5022425141440973, 0.49964355757827245, 0.4991114754846725, 0.49877213392547975, 0.4986793695986131, 0.4983034699493134, 0.4981377151838519, 0.49821709064286174, 0.49803163193328975, 0.49803232295620026, 0.49790611850852406, 0.4979399448512174, 0.49768462894979265, 0.49778493538528873, 0.4980467548439723, 0.49786468541248413, 0.4979423289669024, 0.4978403809220643, 0.4978203006023405, 0.4976970553028185, 0.49764338658745094, 0.49768366535659586, 0.49770719114521106, 0.4975294106348061, 0.4977124801985337, 0.4976254230550853, 0.49756152155068856, 0.49772087505034684, 0.49762997723544755, 0.49768392720888494, 0.4975267032085673, 0.4975025145520254, 0.49742267687603237, 0.49762020957888287, 0.49755044424980355, 0.4975462833723927, 0.4974713360900972, 0.4975610841319281, 0.4975114170032404, 0.4976125116985696, 0.49742127169808004, 0.4976332963436203, 0.49762162493861317, 0.4974770313798113, 0.4974521968342895, 0.49741387095531425, 0.4974829248693124, 0.4976149980593135, 0.4976504341531618], 'accuracy': [0.7522195, 0.75693107, 0.758349, 0.75910515, 0.75903314, 0.7598064, 0.75948066, 0.75953895, 0.7596675, 0.7597155, 0.75946176, 0.7601116, 0.76007384, 0.7602162, 0.75979954, 0.76002586, 0.75980294, 0.76007044, 0.75983554, 0.7599556, 0.7598201, 0.76029503, 0.75985956, 0.7595149, 0.7603568, 0.7604905, 0.7597361, 0.75996757, 0.7600464, 0.76021445, 0.759887, 0.7604082, 0.7600104, 0.76007384, 0.75982356, 0.760235, 0.76038074, 0.76001555, 0.7598973, 0.7602813, 0.7602487, 0.76024705, 0.760271, 0.760067, 0.7599504, 0.76028305, 0.76037735, 0.76024187, 0.75996584, 0.760007], 'precision': [0.7240814, 0.7331249, 0.735561, 0.7364018, 0.73719287, 0.73792815, 0.73812443, 0.7384287, 0.7389608, 0.7391878, 0.7392516, 0.73940885, 0.73986727, 0.7399405, 0.74014115, 0.74001646, 0.74016464, 0.7402161, 0.7402157, 0.7402167, 0.7402318, 0.7402341, 0.74022996, 0.7402309, 0.74021846, 0.7402195, 0.7402208, 0.7402203, 0.74023306, 0.74023145, 0.74022025, 0.7402247, 0.7402196, 0.7402235, 0.74023306, 0.74023324, 0.7402306, 0.7402336, 0.7402325, 0.7402343, 0.7402343, 0.7402334, 0.74023414, 0.74023414, 0.7402345, 0.74023324, 0.7402341, 0.7402335, 0.7402335, 0.7402338], 'recall': [0.77781314, 0.7885705, 0.78882444, 0.7896938, 0.7899787, 0.7900086, 0.79017013, 0.7899887, 0.78986895, 0.7899734, 0.7900352, 0.789974, 0.7898297, 0.78983045, 0.7899707, 0.7899897, 0.78999287, 0.78999037, 0.79004854, 0.7900928, 0.79005235, 0.7902428, 0.7903024, 0.79073554, 0.7907748, 0.79094696, 0.7909874, 0.7910013, 0.7909922, 0.79101145, 0.79106945, 0.7910794, 0.7912269, 0.79122716, 0.79122627, 0.7912271, 0.79126436, 0.7912788, 0.7912913, 0.79128873, 0.79171485, 0.7917312, 0.7917363, 0.79177016, 0.79176944, 0.79192287, 0.7919237, 0.7919359, 0.7919393, 0.7919481], 'f1_score': [2.6745767e-05, 4.699659e-06, 2.7490155e-06, 1.9511951e-06, 1.5130468e-06, 1.2364462e-06, 1.0452706e-06, 9.0554784e-07, 7.9874224e-07, 7.145741e-07, 6.4634605e-07, 5.901276e-07, 5.428264e-07, 5.025934e-07, 4.6791686e-07, 4.3767568e-07, 4.1113685e-07, 3.876343e-07, 3.6667902e-07, 3.4787126e-07, 3.3087673e-07, 3.1548635e-07, 3.014578e-07, 2.8863008e-07, 2.7683868e-07, 2.6598457e-07, 2.5595e-07, 2.4663322e-07, 2.3798293e-07, 2.2991003e-07, 2.2237214e-07, 2.15307e-07, 2.0868328e-07, 2.0245463e-07, 1.9658783e-07, 1.9104581e-07, 1.8581044e-07, 1.8085903e-07, 1.7615594e-07, 1.7169627e-07, 1.6745975e-07, 1.6342607e-07, 1.5957856e-07, 1.5590958e-07, 1.5240643e-07, 1.4905366e-07, 1.458492e-07, 1.427805e-07, 1.3983505e-07, 1.370098e-07]}
144811/144811 [==============================] - 2s 15us/step
results evaluated:
[0.4956409428364497, 0.7614200711250305, 0.7402027249336243, 0.7919785976409912, 1.3529808029488777e-07]
1615/1615 [==============================] - 0s 14us/step
auto results evaluated:
[0.47001773139848246, 0.900928795337677, 0.7401391863822937, 0.7920677065849304, 1.349565224018079e-07]
predictions:
[[0.66386896 0.336131  ]
 [0.25370368 0.74629635]
 [0.18372306 0.81627697]
 ...
 [0.3178803  0.6821197 ]
 [0.45298025 0.5470197 ]
 [0.66736376 0.33263615]]
pred_y:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 10)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_8 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_9 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_10 (Dense)             (None, 2)                 22        
=================================================================
Total params: 472
Trainable params: 472
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.12072835  0.06224739 -0.1153661  -0.03172075 -0.65880084  2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]
 [ 0.51020808  0.44357032  0.89385305 -0.10968772 -1.73149673 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-1.08169307 -0.52091018 -0.1153661   0.09586157 -0.4437917  -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 1.67501381  0.71605772 -2.23472631  0.15965274  1.93423291 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 0.66551551 -0.0068908  -0.8218195  -0.24435795  1.70160007 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]]
train_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.2177955  -0.21637795 -0.61997567 -0.07424819  0.4144825  -0.37939622
  -0.52680264 -0.48076322 -0.54421604 -0.34394896  2.53222775]
 [-1.2370005  -0.76834096  0.89385305 -0.24435795 -1.08764421 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-0.22750221 -0.31272502  0.28832156  0.03207041 -0.22937001 -0.37939622
  -0.52680264 -0.48076322 -0.54421604  2.90740811 -0.39490919]
 [ 1.17026466 -0.07302646 -0.61997567  0.09586157  0.43269366 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [-0.47987678 -0.61967523 -1.32642908 -0.02699548  2.1310309   2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288332, 294910]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71470, 73341]))
Epoch 1/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.5068 - accuracy: 0.7548 - precision_1: 0.7260 - recall_1: 0.7841 - f1_score: 1.6544e-05     
Epoch 2/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4986 - accuracy: 0.7601 - precision_1: 0.7345 - recall_1: 0.7953 - f1_score: 2.3510e-06
Epoch 3/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4971 - accuracy: 0.7607 - precision_1: 0.7353 - recall_1: 0.7972 - f1_score: 2.4530e-06
Epoch 4/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4968 - accuracy: 0.7606 - precision_1: 0.7361 - recall_1: 0.7987 - f1_score: 4.9439e-06
Epoch 5/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4966 - accuracy: 0.7606 - precision_1: 0.7363 - recall_1: 0.7995 - f1_score: 3.8350e-06
Epoch 6/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4961 - accuracy: 0.7611 - precision_1: 0.7362 - recall_1: 0.8007 - f1_score: 4.2664e-06
Epoch 7/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4964 - accuracy: 0.7609 - precision_1: 0.7363 - recall_1: 0.8008 - f1_score: 4.9153e-06
Epoch 8/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4962 - accuracy: 0.7612 - precision_1: 0.7369 - recall_1: 0.8003 - f1_score: 4.6422e-06
Epoch 9/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4962 - accuracy: 0.7612 - precision_1: 0.7372 - recall_1: 0.7999 - f1_score: 5.0059e-06
Epoch 10/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4960 - accuracy: 0.7608 - precision_1: 0.7373 - recall_1: 0.7998 - f1_score: 5.7814e-06
Epoch 11/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4958 - accuracy: 0.7610 - precision_1: 0.7373 - recall_1: 0.8003 - f1_score: 5.2299e-06
Epoch 12/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4959 - accuracy: 0.7614 - precision_1: 0.7373 - recall_1: 0.8006 - f1_score: 4.7745e-06
Epoch 13/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4958 - accuracy: 0.7610 - precision_1: 0.7373 - recall_1: 0.8008 - f1_score: 4.3921e-06
Epoch 14/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4958 - accuracy: 0.7611 - precision_1: 0.7373 - recall_1: 0.8008 - f1_score: 4.0665e-06
Epoch 15/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4960 - accuracy: 0.7614 - precision_1: 0.7373 - recall_1: 0.8014 - f1_score: 4.4208e-06
Epoch 16/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4956 - accuracy: 0.7616 - precision_1: 0.7373 - recall_1: 0.8010 - f1_score: 4.3398e-06
Epoch 17/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4958 - accuracy: 0.7614 - precision_1: 0.7374 - recall_1: 0.8008 - f1_score: 4.5548e-06
Epoch 18/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4957 - accuracy: 0.7620 - precision_1: 0.7373 - recall_1: 0.8010 - f1_score: 4.4931e-06
Epoch 19/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4953 - accuracy: 0.7616 - precision_1: 0.7374 - recall_1: 0.8010 - f1_score: 4.4913e-06
Epoch 20/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4951 - accuracy: 0.7618 - precision_1: 0.7374 - recall_1: 0.8015 - f1_score: 4.5083e-06
Epoch 21/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4952 - accuracy: 0.7618 - precision_1: 0.7374 - recall_1: 0.8017 - f1_score: 5.0009e-06
Epoch 22/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4954 - accuracy: 0.7616 - precision_1: 0.7374 - recall_1: 0.8016 - f1_score: 5.1039e-06
Epoch 23/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4950 - accuracy: 0.7623 - precision_1: 0.7375 - recall_1: 0.8017 - f1_score: 5.4284e-06
Epoch 24/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4950 - accuracy: 0.7620 - precision_1: 0.7375 - recall_1: 0.8018 - f1_score: 5.6128e-06
Epoch 25/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4951 - accuracy: 0.7618 - precision_1: 0.7378 - recall_1: 0.8017 - f1_score: 5.5223e-06
Epoch 26/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4947 - accuracy: 0.7618 - precision_1: 0.7380 - recall_1: 0.8011 - f1_score: 5.4921e-06
Epoch 27/50
583242/583242 [==============================] - 17s 28us/step - loss: 0.4949 - accuracy: 0.7620 - precision_1: 0.7382 - recall_1: 0.8008 - f1_score: 5.3178e-06
Epoch 28/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4949 - accuracy: 0.7622 - precision_1: 0.7382 - recall_1: 0.8008 - f1_score: 5.2159e-06
Epoch 29/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4948 - accuracy: 0.7620 - precision_1: 0.7383 - recall_1: 0.8008 - f1_score: 5.0328e-06
Epoch 30/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4946 - accuracy: 0.7620 - precision_1: 0.7383 - recall_1: 0.8008 - f1_score: 4.9574e-06
Epoch 31/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4947 - accuracy: 0.7621 - precision_1: 0.7383 - recall_1: 0.8008 - f1_score: 5.0661e-06
Epoch 32/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4947 - accuracy: 0.7618 - precision_1: 0.7383 - recall_1: 0.8008 - f1_score: 4.9865e-06
Epoch 33/50
583242/583242 [==============================] - 20s 33us/step - loss: 0.4949 - accuracy: 0.7624 - precision_1: 0.7383 - recall_1: 0.8007 - f1_score: 4.8745e-06
Epoch 34/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4949 - accuracy: 0.7621 - precision_1: 0.7384 - recall_1: 0.8007 - f1_score: 4.8536e-06
Epoch 35/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7619 - precision_1: 0.7385 - recall_1: 0.8006 - f1_score: 4.8477e-06
Epoch 36/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4947 - accuracy: 0.7621 - precision_1: 0.7385 - recall_1: 0.8007 - f1_score: 4.7111e-06
Epoch 37/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4951 - accuracy: 0.7621 - precision_1: 0.7385 - recall_1: 0.8008 - f1_score: 4.5820e-06
Epoch 38/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4949 - accuracy: 0.7623 - precision_1: 0.7385 - recall_1: 0.8008 - f1_score: 4.5723e-06
Epoch 39/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7623 - precision_1: 0.7385 - recall_1: 0.8008 - f1_score: 4.5576e-06
Epoch 40/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4951 - accuracy: 0.7620 - precision_1: 0.7384 - recall_1: 0.8009 - f1_score: 4.6533e-06
Epoch 41/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4947 - accuracy: 0.7619 - precision_1: 0.7384 - recall_1: 0.8010 - f1_score: 4.6208e-06
Epoch 42/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7621 - precision_1: 0.7384 - recall_1: 0.8010 - f1_score: 4.5425e-06
Epoch 43/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4947 - accuracy: 0.7620 - precision_1: 0.7385 - recall_1: 0.8010 - f1_score: 4.5734e-06
Epoch 44/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4949 - accuracy: 0.7616 - precision_1: 0.7385 - recall_1: 0.8009 - f1_score: 4.6082e-06
Epoch 45/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4949 - accuracy: 0.7623 - precision_1: 0.7385 - recall_1: 0.8008 - f1_score: 4.6316e-06
Epoch 46/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4947 - accuracy: 0.7625 - precision_1: 0.7386 - recall_1: 0.8008 - f1_score: 4.6478e-06
Epoch 47/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4948 - accuracy: 0.7621 - precision_1: 0.7386 - recall_1: 0.8008 - f1_score: 4.7170e-06
Epoch 48/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7621 - precision_1: 0.7390 - recall_1: 0.8008 - f1_score: 4.6760e-06
Epoch 49/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7624 - precision_1: 0.7390 - recall_1: 0.8007 - f1_score: 4.6898e-06
Epoch 50/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4945 - accuracy: 0.7626 - precision_1: 0.7390 - recall_1: 0.8007 - f1_score: 4.6759e-06
history.history:
{'loss': [0.5067836539541578, 0.4985666790483664, 0.4970976357923237, 0.4968184701643523, 0.4966130900899736, 0.4961288724819437, 0.496438822723475, 0.49623093930071904, 0.49622910391113195, 0.49603515118311975, 0.4957500317016908, 0.49592849143721246, 0.49575358359819194, 0.49580697278320496, 0.496040875718498, 0.4955509390385577, 0.49576202946216347, 0.4956616993984796, 0.49529583450714665, 0.49505260592676137, 0.49516292482078095, 0.4953970917373996, 0.49500617087757043, 0.49500480577104444, 0.49505292663123057, 0.49468003834584967, 0.4949390347739474, 0.4948721796556347, 0.49484866322852616, 0.4945920200236728, 0.4947079661301505, 0.4946637979578865, 0.4949107212592715, 0.49488017743757423, 0.49456329811464755, 0.49471012366148276, 0.4950839260857685, 0.49493354131992795, 0.49461522662031066, 0.49506900904749856, 0.49474359659605743, 0.49463546566031075, 0.4947370143799252, 0.4948808566625888, 0.4948672651867467, 0.49466779622198975, 0.4947935961431567, 0.4945714696521561, 0.49461656021478534, 0.4944810452703887], 'accuracy': [0.7548273, 0.7601493, 0.7606894, 0.7606139, 0.7606174, 0.76113176, 0.7609483, 0.7612089, 0.7612003, 0.76078886, 0.761034, 0.76144207, 0.760998, 0.7610872, 0.76138204, 0.76161355, 0.7613718, 0.7619667, 0.76163924, 0.76180387, 0.7617644, 0.7616015, 0.7623422, 0.76197356, 0.761761, 0.7618176, 0.762013, 0.7621742, 0.7620336, 0.7620336, 0.7620919, 0.76180214, 0.7624331, 0.76213646, 0.761917, 0.762121, 0.7620919, 0.76227194, 0.76233363, 0.7620302, 0.7619084, 0.7620713, 0.7619582, 0.76161695, 0.76228905, 0.76246566, 0.76211244, 0.76213473, 0.7623782, 0.76263714], 'precision_1': [0.7259953, 0.7344857, 0.7353114, 0.73609626, 0.7362707, 0.7362139, 0.73630667, 0.7369217, 0.73723024, 0.7373094, 0.73725474, 0.7372577, 0.7372861, 0.73725003, 0.7372699, 0.7372994, 0.73739207, 0.7373479, 0.73742276, 0.73737913, 0.7373719, 0.73741883, 0.73752695, 0.7375307, 0.73782015, 0.7379983, 0.7382069, 0.73824555, 0.7382767, 0.73827755, 0.73828197, 0.7382952, 0.73834133, 0.7383798, 0.738518, 0.73852205, 0.7385087, 0.73851836, 0.73850846, 0.7383559, 0.7383518, 0.73835117, 0.7385031, 0.7385046, 0.7385216, 0.7385739, 0.7385673, 0.73896074, 0.73897463, 0.7389974], 'recall_1': [0.784115, 0.79534495, 0.79718614, 0.79872125, 0.79954743, 0.8006536, 0.8008311, 0.80028987, 0.79989076, 0.79979366, 0.800318, 0.80060196, 0.80076784, 0.80084425, 0.8013929, 0.8010341, 0.80079526, 0.8010028, 0.80102736, 0.80147934, 0.8016883, 0.8015877, 0.80173826, 0.8017569, 0.8016865, 0.8010583, 0.8008485, 0.80078804, 0.80078155, 0.8007936, 0.80078536, 0.8007845, 0.80071723, 0.80071914, 0.800564, 0.8006562, 0.80076295, 0.8007639, 0.8007815, 0.80088645, 0.801037, 0.8010388, 0.80100393, 0.80094534, 0.8007988, 0.800781, 0.80079424, 0.800781, 0.8007386, 0.80072355], 'f1_score': [1.6544413e-05, 2.3509576e-06, 2.4530143e-06, 4.9439127e-06, 3.834968e-06, 4.2663833e-06, 4.9153186e-06, 4.6421774e-06, 5.0058898e-06, 5.7813536e-06, 5.2298947e-06, 4.774513e-06, 4.392123e-06, 4.0664745e-06, 4.420802e-06, 4.339822e-06, 4.5547877e-06, 4.493129e-06, 4.491288e-06, 4.5083384e-06, 5.0009435e-06, 5.10394e-06, 5.428412e-06, 5.612815e-06, 5.522347e-06, 5.4920574e-06, 5.3178123e-06, 5.2158953e-06, 5.032833e-06, 4.9573923e-06, 5.066146e-06, 4.9865134e-06, 4.8744782e-06, 4.853601e-06, 4.847708e-06, 4.7111466e-06, 4.5820498e-06, 4.5722636e-06, 4.5576007e-06, 4.653327e-06, 4.620815e-06, 4.5425427e-06, 4.573407e-06, 4.6081977e-06, 4.6315927e-06, 4.647812e-06, 4.7169774e-06, 4.6760215e-06, 4.689828e-06, 4.675877e-06]}
144811/144811 [==============================] - 2s 13us/step
results evaluated:
[0.4931109100630637, 0.7622280120849609, 0.7388680577278137, 0.800643265247345, 4.698631983046653e-06]
1615/1615 [==============================] - 0s 14us/step
auto results evaluated:
[0.9554294031844045, 0.900928795337677, 0.7388615608215332, 0.8007104992866516, 4.715955128631322e-06]
predictions:
[[0.63038117 0.3696188 ]
 [0.33042526 0.66957474]
 [0.13558953 0.86441046]
 ...
 [0.35826403 0.641736  ]
 [0.43011263 0.5698874 ]
 [0.70238465 0.29761535]]
pred_y:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_12 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_13 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_14 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_15 (Dense)             (None, 2)                 42        
=================================================================
Total params: 1,542
Trainable params: 1,542
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.12072835  0.06224739 -0.1153661  -0.03172075 -0.65880084  2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]
 [ 0.51020808  0.44357032  0.89385305 -0.10968772 -1.73149673 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-1.08169307 -0.52091018 -0.1153661   0.09586157 -0.4437917  -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 1.67501381  0.71605772 -2.23472631  0.15965274  1.93423291 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 0.66551551 -0.0068908  -0.8218195  -0.24435795  1.70160007 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]]
train_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.2177955  -0.21637795 -0.61997567 -0.07424819  0.4144825  -0.37939622
  -0.52680264 -0.48076322 -0.54421604 -0.34394896  2.53222775]
 [-1.2370005  -0.76834096  0.89385305 -0.24435795 -1.08764421 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-0.22750221 -0.31272502  0.28832156  0.03207041 -0.22937001 -0.37939622
  -0.52680264 -0.48076322 -0.54421604  2.90740811 -0.39490919]
 [ 1.17026466 -0.07302646 -0.61997567  0.09586157  0.43269366 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [-0.47987678 -0.61967523 -1.32642908 -0.02699548  2.1310309   2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288332, 294910]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71470, 73341]))
Epoch 1/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5055 - accuracy: 0.7556 - precision_2: 0.7307 - recall_2: 0.7835 - f1_score: 0.0000e+00
Epoch 2/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4990 - accuracy: 0.7597 - precision_2: 0.7364 - recall_2: 0.7908 - f1_score: 7.6111e-06
Epoch 3/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5011 - accuracy: 0.7598 - precision_2: 0.7379 - recall_2: 0.7920 - f1_score: 2.3427e-05
Epoch 4/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4973 - accuracy: 0.7608 - precision_2: 0.7383 - recall_2: 0.7931 - f1_score: 2.9176e-05
Epoch 5/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4968 - accuracy: 0.7607 - precision_2: 0.7385 - recall_2: 0.7940 - f1_score: 2.8885e-05
Epoch 6/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4964 - accuracy: 0.7615 - precision_2: 0.7389 - recall_2: 0.7946 - f1_score: 2.5807e-05
Epoch 7/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4964 - accuracy: 0.7617 - precision_2: 0.7392 - recall_2: 0.7948 - f1_score: 2.6947e-05
Epoch 8/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4964 - accuracy: 0.7616 - precision_2: 0.7392 - recall_2: 0.7954 - f1_score: 2.8968e-05
Epoch 9/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4976 - accuracy: 0.7612 - precision_2: 0.7393 - recall_2: 0.7954 - f1_score: 3.1492e-05
Epoch 10/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4969 - accuracy: 0.7613 - precision_2: 0.7393 - recall_2: 0.7955 - f1_score: 3.6349e-05
Epoch 11/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4961 - accuracy: 0.7615 - precision_2: 0.7395 - recall_2: 0.7952 - f1_score: 3.6763e-05
Epoch 12/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4963 - accuracy: 0.7609 - precision_2: 0.7399 - recall_2: 0.7952 - f1_score: 3.4682e-05
Epoch 13/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4957 - accuracy: 0.7611 - precision_2: 0.7400 - recall_2: 0.7950 - f1_score: 3.3582e-05
Epoch 14/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4964 - accuracy: 0.7618 - precision_2: 0.7402 - recall_2: 0.7949 - f1_score: 3.1450e-05
Epoch 15/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4955 - accuracy: 0.7618 - precision_2: 0.7402 - recall_2: 0.7949 - f1_score: 3.0936e-05
Epoch 16/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4955 - accuracy: 0.7614 - precision_2: 0.7403 - recall_2: 0.7949 - f1_score: 3.0265e-05
Epoch 17/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4959 - accuracy: 0.7614 - precision_2: 0.7404 - recall_2: 0.7947 - f1_score: 2.9378e-05
Epoch 18/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4955 - accuracy: 0.7615 - precision_2: 0.7405 - recall_2: 0.7947 - f1_score: 2.8806e-05
Epoch 19/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4953 - accuracy: 0.7615 - precision_2: 0.7406 - recall_2: 0.7942 - f1_score: 2.8021e-05
Epoch 20/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4953 - accuracy: 0.7620 - precision_2: 0.7409 - recall_2: 0.7942 - f1_score: 2.7019e-05
Epoch 21/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7620 - precision_2: 0.7410 - recall_2: 0.7942 - f1_score: 2.6344e-05
Epoch 22/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4960 - accuracy: 0.7616 - precision_2: 0.7411 - recall_2: 0.7940 - f1_score: 2.9489e-05
Epoch 23/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4954 - accuracy: 0.7622 - precision_2: 0.7411 - recall_2: 0.7942 - f1_score: 3.0792e-05
Epoch 24/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4958 - accuracy: 0.7616 - precision_2: 0.7411 - recall_2: 0.7942 - f1_score: 3.1230e-05
Epoch 25/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4951 - accuracy: 0.7618 - precision_2: 0.7412 - recall_2: 0.7942 - f1_score: 3.1247e-05
Epoch 26/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7618 - precision_2: 0.7412 - recall_2: 0.7942 - f1_score: 3.1271e-05
Epoch 27/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7619 - precision_2: 0.7412 - recall_2: 0.7942 - f1_score: 3.2072e-05
Epoch 28/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7616 - precision_2: 0.7412 - recall_2: 0.7942 - f1_score: 3.2878e-05
Epoch 29/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4949 - accuracy: 0.7617 - precision_2: 0.7412 - recall_2: 0.7940 - f1_score: 3.2622e-05
Epoch 30/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7620 - precision_2: 0.7412 - recall_2: 0.7942 - f1_score: 3.2091e-05
Epoch 31/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4953 - accuracy: 0.7619 - precision_2: 0.7412 - recall_2: 0.7941 - f1_score: 3.2244e-05
Epoch 32/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4954 - accuracy: 0.7613 - precision_2: 0.7413 - recall_2: 0.7940 - f1_score: 3.2410e-05
Epoch 33/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4948 - accuracy: 0.7618 - precision_2: 0.7413 - recall_2: 0.7940 - f1_score: 3.2046e-05
Epoch 34/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7620 - precision_2: 0.7413 - recall_2: 0.7940 - f1_score: 3.1973e-05
Epoch 35/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7618 - precision_2: 0.7413 - recall_2: 0.7940 - f1_score: 3.1290e-05
Epoch 36/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4951 - accuracy: 0.7619 - precision_2: 0.7414 - recall_2: 0.7940 - f1_score: 3.0560e-05
Epoch 37/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4951 - accuracy: 0.7622 - precision_2: 0.7414 - recall_2: 0.7940 - f1_score: 3.0052e-05
Epoch 38/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4953 - accuracy: 0.7620 - precision_2: 0.7414 - recall_2: 0.7940 - f1_score: 2.9894e-05
Epoch 39/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4953 - accuracy: 0.7622 - precision_2: 0.7414 - recall_2: 0.7940 - f1_score: 2.9480e-05
Epoch 40/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4949 - accuracy: 0.7623 - precision_2: 0.7414 - recall_2: 0.7940 - f1_score: 2.9486e-05
Epoch 41/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4951 - accuracy: 0.7627 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.9117e-05
Epoch 42/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7622 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.8714e-05
Epoch 43/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4946 - accuracy: 0.7621 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.8595e-05
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4951 - accuracy: 0.7619 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.8824e-05
Epoch 45/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7619 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.8731e-05
Epoch 46/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7618 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.8860e-05
Epoch 47/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4949 - accuracy: 0.7622 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 3.0059e-05
Epoch 48/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4950 - accuracy: 0.7624 - precision_2: 0.7415 - recall_2: 0.7942 - f1_score: 2.9928e-05
Epoch 49/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4948 - accuracy: 0.7622 - precision_2: 0.7415 - recall_2: 0.7943 - f1_score: 3.0520e-05
Epoch 50/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4954 - accuracy: 0.7622 - precision_2: 0.7415 - recall_2: 0.7946 - f1_score: 3.0737e-05
history.history:
{'loss': [0.5055286012509631, 0.4989797269380576, 0.5011359582274074, 0.497316205609393, 0.4967557703808834, 0.4964115523247269, 0.4964024443914856, 0.4964396632216068, 0.49762409660897383, 0.4968586928639955, 0.4961457419883164, 0.49633217162002763, 0.4957229469149145, 0.4963855571734252, 0.4955313370135028, 0.4954970708425883, 0.49592309769912035, 0.4955092270472054, 0.4953096339903653, 0.4953406623256276, 0.4951686512173416, 0.49602542823545503, 0.4954474597363925, 0.49578748725047295, 0.49505199318710513, 0.49495017983814404, 0.49518354858197666, 0.4951889890869284, 0.4948542127231261, 0.49499697113689445, 0.49534304666655127, 0.49543299991437867, 0.4948268055778482, 0.4952348809825318, 0.4951932452265914, 0.4950856479113395, 0.49507000525226685, 0.4952701925145047, 0.49525400059720276, 0.49492740150560344, 0.49506436731215403, 0.49503781155198523, 0.4946496976543561, 0.4951064750736589, 0.49503066744923696, 0.49503683188815434, 0.49489132277596115, 0.49499775556777276, 0.4948406072516251, 0.4954173716666958], 'accuracy': [0.75558347, 0.7597121, 0.7598475, 0.76084197, 0.7607254, 0.7614575, 0.761701, 0.7616375, 0.7611729, 0.7612552, 0.7614935, 0.76092255, 0.76107347, 0.7617987, 0.761833, 0.76143867, 0.7614489, 0.7615467, 0.7614918, 0.76197356, 0.762013, 0.7615655, 0.7622325, 0.7615964, 0.7617627, 0.76180553, 0.7618639, 0.7615535, 0.76167524, 0.76204216, 0.761941, 0.7612689, 0.7617833, 0.76200104, 0.76181585, 0.7619307, 0.76222736, 0.7620439, 0.76217246, 0.7623422, 0.762728, 0.7622222, 0.76214504, 0.7618999, 0.7619444, 0.761821, 0.7622462, 0.7624331, 0.7621622, 0.76220506], 'precision_2': [0.73074824, 0.7363872, 0.73788387, 0.73828757, 0.7384818, 0.7388983, 0.7391837, 0.7392266, 0.7392647, 0.7393177, 0.7395272, 0.7399385, 0.74000585, 0.7401772, 0.7402334, 0.74025166, 0.7403991, 0.7404569, 0.7405555, 0.7409156, 0.7409705, 0.74113685, 0.74113804, 0.7411415, 0.74115443, 0.741172, 0.74119705, 0.74121064, 0.741212, 0.7412139, 0.7412263, 0.7412698, 0.74126995, 0.7412796, 0.7412841, 0.7414365, 0.74143666, 0.7414375, 0.7414465, 0.7414469, 0.74145037, 0.7414511, 0.74145114, 0.7414513, 0.7414518, 0.7414904, 0.7414922, 0.7414921, 0.7415021, 0.7414929], 'recall_2': [0.78349644, 0.79077977, 0.7920081, 0.79308087, 0.79403424, 0.7946445, 0.7948405, 0.795368, 0.7953615, 0.79546607, 0.79519063, 0.7951668, 0.79497695, 0.79487014, 0.79490453, 0.79488313, 0.79473555, 0.794715, 0.7942245, 0.7942008, 0.7941672, 0.7940145, 0.79415333, 0.79417205, 0.7942027, 0.7941804, 0.7941614, 0.7941514, 0.7940137, 0.7941532, 0.7941072, 0.7940091, 0.7940008, 0.79399955, 0.7939604, 0.7939576, 0.7939585, 0.79396236, 0.7940006, 0.79401314, 0.79415214, 0.7941652, 0.794169, 0.79420555, 0.7942052, 0.7941737, 0.7941975, 0.7942078, 0.7943054, 0.7946456], 'f1_score': [0.0, 7.6110964e-06, 2.3427161e-05, 2.9176073e-05, 2.8884577e-05, 2.580686e-05, 2.6946953e-05, 2.8968496e-05, 3.1491683e-05, 3.6348894e-05, 3.6762853e-05, 3.4682125e-05, 3.3581568e-05, 3.1449596e-05, 3.0935902e-05, 3.0265472e-05, 2.9377921e-05, 2.8806387e-05, 2.8020759e-05, 2.701857e-05, 2.6344003e-05, 2.9489092e-05, 3.0792096e-05, 3.1230036e-05, 3.1247033e-05, 3.1271316e-05, 3.207199e-05, 3.2878244e-05, 3.2621614e-05, 3.2090757e-05, 3.2243905e-05, 3.240969e-05, 3.2046042e-05, 3.197305e-05, 3.129031e-05, 3.0560383e-05, 3.0051604e-05, 2.9894158e-05, 2.9479861e-05, 2.948581e-05, 2.9117411e-05, 2.8714432e-05, 2.859489e-05, 2.882375e-05, 2.8730828e-05, 2.8860244e-05, 3.0058645e-05, 2.9927922e-05, 3.051976e-05, 3.073687e-05]}
144811/144811 [==============================] - 2s 14us/step
results evaluated:
[0.4903135287943699, 0.7638853192329407, 0.7416563630104065, 0.7944512367248535, 3.076072971452959e-05]
1615/1615 [==============================] - 0s 15us/step
auto results evaluated:
[0.8084433165894268, 0.900928795337677, 0.7416433691978455, 0.7944726347923279, 3.367896351846866e-05]
predictions:
[[0.787426   0.21257408]
 [0.19244395 0.80755603]
 [0.13954636 0.8604536 ]
 ...
 [0.39977005 0.6002299 ]
 [0.46770746 0.5322925 ]
 [0.8079368  0.19206324]]
pred_y:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_17 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_18 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_19 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_20 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_21 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_22 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_23 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_24 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_25 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_26 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_27 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_28 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_29 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_30 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_31 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_32 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_33 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_34 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_35 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_36 (Dense)             (None, 2)                 42        
=================================================================
Total params: 8,262
Trainable params: 8,262
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.12072835  0.06224739 -0.1153661  -0.03172075 -0.65880084  2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]
 [ 0.51020808  0.44357032  0.89385305 -0.10968772 -1.73149673 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-1.08169307 -0.52091018 -0.1153661   0.09586157 -0.4437917  -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 1.67501381  0.71605772 -2.23472631  0.15965274  1.93423291 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 0.66551551 -0.0068908  -0.8218195  -0.24435795  1.70160007 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]]
train_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.2177955  -0.21637795 -0.61997567 -0.07424819  0.4144825  -0.37939622
  -0.52680264 -0.48076322 -0.54421604 -0.34394896  2.53222775]
 [-1.2370005  -0.76834096  0.89385305 -0.24435795 -1.08764421 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-0.22750221 -0.31272502  0.28832156  0.03207041 -0.22937001 -0.37939622
  -0.52680264 -0.48076322 -0.54421604  2.90740811 -0.39490919]
 [ 1.17026466 -0.07302646 -0.61997567  0.09586157  0.43269366 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [-0.47987678 -0.61967523 -1.32642908 -0.02699548  2.1310309   2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288332, 294910]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71470, 73341]))
Epoch 1/50
583242/583242 [==============================] - 41s 70us/step - loss: 3.9955 - accuracy: 0.7037 - precision_3: 0.6927 - recall_3: 0.7235 - f1_score: 0.0024           
Epoch 2/50
583242/583242 [==============================] - 45s 77us/step - loss: 16.5970 - accuracy: 0.5809 - precision_3: 0.6626 - recall_3: 0.6896 - f1_score: 0.0101
Epoch 3/50
583242/583242 [==============================] - 45s 77us/step - loss: 165.1296 - accuracy: 0.5557 - precision_3: 0.6167 - recall_3: 0.6267 - f1_score: 0.0184
Epoch 4/50
583242/583242 [==============================] - 45s 78us/step - loss: 279.7377 - accuracy: 0.5180 - precision_3: 0.5924 - recall_3: 0.6124 - f1_score: 0.0412
Epoch 5/50
583242/583242 [==============================] - 46s 78us/step - loss: 6084.0548 - accuracy: 0.5125 - precision_3: 0.5749 - recall_3: 0.5891 - f1_score: 0.0705
Epoch 6/50
583242/583242 [==============================] - 45s 78us/step - loss: 23268.7319 - accuracy: 0.5103 - precision_3: 0.5626 - recall_3: 0.5731 - f1_score: 0.1220
Epoch 7/50
583242/583242 [==============================] - 45s 78us/step - loss: 15684.1357 - accuracy: 0.5132 - precision_3: 0.5535 - recall_3: 0.5619 - f1_score: 0.1539
Epoch 8/50
583242/583242 [==============================] - 45s 78us/step - loss: 332265.7646 - accuracy: 0.5169 - precision_3: 0.5481 - recall_3: 0.5556 - f1_score: 0.2087
Epoch 9/50
583242/583242 [==============================] - 45s 78us/step - loss: 6075.8299 - accuracy: 0.5221 - precision_3: 0.5444 - recall_3: 0.5503 - f1_score: 0.2485
Epoch 10/50
583242/583242 [==============================] - 45s 78us/step - loss: 352622.0969 - accuracy: 0.5294 - precision_3: 0.5418 - recall_3: 0.5464 - f1_score: 0.2541
Epoch 11/50
583242/583242 [==============================] - 45s 78us/step - loss: 3233.7115 - accuracy: 0.5039 - precision_3: 0.5387 - recall_3: 0.5423 - f1_score: 0.2695
Epoch 12/50
583242/583242 [==============================] - 46s 78us/step - loss: 300694.4442 - accuracy: 0.5159 - precision_3: 0.5358 - recall_3: 0.5387 - f1_score: 0.2890
Epoch 13/50
583242/583242 [==============================] - 45s 77us/step - loss: 66722.6246 - accuracy: 0.5167 - precision_3: 0.5339 - recall_3: 0.5360 - f1_score: 0.3091
Epoch 14/50
583242/583242 [==============================] - 43s 74us/step - loss: 190452.7951 - accuracy: 0.5009 - precision_3: 0.5315 - recall_3: 0.5338 - f1_score: 0.3254
Epoch 15/50
583242/583242 [==============================] - 37s 64us/step - loss: 27958.0619 - accuracy: 0.5156 - precision_3: 0.5296 - recall_3: 0.5325 - f1_score: 0.3389
Epoch 16/50
583242/583242 [==============================] - 41s 70us/step - loss: 5559232.1929 - accuracy: 0.5225 - precision_3: 0.5286 - recall_3: 0.5320 - f1_score: 0.3524
Epoch 17/50
583242/583242 [==============================] - 45s 76us/step - loss: 615819.1032 - accuracy: 0.5252 - precision_3: 0.5277 - recall_3: 0.5311 - f1_score: 0.3653
Epoch 18/50
583242/583242 [==============================] - 45s 76us/step - loss: 112551.6930 - accuracy: 0.5146 - precision_3: 0.5271 - recall_3: 0.5301 - f1_score: 0.3764
Epoch 19/50
583242/583242 [==============================] - 45s 77us/step - loss: 41157966.2056 - accuracy: 0.5099 - precision_3: 0.5260 - recall_3: 0.5249 - f1_score: 0.3852
Epoch 20/50
583242/583242 [==============================] - 45s 78us/step - loss: 5404132.9656 - accuracy: 0.5098 - precision_3: 0.5251 - recall_3: 0.5217 - f1_score: 0.3929
Epoch 21/50
583242/583242 [==============================] - 45s 78us/step - loss: 2060524.6741 - accuracy: 0.5049 - precision_3: 0.5241 - recall_3: 0.5193 - f1_score: 0.3996
Epoch 22/50
583242/583242 [==============================] - 45s 78us/step - loss: 9332769.1056 - accuracy: 0.5123 - precision_3: 0.5232 - recall_3: 0.5171 - f1_score: 0.4056
Epoch 23/50
583242/583242 [==============================] - 46s 78us/step - loss: 57238583.1864 - accuracy: 0.5118 - precision_3: 0.5226 - recall_3: 0.5143 - f1_score: 0.4111
Epoch 24/50
583242/583242 [==============================] - 45s 77us/step - loss: 105359114.8224 - accuracy: 0.5150 - precision_3: 0.5220 - recall_3: 0.5125 - f1_score: 0.4163
Epoch 25/50
583242/583242 [==============================] - 46s 78us/step - loss: 2887773.1438 - accuracy: 0.5232 - precision_3: 0.5219 - recall_3: 0.5112 - f1_score: 0.4213
Epoch 26/50
583242/583242 [==============================] - 45s 78us/step - loss: 1292716.4113 - accuracy: 0.5106 - precision_3: 0.5211 - recall_3: 0.5113 - f1_score: 0.4254
Epoch 27/50
583242/583242 [==============================] - 46s 78us/step - loss: 220692992.1606 - accuracy: 0.5112 - precision_3: 0.5208 - recall_3: 0.5107 - f1_score: 0.4294
Epoch 28/50
583242/583242 [==============================] - 45s 77us/step - loss: 329946.2401 - accuracy: 0.5070 - precision_3: 0.5200 - recall_3: 0.5106 - f1_score: 0.4326
Epoch 29/50
583242/583242 [==============================] - 43s 73us/step - loss: 6828951.3795 - accuracy: 0.5177 - precision_3: 0.5196 - recall_3: 0.5101 - f1_score: 0.4356
Epoch 30/50
583242/583242 [==============================] - 38s 64us/step - loss: 10012295.4526 - accuracy: 0.5165 - precision_3: 0.5194 - recall_3: 0.5104 - f1_score: 0.4389
Epoch 31/50
583242/583242 [==============================] - 41s 71us/step - loss: 1984478.1749 - accuracy: 0.5309 - precision_3: 0.5193 - recall_3: 0.5092 - f1_score: 0.4419
Epoch 32/50
583242/583242 [==============================] - 44s 76us/step - loss: 39847723.7745 - accuracy: 0.5347 - precision_3: 0.5195 - recall_3: 0.5087 - f1_score: 0.4451
Epoch 33/50
583242/583242 [==============================] - 45s 77us/step - loss: 107803460.8936 - accuracy: 0.5291 - precision_3: 0.5199 - recall_3: 0.5086 - f1_score: 0.4482
Epoch 34/50
583242/583242 [==============================] - 45s 78us/step - loss: 114807950.2200 - accuracy: 0.5363 - precision_3: 0.5204 - recall_3: 0.5090 - f1_score: 0.4512
Epoch 35/50
583242/583242 [==============================] - 45s 78us/step - loss: 5995680.3406 - accuracy: 0.5125 - precision_3: 0.5201 - recall_3: 0.5092 - f1_score: 0.4534
Epoch 36/50
583242/583242 [==============================] - 46s 78us/step - loss: 2899512.4834 - accuracy: 0.5201 - precision_3: 0.5196 - recall_3: 0.5095 - f1_score: 0.4552
Epoch 37/50
583242/583242 [==============================] - 45s 78us/step - loss: 27492814.0350 - accuracy: 0.5324 - precision_3: 0.5198 - recall_3: 0.5102 - f1_score: 0.4575
Epoch 38/50
583242/583242 [==============================] - 46s 78us/step - loss: 32569225908.4551 - accuracy: 0.4929 - precision_3: 0.5195 - recall_3: 0.5099 - f1_score: 0.4591
Epoch 39/50
583242/583242 [==============================] - 46s 78us/step - loss: 1570624870.1125 - accuracy: 0.5055 - precision_3: 0.5188 - recall_3: 0.5090 - f1_score: 0.4603
Epoch 40/50
583242/583242 [==============================] - 45s 78us/step - loss: 452908505.4724 - accuracy: 0.5133 - precision_3: 0.5186 - recall_3: 0.5085 - f1_score: 0.4616
Epoch 41/50
583242/583242 [==============================] - 46s 78us/step - loss: 73861083.5196 - accuracy: 0.5144 - precision_3: 0.5182 - recall_3: 0.5083 - f1_score: 0.4630
Epoch 42/50
583242/583242 [==============================] - 45s 78us/step - loss: 486073786.4095 - accuracy: 0.5109 - precision_3: 0.5180 - recall_3: 0.5080 - f1_score: 0.4644
Epoch 43/50
583242/583242 [==============================] - 45s 77us/step - loss: 270112.4049 - accuracy: 0.5070 - precision_3: 0.5176 - recall_3: 0.5075 - f1_score: 0.4655
Epoch 44/50
583242/583242 [==============================] - 42s 72us/step - loss: 5699812.5480 - accuracy: 0.5043 - precision_3: 0.5171 - recall_3: 0.5077 - f1_score: 0.4665
Epoch 45/50
583242/583242 [==============================] - 37s 64us/step - loss: 45815816.3358 - accuracy: 0.5036 - precision_3: 0.5167 - recall_3: 0.5070 - f1_score: 0.4674
Epoch 46/50
583242/583242 [==============================] - 42s 73us/step - loss: 53473289.9130 - accuracy: 0.5131 - precision_3: 0.5165 - recall_3: 0.5062 - f1_score: 0.4683
Epoch 47/50
583242/583242 [==============================] - 45s 76us/step - loss: 4678294252.8802 - accuracy: 0.5183 - precision_3: 0.5163 - recall_3: 0.5060 - f1_score: 0.4694
Epoch 48/50
583242/583242 [==============================] - 45s 77us/step - loss: 2893004.4864 - accuracy: 0.5124 - precision_3: 0.5162 - recall_3: 0.5058 - f1_score: 0.4705
Epoch 49/50
583242/583242 [==============================] - 46s 78us/step - loss: 2757151753.2296 - accuracy: 0.5214 - precision_3: 0.5161 - recall_3: 0.5057 - f1_score: 0.4715
Epoch 50/50
583242/583242 [==============================] - 45s 78us/step - loss: 147877701.0482 - accuracy: 0.5202 - precision_3: 0.5162 - recall_3: 0.5050 - f1_score: 0.4726
history.history:
{'loss': [3.9954615698124596, 16.596983950998315, 165.12957708819275, 279.73770124081, 6084.054811430707, 23268.731879951087, 15684.135667330027, 332265.7645616773, 6075.82988350166, 352622.09694775456, 3233.7114538832398, 300694.44418390596, 66722.62459697746, 190452.79507852293, 27958.0618791758, 5559232.192892495, 615819.103226319, 112551.69300268353, 41157966.20557486, 5404132.965571684, 2060524.6740602562, 9332769.10559241, 57238583.18642828, 105359114.82243118, 2887773.1438409877, 1292716.4112975462, 220692992.1606119, 329946.24009245296, 6828951.37950554, 10012295.45256456, 1984478.174885038, 39847723.77447993, 107803460.89362742, 114807950.22001657, 5995680.340616746, 2899512.4833848206, 27492814.0349584, 32569225908.455147, 1570624870.1125407, 452908505.4724159, 73861083.51959617, 486073786.40954614, 270112.4048638123, 5699812.547977802, 45815816.335831195, 53473289.91303731, 4678294252.880211, 2893004.4863921865, 2757151753.229591, 147877701.0481933], 'accuracy': [0.7036719, 0.5808824, 0.55569214, 0.5179531, 0.5124854, 0.5102993, 0.51320034, 0.5168832, 0.5221212, 0.52944916, 0.5038972, 0.51590765, 0.5167118, 0.50094646, 0.5155716, 0.5224881, 0.5251765, 0.51459944, 0.5098655, 0.50983983, 0.5048779, 0.5123191, 0.51181155, 0.5149835, 0.52315336, 0.51060283, 0.5112183, 0.5070091, 0.5177405, 0.51654375, 0.5309306, 0.53474027, 0.52913713, 0.53628683, 0.5124751, 0.52006197, 0.5323982, 0.49291375, 0.5055037, 0.5132569, 0.51436967, 0.5109114, 0.5070245, 0.50425893, 0.5035954, 0.5130769, 0.51826173, 0.5124151, 0.52140105, 0.52024376], 'precision_3': [0.6927495, 0.662613, 0.6167451, 0.59235764, 0.5749385, 0.56256926, 0.5534988, 0.548117, 0.54438055, 0.5418247, 0.5387281, 0.53583586, 0.5338864, 0.5315194, 0.52960384, 0.5286394, 0.5277375, 0.5270796, 0.5259871, 0.5251286, 0.5240994, 0.5231707, 0.5225905, 0.5219571, 0.52187145, 0.52110803, 0.52080214, 0.5200161, 0.5195571, 0.5194125, 0.51932156, 0.51949555, 0.519922, 0.52036077, 0.5200577, 0.51962143, 0.5198421, 0.5194704, 0.51877093, 0.5185551, 0.5181666, 0.5179747, 0.51763654, 0.5171353, 0.51671696, 0.5165247, 0.5163103, 0.51619065, 0.51608187, 0.5161585], 'recall_3': [0.7234754, 0.6896322, 0.62674826, 0.6124103, 0.5890593, 0.5731435, 0.5618844, 0.5555615, 0.5502533, 0.54643667, 0.54229444, 0.5387246, 0.5359504, 0.5338029, 0.5325326, 0.53195065, 0.53107125, 0.53010947, 0.5249137, 0.52170736, 0.51930577, 0.5170701, 0.51426864, 0.51250494, 0.5112119, 0.5112996, 0.5106565, 0.51057553, 0.51007307, 0.5103573, 0.5092325, 0.5087178, 0.5086385, 0.50899404, 0.5092103, 0.509471, 0.5102419, 0.50986737, 0.509025, 0.50848806, 0.50834227, 0.5079556, 0.50751144, 0.50767106, 0.50699455, 0.50624585, 0.50595397, 0.50584763, 0.5057494, 0.50502044], 'f1_score': [0.002445526, 0.010081488, 0.018359717, 0.04122652, 0.070547365, 0.12203576, 0.15392484, 0.20874786, 0.24847502, 0.25413567, 0.26952147, 0.2889737, 0.30910072, 0.32544535, 0.3388526, 0.35237274, 0.36532238, 0.37642688, 0.3852291, 0.3929118, 0.3995764, 0.4055779, 0.4111425, 0.41626585, 0.4213296, 0.42543805, 0.42938814, 0.43258476, 0.43561482, 0.4388861, 0.44192022, 0.44506386, 0.44819084, 0.45119646, 0.45340413, 0.45519087, 0.45751595, 0.4590715, 0.4602631, 0.46161428, 0.46300507, 0.46435508, 0.4654897, 0.46646973, 0.46737087, 0.46828863, 0.46944436, 0.4705111, 0.47148144, 0.47255176]}
144811/144811 [==============================] - 3s 21us/step
results evaluated:
[396861.3024854379, 0.5064187049865723, 0.5161128640174866, 0.5038082003593445, 0.4731348752975464]
1615/1615 [==============================] - 0s 20us/step
auto results evaluated:
[79065.16501547987, 0.900928795337677, 0.5161062479019165, 0.5025603175163269, 0.47321030497550964]
predictions:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [1. 0.]
 [1. 0.]]
pred_y:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_37 (Dense)             (None, 40)                480       
_________________________________________________________________
dense_38 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_39 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_40 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_41 (Dense)             (None, 2)                 82        
=================================================================
Total params: 5,482
Trainable params: 5,482
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-0.12072835  0.06224739 -0.1153661  -0.03172075 -0.65880084  2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]
 [ 0.51020808  0.44357032  0.89385305 -0.10968772 -1.73149673 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-1.08169307 -0.52091018 -0.1153661   0.09586157 -0.4437917  -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 1.67501381  0.71605772 -2.23472631  0.15965274  1.93423291 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [ 0.66551551 -0.0068908  -0.8218195  -0.24435795  1.70160007 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]]
train_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.2177955  -0.21637795 -0.61997567 -0.07424819  0.4144825  -0.37939622
  -0.52680264 -0.48076322 -0.54421604 -0.34394896  2.53222775]
 [-1.2370005  -0.76834096  0.89385305 -0.24435795 -1.08764421 -0.37939622
  -0.52680264 -0.48076322  1.83750554 -0.34394896 -0.39490919]
 [-0.22750221 -0.31272502  0.28832156  0.03207041 -0.22937001 -0.37939622
  -0.52680264 -0.48076322 -0.54421604  2.90740811 -0.39490919]
 [ 1.17026466 -0.07302646 -0.61997567  0.09586157  0.43269366 -0.37939622
  -0.52680264  2.08002602 -0.54421604 -0.34394896 -0.39490919]
 [-0.47987678 -0.61967523 -1.32642908 -0.02699548  2.1310309   2.63576693
  -0.52680264 -0.48076322 -0.54421604 -0.34394896 -0.39490919]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288332, 294910]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71470, 73341]))
Epoch 1/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.5299 - accuracy: 0.7524 - precision_4: 0.7288 - recall_4: 0.7800 - f1_score: 2.1364e-04
Epoch 2/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.5054 - accuracy: 0.7573 - precision_4: 0.7327 - recall_4: 0.7910 - f1_score: 2.1858e-04
Epoch 3/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.5321 - accuracy: 0.7566 - precision_4: 0.7335 - recall_4: 0.7927 - f1_score: 1.7713e-04
Epoch 4/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5044 - accuracy: 0.7575 - precision_4: 0.7342 - recall_4: 0.7930 - f1_score: 1.6872e-04
Epoch 5/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.5043 - accuracy: 0.7579 - precision_4: 0.7351 - recall_4: 0.7923 - f1_score: 1.6873e-04
Epoch 6/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.5026 - accuracy: 0.7592 - precision_4: 0.7357 - recall_4: 0.7920 - f1_score: 1.5936e-04
Epoch 7/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5057 - accuracy: 0.7588 - precision_4: 0.7363 - recall_4: 0.7922 - f1_score: 1.5893e-04
Epoch 8/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.5103 - accuracy: 0.7588 - precision_4: 0.7365 - recall_4: 0.7920 - f1_score: 1.8131e-04
Epoch 9/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5002 - accuracy: 0.7597 - precision_4: 0.7370 - recall_4: 0.7922 - f1_score: 1.8845e-04
Epoch 10/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4997 - accuracy: 0.7599 - precision_4: 0.7371 - recall_4: 0.7927 - f1_score: 1.9567e-04
Epoch 11/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5021 - accuracy: 0.7600 - precision_4: 0.7372 - recall_4: 0.7930 - f1_score: 1.8830e-04
Epoch 12/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4993 - accuracy: 0.7602 - precision_4: 0.7373 - recall_4: 0.7930 - f1_score: 1.9361e-04
Epoch 13/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4998 - accuracy: 0.7600 - precision_4: 0.7375 - recall_4: 0.7930 - f1_score: 1.8778e-04
Epoch 14/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4999 - accuracy: 0.7602 - precision_4: 0.7376 - recall_4: 0.7932 - f1_score: 1.8220e-04
Epoch 15/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4988 - accuracy: 0.7603 - precision_4: 0.7380 - recall_4: 0.7932 - f1_score: 1.7592e-04
Epoch 16/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4987 - accuracy: 0.7606 - precision_4: 0.7381 - recall_4: 0.7937 - f1_score: 1.7049e-04
Epoch 17/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4989 - accuracy: 0.7605 - precision_4: 0.7382 - recall_4: 0.7939 - f1_score: 1.6713e-04
Epoch 18/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4987 - accuracy: 0.7605 - precision_4: 0.7381 - recall_4: 0.7939 - f1_score: 1.6549e-04
Epoch 19/50
583242/583242 [==============================] - 18s 30us/step - loss: 0.5017 - accuracy: 0.7602 - precision_4: 0.7382 - recall_4: 0.7942 - f1_score: 1.6180e-04
Epoch 20/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4983 - accuracy: 0.7609 - precision_4: 0.7382 - recall_4: 0.7946 - f1_score: 1.6298e-04
Epoch 21/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.5010 - accuracy: 0.7598 - precision_4: 0.7381 - recall_4: 0.7949 - f1_score: 1.6125e-04
Epoch 22/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.5040 - accuracy: 0.7599 - precision_4: 0.7381 - recall_4: 0.7949 - f1_score: 1.6694e-04
Epoch 23/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4983 - accuracy: 0.7607 - precision_4: 0.7380 - recall_4: 0.7951 - f1_score: 1.7602e-04
Epoch 24/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4984 - accuracy: 0.7605 - precision_4: 0.7381 - recall_4: 0.7951 - f1_score: 1.7657e-04
Epoch 25/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4985 - accuracy: 0.7604 - precision_4: 0.7382 - recall_4: 0.7951 - f1_score: 1.7560e-04
Epoch 26/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4984 - accuracy: 0.7607 - precision_4: 0.7382 - recall_4: 0.7952 - f1_score: 1.7242e-04
Epoch 27/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5025 - accuracy: 0.7601 - precision_4: 0.7383 - recall_4: 0.7952 - f1_score: 1.6994e-04
Epoch 28/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5000 - accuracy: 0.7604 - precision_4: 0.7383 - recall_4: 0.7952 - f1_score: 1.6948e-04
Epoch 29/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4986 - accuracy: 0.7604 - precision_4: 0.7383 - recall_4: 0.7952 - f1_score: 1.6624e-04
Epoch 30/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4994 - accuracy: 0.7603 - precision_4: 0.7383 - recall_4: 0.7951 - f1_score: 1.6517e-04
Epoch 31/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4987 - accuracy: 0.7602 - precision_4: 0.7383 - recall_4: 0.7952 - f1_score: 1.6243e-04
Epoch 32/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4993 - accuracy: 0.7607 - precision_4: 0.7383 - recall_4: 0.7952 - f1_score: 1.6108e-04
Epoch 33/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4985 - accuracy: 0.7600 - precision_4: 0.7384 - recall_4: 0.7951 - f1_score: 1.6006e-04
Epoch 34/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4998 - accuracy: 0.7603 - precision_4: 0.7385 - recall_4: 0.7951 - f1_score: 1.5803e-04
Epoch 35/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4976 - accuracy: 0.7604 - precision_4: 0.7385 - recall_4: 0.7950 - f1_score: 1.5891e-04
Epoch 36/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5007 - accuracy: 0.7605 - precision_4: 0.7385 - recall_4: 0.7950 - f1_score: 1.5876e-04
Epoch 37/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4998 - accuracy: 0.7604 - precision_4: 0.7386 - recall_4: 0.7950 - f1_score: 1.5610e-04
Epoch 38/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4978 - accuracy: 0.7605 - precision_4: 0.7386 - recall_4: 0.7949 - f1_score: 1.5383e-04
Epoch 39/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4978 - accuracy: 0.7607 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5274e-04
Epoch 40/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4989 - accuracy: 0.7605 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5073e-04
Epoch 41/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4984 - accuracy: 0.7605 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.4881e-04
Epoch 42/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4977 - accuracy: 0.7602 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.4728e-04
Epoch 43/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4997 - accuracy: 0.7599 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.4920e-04
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5121 - accuracy: 0.7598 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.4996e-04
Epoch 45/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4982 - accuracy: 0.7599 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5483e-04
Epoch 46/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4983 - accuracy: 0.7608 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5887e-04
Epoch 47/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4978 - accuracy: 0.7605 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.6027e-04
Epoch 48/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4982 - accuracy: 0.7606 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5939e-04
Epoch 49/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4977 - accuracy: 0.7607 - precision_4: 0.7390 - recall_4: 0.7949 - f1_score: 1.5894e-04
Epoch 50/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4981 - accuracy: 0.7605 - precision_4: 0.7391 - recall_4: 0.7949 - f1_score: 1.5804e-04
history.history:
{'loss': [0.5298514885241176, 0.5054481045591965, 0.5321408503523529, 0.5044487694399914, 0.5043492822278971, 0.5026011581522619, 0.5056545872173082, 0.5102941964585905, 0.5001802234285009, 0.49967833817917556, 0.5020574515235274, 0.49930373804581535, 0.4998488847846244, 0.49994192727927445, 0.49878019132545165, 0.49872777596809514, 0.4988898472536413, 0.49866661335702256, 0.5017346847437959, 0.49832341498063126, 0.5010130161082975, 0.5039908434987985, 0.4982519000743624, 0.4983943387451158, 0.4985391586086638, 0.4984230090791415, 0.5024734650112818, 0.5000093857192227, 0.49855359770964214, 0.499350193114266, 0.49873984710894703, 0.4992778453440655, 0.49845986273850545, 0.49978211548335916, 0.49762369167751536, 0.5006690321064658, 0.4998175864562903, 0.4977820447558235, 0.4978414804374828, 0.49887959947326355, 0.4984336335546465, 0.49774052801428603, 0.4997149284735543, 0.5120634432885405, 0.4981747332761636, 0.49833773066708187, 0.49776658717127703, 0.49821009027507074, 0.4977217631914594, 0.4981412122049022], 'accuracy': [0.7523549, 0.757334, 0.7566499, 0.75751746, 0.75786895, 0.7591754, 0.7587828, 0.7588085, 0.75966924, 0.7598921, 0.76003784, 0.76015276, 0.759983, 0.76024705, 0.76026076, 0.76055396, 0.7605145, 0.7605059, 0.76020247, 0.7609071, 0.759779, 0.7598527, 0.7607271, 0.76047504, 0.7603979, 0.7607185, 0.76008415, 0.76035506, 0.7604288, 0.76034135, 0.7602402, 0.76066023, 0.760019, 0.7603173, 0.7603722, 0.7604562, 0.76036364, 0.7605334, 0.7606808, 0.76050425, 0.7604922, 0.7602076, 0.7599418, 0.75980294, 0.7598921, 0.7608317, 0.7605025, 0.7606311, 0.76066023, 0.7604699], 'precision_4': [0.7287912, 0.7326844, 0.73346835, 0.73424786, 0.73507255, 0.73571503, 0.7362555, 0.7364988, 0.73700285, 0.73712873, 0.7372334, 0.73730093, 0.73751026, 0.7376288, 0.7379869, 0.73806334, 0.7381923, 0.7380574, 0.73820984, 0.738172, 0.738061, 0.7380773, 0.73804647, 0.7381064, 0.73821914, 0.7382191, 0.73826265, 0.73826635, 0.738281, 0.7382861, 0.73829615, 0.73833716, 0.7383509, 0.7385031, 0.7385082, 0.7385189, 0.73856276, 0.73857456, 0.7389605, 0.7389608, 0.7389645, 0.7389749, 0.7389758, 0.738996, 0.7390167, 0.73902, 0.7390204, 0.73903054, 0.7390313, 0.73906213], 'recall_4': [0.7800014, 0.7910257, 0.7926985, 0.79295117, 0.79227805, 0.7920442, 0.79218096, 0.79201114, 0.79220694, 0.79270643, 0.79297197, 0.79302484, 0.7930318, 0.7931892, 0.79322743, 0.79366434, 0.79387116, 0.79394513, 0.79421335, 0.7946456, 0.79485697, 0.79492074, 0.7951215, 0.7951461, 0.79515, 0.79518485, 0.7951918, 0.7951903, 0.79517394, 0.7951458, 0.79517764, 0.79515326, 0.7951428, 0.7950918, 0.7949898, 0.7949782, 0.79496646, 0.7949262, 0.7949351, 0.7949383, 0.79493594, 0.794938, 0.79493344, 0.7949252, 0.7949219, 0.79491764, 0.7949217, 0.7949219, 0.7949229, 0.79492277], 'f1_score': [0.00021363907, 0.00021857569, 0.00017713313, 0.00016872103, 0.00016873342, 0.00015935783, 0.00015893235, 0.00018131052, 0.00018845414, 0.0001956676, 0.00018829611, 0.00019361384, 0.0001877836, 0.00018220201, 0.00017592136, 0.0001704888, 0.00016712748, 0.00016548873, 0.00016179592, 0.00016297596, 0.00016124686, 0.00016693563, 0.00017601914, 0.00017657307, 0.00017560479, 0.00017242426, 0.00016994293, 0.00016947713, 0.00016623634, 0.00016517406, 0.00016243279, 0.00016108344, 0.0001600562, 0.00015802683, 0.00015890722, 0.000158756, 0.00015610289, 0.00015383195, 0.0001527388, 0.00015072853, 0.00014881026, 0.000147284, 0.00014920188, 0.00014995507, 0.00015482945, 0.00015886816, 0.00016026727, 0.00015938844, 0.0001589364, 0.0001580388]}
144811/144811 [==============================] - 2s 13us/step
results evaluated:
[0.49208417838447155, 0.7619587182998657, 0.7390128374099731, 0.7950779795646667, 0.000157430418767035]
1615/1615 [==============================] - 0s 12us/step
auto results evaluated:
[13.640436759452701, 0.900928795337677, 0.738939642906189, 0.7951356768608093, 0.00020649551879614592]
predictions:
[[0.73376983 0.26623017]
 [0.2919914  0.70800865]
 [0.1769022  0.8230978 ]
 ...
 [0.35120746 0.64879256]
 [0.36006382 0.6399362 ]
 [0.7482359  0.2517641 ]]
pred_y:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 ...
 [0. 1.]
 [0. 1.]
 [1. 0.]]
(base) lawn-128-61-35-176:NeuralNetwork nathanflorez$ 
