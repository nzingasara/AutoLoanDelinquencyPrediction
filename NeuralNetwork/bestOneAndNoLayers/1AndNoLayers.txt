_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1)                 12        
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 4         
=================================================================
Total params: 16
Trainable params: 16
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.12681228e-01 -2.82785224e-02  1.69966182e+00 -1.78511329e-01
  -1.73242798e+00 -3.79585434e-01 -5.26130640e-01  2.08200430e+00
  -5.44337784e-01 -3.44048699e-01 -3.95721002e-01]
 [-5.76705937e-01  2.13495190e-01  8.92739043e-01 -3.62576248e-04
  -1.28461351e+00 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
   1.83709460e+00 -3.44048699e-01 -3.95721002e-01]
 [ 2.29433287e-01 -4.15227768e-01 -1.02370256e+00  1.61158960e-01
   4.14378153e-01 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
  -5.44337784e-01 -3.44048699e-01  2.52703292e+00]
 [-6.83543665e-01 -5.26719822e-01  1.09446974e+00 -2.71148681e-01
  -1.30283168e+00  2.63445304e+00 -5.26130640e-01 -4.80306404e-01
  -5.44337784e-01 -3.44048699e-01 -3.95721002e-01]
 [-8.00093915e-01 -1.97914994e-01  9.93604391e-01  9.22747753e-02
  -1.73242798e+00 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
   1.83709460e+00 -3.44048699e-01 -3.95721002e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [1. 0.]]
test_X BEFORE fitting:
[[-0.80009391 -0.30116506  0.38841231  0.16353428  0.41437815 -0.37958543
  -0.52613064  2.0820043  -0.54433778 -0.3440487  -0.395721  ]
 [-1.18859475 -0.55634856  0.28754696 -0.14288158 -0.01521815 -0.37958543
   1.90066863 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]
 [-1.33428256 -0.84558657  2.40571926 -0.15000753 -1.30283168 -0.37958543
  -0.52613064  2.0820043  -0.54433778 -0.3440487  -0.395721  ]
 [-1.54795801 -0.92371316 -0.01504908 -0.2450202   0.21750434 -0.37958543
   1.90066863 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]
 [-1.27600743 -0.75487173  2.40571926 -0.27827463 -1.30283168  2.63445304
  -0.52613064 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288372, 294870]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71405, 73406]))
Epoch 1/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5840 - accuracy: 0.7046 - precision: 0.6939 - recall: 0.7000
Epoch 2/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5825 - accuracy: 0.7066 - precision: 0.6978 - recall: 0.7125
Epoch 3/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7065 - precision: 0.6982 - recall: 0.7135
Epoch 4/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5825 - accuracy: 0.7056 - precision: 0.6982 - recall: 0.7135
Epoch 5/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7066 - precision: 0.6982 - recall: 0.7136
Epoch 6/50
583242/583242 [==============================] - 17s 28us/step - loss: 0.5824 - accuracy: 0.7062 - precision: 0.6983 - recall: 0.7136
Epoch 7/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5826 - accuracy: 0.7057 - precision: 0.6983 - recall: 0.7137
Epoch 8/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5823 - accuracy: 0.7066 - precision: 0.6983 - recall: 0.7136
Epoch 9/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7060 - precision: 0.6983 - recall: 0.7138
Epoch 10/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5823 - accuracy: 0.7067 - precision: 0.6983 - recall: 0.7138
Epoch 11/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7064 - precision: 0.6985 - recall: 0.7139
Epoch 12/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5825 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7138
Epoch 13/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5824 - accuracy: 0.7064 - precision: 0.6984 - recall: 0.7139
Epoch 14/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5823 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 15/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.5824 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 16/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.5825 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7138
Epoch 17/50
583242/583242 [==============================] - 18s 31us/step - loss: 0.5825 - accuracy: 0.7061 - precision: 0.6985 - recall: 0.7139
Epoch 18/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7064 - precision: 0.6985 - recall: 0.7139
Epoch 19/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5826 - accuracy: 0.7060 - precision: 0.6985 - recall: 0.7139
Epoch 20/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5825 - accuracy: 0.7067 - precision: 0.6985 - recall: 0.7138
Epoch 21/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7063 - precision: 0.6985 - recall: 0.7139
Epoch 22/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5825 - accuracy: 0.7063 - precision: 0.6985 - recall: 0.7139
Epoch 23/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.5825 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7138
Epoch 24/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.5824 - accuracy: 0.7068 - precision: 0.6985 - recall: 0.7139
Epoch 25/50
583242/583242 [==============================] - 30s 51us/step - loss: 0.5824 - accuracy: 0.7060 - precision: 0.6985 - recall: 0.7139
Epoch 26/50
583242/583242 [==============================] - 23s 40us/step - loss: 0.5823 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
Epoch 27/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.5824 - accuracy: 0.7063 - precision: 0.6985 - recall: 0.7139
Epoch 28/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 29/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5823 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 30/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7063 - precision: 0.6985 - recall: 0.7139
Epoch 31/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
Epoch 32/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5825 - accuracy: 0.7064 - precision: 0.6986 - recall: 0.7139
Epoch 33/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5826 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 34/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5825 - accuracy: 0.7061 - precision: 0.6985 - recall: 0.7139
Epoch 35/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5826 - accuracy: 0.7061 - precision: 0.6985 - recall: 0.7139
Epoch 36/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5826 - accuracy: 0.7067 - precision: 0.6985 - recall: 0.7139
Epoch 37/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7060 - precision: 0.6985 - recall: 0.7139
Epoch 38/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5824 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 39/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5825 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
Epoch 40/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.5825 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 41/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5826 - accuracy: 0.7060 - precision: 0.6985 - recall: 0.7139
Epoch 42/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.5825 - accuracy: 0.7062 - precision: 0.6985 - recall: 0.7139
Epoch 43/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5825 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
Epoch 44/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.5824 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 45/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5825 - accuracy: 0.7060 - precision: 0.6985 - recall: 0.7139
Epoch 46/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5825 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 47/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5825 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 48/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5822 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
Epoch 49/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5823 - accuracy: 0.7065 - precision: 0.6985 - recall: 0.7139
Epoch 50/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5824 - accuracy: 0.7066 - precision: 0.6985 - recall: 0.7139
history.history:
{'loss': [0.5840313450424764, 0.5824693409440624, 0.5824043830729811, 0.5824526575697339, 0.5824202643673894, 0.5824450753554427, 0.5826065057320925, 0.5822833140821727, 0.5824489555716047, 0.5823441860165415, 0.5824173872123866, 0.5824597397722223, 0.5824101766246887, 0.582330369416821, 0.5824082447998508, 0.582504307738378, 0.582469993292813, 0.5824436880709837, 0.5825573315856976, 0.582538821663012, 0.5824410151511211, 0.5824708664163127, 0.5825314276546159, 0.5823633511038697, 0.5824291846581386, 0.5823123582336056, 0.5824112532200313, 0.5823784858711595, 0.5823385043421151, 0.5824232828805912, 0.5823913394401665, 0.5825034546231009, 0.5825691987794981, 0.5824542121494801, 0.5825820361350019, 0.5825854662898117, 0.5824129166370524, 0.5824318459980369, 0.5824942991639949, 0.582487833125628, 0.5825803742531599, 0.5824715973522978, 0.5825310077224068, 0.5824442569373406, 0.5824638744015643, 0.5825023699250945, 0.582478432417305, 0.5822472443789384, 0.5823239810906918, 0.5823632765915743], 'accuracy': [0.70461833, 0.70658666, 0.7065455, 0.7056299, 0.7065883, 0.70618373, 0.7057071, 0.70656604, 0.7060431, 0.70666206, 0.70636886, 0.706158, 0.7064032, 0.7064889, 0.70623344, 0.7065009, 0.70611, 0.70637405, 0.7060277, 0.70667064, 0.70629513, 0.7062986, 0.70617515, 0.7067512, 0.7060174, 0.70660377, 0.70625746, 0.7064923, 0.70619744, 0.7062883, 0.70658666, 0.70635176, 0.7062214, 0.70606714, 0.7061271, 0.7067272, 0.70597285, 0.70618886, 0.70655406, 0.70623344, 0.7060191, 0.70621973, 0.7065746, 0.7064803, 0.7060242, 0.7064872, 0.7065369, 0.70655406, 0.7064786, 0.7065523], 'precision': [0.6938987, 0.6978329, 0.698189, 0.69823605, 0.69824475, 0.69830495, 0.69831383, 0.69830096, 0.6982879, 0.69830537, 0.698459, 0.6984655, 0.69836056, 0.6984809, 0.6984825, 0.6984925, 0.69849306, 0.69849104, 0.6984935, 0.6984935, 0.69849646, 0.6984971, 0.69849724, 0.69850534, 0.6984998, 0.6985367, 0.6985376, 0.6985331, 0.6985403, 0.6985404, 0.69854456, 0.6985505, 0.6985407, 0.69854116, 0.6985404, 0.6985405, 0.6985413, 0.69854075, 0.69854057, 0.6985406, 0.69854087, 0.6985404, 0.69854033, 0.6985395, 0.69854045, 0.69853956, 0.69854057, 0.69854075, 0.69854146, 0.6985415], 'recall': [0.70000064, 0.712496, 0.7135122, 0.71346563, 0.7136392, 0.71363646, 0.71366453, 0.71364814, 0.71380347, 0.7138037, 0.71386087, 0.7138488, 0.7138557, 0.71386266, 0.7138536, 0.71383035, 0.71385187, 0.7138539, 0.7138549, 0.7138488, 0.7138502, 0.71386284, 0.71382487, 0.7138523, 0.713856, 0.71385264, 0.71385366, 0.71385264, 0.71385914, 0.7138621, 0.7138622, 0.7138624, 0.71385276, 0.7138525, 0.7138597, 0.713853, 0.7138541, 0.7138518, 0.71385175, 0.7138606, 0.7138624, 0.7138527, 0.7138625, 0.71386325, 0.7138649, 0.7138634, 0.71386355, 0.71386594, 0.713866, 0.71386576]}
144811/144811 [==============================] - 2s 14us/step
results evaluated:
[0.5849122447949452, 0.7114376425743103, 0.6985828876495361, 0.7140130996704102]
predictions:
[[0.83989906 0.16010088]
 [0.5130115  0.4869885 ]
 [0.04636049 0.9536395 ]
 ...
 [0.37963912 0.6203609 ]
 [0.37064296 0.62935704]
 [0.10459267 0.8954074 ]]
pred_y:
[[1. 0.]
 [1. 0.]
 [0. 1.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_3 (Dense)              (None, 2)                 24        
=================================================================
Total params: 24
Trainable params: 24
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.12681228e-01 -2.82785224e-02  1.69966182e+00 -1.78511329e-01
  -1.73242798e+00 -3.79585434e-01 -5.26130640e-01  2.08200430e+00
  -5.44337784e-01 -3.44048699e-01 -3.95721002e-01]
 [-5.76705937e-01  2.13495190e-01  8.92739043e-01 -3.62576248e-04
  -1.28461351e+00 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
   1.83709460e+00 -3.44048699e-01 -3.95721002e-01]
 [ 2.29433287e-01 -4.15227768e-01 -1.02370256e+00  1.61158960e-01
   4.14378153e-01 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
  -5.44337784e-01 -3.44048699e-01  2.52703292e+00]
 [-6.83543665e-01 -5.26719822e-01  1.09446974e+00 -2.71148681e-01
  -1.30283168e+00  2.63445304e+00 -5.26130640e-01 -4.80306404e-01
  -5.44337784e-01 -3.44048699e-01 -3.95721002e-01]
 [-8.00093915e-01 -1.97914994e-01  9.93604391e-01  9.22747753e-02
  -1.73242798e+00 -3.79585434e-01 -5.26130640e-01 -4.80306404e-01
   1.83709460e+00 -3.44048699e-01 -3.95721002e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]
 [1. 0.]]
test_X BEFORE fitting:
[[-0.80009391 -0.30116506  0.38841231  0.16353428  0.41437815 -0.37958543
  -0.52613064  2.0820043  -0.54433778 -0.3440487  -0.395721  ]
 [-1.18859475 -0.55634856  0.28754696 -0.14288158 -0.01521815 -0.37958543
   1.90066863 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]
 [-1.33428256 -0.84558657  2.40571926 -0.15000753 -1.30283168 -0.37958543
  -0.52613064  2.0820043  -0.54433778 -0.3440487  -0.395721  ]
 [-1.54795801 -0.92371316 -0.01504908 -0.2450202   0.21750434 -0.37958543
   1.90066863 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]
 [-1.27600743 -0.75487173  2.40571926 -0.27827463 -1.30283168  2.63445304
  -0.52613064 -0.4803064  -0.54433778 -0.3440487  -0.395721  ]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288372, 294870]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71405, 73406]))
Epoch 1/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6112 - accuracy: 0.6747 - precision_1: 0.6593 - recall_1: 0.6930
Epoch 2/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6111 - accuracy: 0.6749 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 3/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6110 - accuracy: 0.6749 - precision_1: 0.6624 - recall_1: 0.6984
Epoch 4/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6110 - accuracy: 0.6746 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 5/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.6110 - accuracy: 0.6754 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 6/50
583242/583242 [==============================] - 18s 31us/step - loss: 0.6110 - accuracy: 0.6751 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 7/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.6109 - accuracy: 0.6746 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 8/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.6110 - accuracy: 0.6746 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 9/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.6109 - accuracy: 0.6747 - precision_1: 0.6623 - recall_1: 0.6983
Epoch 10/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.6110 - accuracy: 0.6746 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 11/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.6109 - accuracy: 0.6747 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 12/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.6110 - accuracy: 0.6749 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 13/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.6109 - accuracy: 0.6752 - precision_1: 0.6622 - recall_1: 0.6982
Epoch 14/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6109 - accuracy: 0.6747 - precision_1: 0.6622 - recall_1: 0.6982
Epoch 15/50
583242/583242 [==============================] - 13s 22us/step - loss: 0.6108 - accuracy: 0.6750 - precision_1: 0.6622 - recall_1: 0.6983
Epoch 16/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6109 - accuracy: 0.6749 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 17/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6110 - accuracy: 0.6744 - precision_1: 0.6623 - recall_1: 0.6982
Epoch 18/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6110 - accuracy: 0.6752 - precision_1: 0.6622 - recall_1: 0.6982
Epoch 19/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6111 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 20/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6110 - accuracy: 0.6744 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 21/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6109 - accuracy: 0.6749 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 22/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6110 - accuracy: 0.6750 - precision_1: 0.6624 - recall_1: 0.6982
Epoch 23/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6108 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 24/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6108 - accuracy: 0.6747 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 25/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6110 - accuracy: 0.6748 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 26/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6109 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 27/50
583242/583242 [==============================] - 15s 26us/step - loss: 0.6110 - accuracy: 0.6744 - precision_1: 0.6623 - recall_1: 0.6983
Epoch 28/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.6109 - accuracy: 0.6746 - precision_1: 0.6623 - recall_1: 0.6983
Epoch 29/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.6110 - accuracy: 0.6749 - precision_1: 0.6623 - recall_1: 0.6983
Epoch 30/50
583242/583242 [==============================] - 15s 25us/step - loss: 0.6111 - accuracy: 0.6751 - precision_1: 0.6622 - recall_1: 0.6983
Epoch 31/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.6110 - accuracy: 0.6751 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 32/50
583242/583242 [==============================] - 14s 25us/step - loss: 0.6109 - accuracy: 0.6746 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 33/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6110 - accuracy: 0.6748 - precision_1: 0.6623 - recall_1: 0.6983
Epoch 34/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6110 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 35/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6109 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 36/50
583242/583242 [==============================] - 13s 22us/step - loss: 0.6109 - accuracy: 0.6753 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 37/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6107 - accuracy: 0.6752 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 38/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6109 - accuracy: 0.6743 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 39/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6110 - accuracy: 0.6751 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 40/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6107 - accuracy: 0.6750 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 41/50
583242/583242 [==============================] - 13s 22us/step - loss: 0.6111 - accuracy: 0.6750 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 42/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6109 - accuracy: 0.6751 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 43/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6110 - accuracy: 0.6748 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 44/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6109 - accuracy: 0.6746 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 45/50
583242/583242 [==============================] - 13s 23us/step - loss: 0.6108 - accuracy: 0.6748 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 46/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6111 - accuracy: 0.6745 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 47/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6108 - accuracy: 0.6748 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 48/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6110 - accuracy: 0.6754 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 49/50
583242/583242 [==============================] - 14s 23us/step - loss: 0.6110 - accuracy: 0.6753 - precision_1: 0.6624 - recall_1: 0.6983
Epoch 50/50
583242/583242 [==============================] - 14s 24us/step - loss: 0.6111 - accuracy: 0.6750 - precision_1: 0.6624 - recall_1: 0.6983
history.history:
{'loss': [0.6112089601464363, 0.6110807480885277, 0.6109565727351103, 0.610974900454858, 0.6109648629162568, 0.611011002334618, 0.610935116712016, 0.6109735358508268, 0.6109230958966716, 0.6109765836317669, 0.6109148987916133, 0.6109982570460231, 0.6108957571902169, 0.6108986779428981, 0.6108312341495935, 0.6109478929039864, 0.610989333579552, 0.6109633382376559, 0.6111485161471978, 0.6109915194748066, 0.6109194815332372, 0.6109729144766819, 0.6107725513823669, 0.6107502156217892, 0.6110262656912014, 0.6108507112897333, 0.6109621277905702, 0.610913935982664, 0.6109521157136444, 0.6110680739519414, 0.6109600349365554, 0.610881424762526, 0.6110423576428105, 0.611023426599792, 0.6109108536940191, 0.6108690726051942, 0.6106624708273162, 0.6108925015640844, 0.6110300215932097, 0.61073390127451, 0.6111016596497291, 0.6108507771270706, 0.6110316264446066, 0.610851460081706, 0.6108435494006946, 0.6111495591725805, 0.610817107835905, 0.6109854601630625, 0.6110007592438846, 0.6110895192748717], 'accuracy': [0.6746771, 0.6749154, 0.67493767, 0.6746188, 0.675368, 0.67505425, 0.67458105, 0.67457074, 0.6746942, 0.6745691, 0.6747456, 0.6749274, 0.675212, 0.67474395, 0.67501825, 0.6749428, 0.6744439, 0.6752103, 0.6752137, 0.6744164, 0.6749497, 0.6750217, 0.67520857, 0.6746788, 0.6748039, 0.6751571, 0.67436504, 0.6746462, 0.6748794, 0.67508, 0.6750937, 0.6746188, 0.67482966, 0.67518115, 0.6751983, 0.67526, 0.6751606, 0.674281, 0.6751211, 0.67498225, 0.67497027, 0.67506284, 0.6748177, 0.6745656, 0.6748468, 0.67448676, 0.6748348, 0.6753817, 0.6752943, 0.6749977], 'precision_1': [0.65929997, 0.66231334, 0.6624209, 0.66235137, 0.66229516, 0.6623133, 0.66237634, 0.66237205, 0.6623068, 0.6622705, 0.66234064, 0.6623567, 0.66219187, 0.6621924, 0.6621921, 0.66235846, 0.6622755, 0.662245, 0.66236216, 0.6623618, 0.6623592, 0.6623613, 0.6623616, 0.6623618, 0.6623615, 0.6623616, 0.6623467, 0.6623487, 0.6623227, 0.6622161, 0.6623588, 0.6623589, 0.662267, 0.6623584, 0.66235894, 0.6623615, 0.662362, 0.6623627, 0.66236174, 0.6623618, 0.66236264, 0.662362, 0.6623627, 0.66236275, 0.662362, 0.662362, 0.6623618, 0.66236186, 0.662362, 0.66236204], 'recall_1': [0.69295245, 0.6982127, 0.6983929, 0.6981707, 0.6982101, 0.6982461, 0.69822973, 0.69822174, 0.69827586, 0.698244, 0.69823974, 0.69824123, 0.69823873, 0.6982466, 0.698259, 0.6982545, 0.6982498, 0.69824326, 0.69825935, 0.6982484, 0.69825673, 0.6982463, 0.69826895, 0.6982615, 0.6982617, 0.6982992, 0.6982663, 0.6982626, 0.698261, 0.6982608, 0.698301, 0.69826233, 0.6982929, 0.69826186, 0.698302, 0.6983019, 0.6983041, 0.6983048, 0.69829446, 0.6982629, 0.6983039, 0.698307, 0.6983049, 0.6983021, 0.6982968, 0.6982777, 0.6982618, 0.6982945, 0.698299, 0.6983044]}
144811/144811 [==============================] - 2s 14us/step
results evaluated:
[0.6107970611720199, 0.6740372180938721, 0.6623706817626953, 0.6984230279922485]
predictions:
[[0.7263876  0.27361244]
 [0.45517364 0.5448264 ]
 [0.08792131 0.9120786 ]
 ...
 [0.36316657 0.6368334 ]
 [0.44039527 0.5596047 ]
 [0.3155564  0.6844436 ]]
pred_y:
[[1. 0.]
 [1. 0.]
 [0. 1.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]