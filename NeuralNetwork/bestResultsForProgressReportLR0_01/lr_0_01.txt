_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 5)                 60        
_________________________________________________________________
dense_2 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_4 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_5 (Dense)              (None, 2)                 12        
=================================================================
Total params: 162
Trainable params: 162
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[ 5.78226430e-03  1.03361441e+00 -1.48767723e-02 -7.16901572e-02
   4.14380086e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 5.98095958e-01  2.81538334e-01  4.89436463e-01  3.96717282e-02
  -2.29826710e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [-1.16913506e+00 -4.36462010e-01  1.39720029e+00 -4.08879336e-02
  -1.30311285e+00 -3.78993973e-01 -5.26624687e-01  2.08117268e+00
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 6.56356322e-01 -1.95104693e-03 -1.52781648e+00  1.36817203e-01
   1.91674557e+00 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]
 [-9.45803670e-01 -5.86628460e-01  6.91161757e-01 -2.09115463e-01
   1.99252634e-01 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[ 0.73403681  1.84104089 -0.62005265 -0.05747375  0.41438009 -0.37899397
  -0.52662469 -0.48049833 -0.54483795 -0.34361846  2.52903353]
 [ 1.98663462  0.45632223  0.18684852 -0.19489905 -0.44436638 -0.37899397
  -0.52662469  2.08117268 -0.54483795 -0.34361846 -0.39540796]
 [-0.88754331 -0.55387429  0.89288705  0.05862694 -1.73277997 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [ 1.15156941  0.71293972  1.90151352 -0.1261864   0.19925263 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [-1.09145458 -0.64757609  0.18684852 -0.31336914 -0.87403351 -0.37899397
  -0.52662469 -0.48049833 -0.54483795  2.91020452 -0.39540796]]
test_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288321, 294921]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71450, 73361]))
Epoch 1/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5197 - accuracy: 0.7458 - precision: 0.7152 - recall: 0.7557       
Epoch 2/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5069 - accuracy: 0.7551 - precision: 0.7356 - recall: 0.7667
Epoch 3/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.5051 - accuracy: 0.7563 - precision: 0.7372 - recall: 0.7731
Epoch 4/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5046 - accuracy: 0.7570 - precision: 0.7369 - recall: 0.7784
Epoch 5/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5036 - accuracy: 0.7572 - precision: 0.7364 - recall: 0.7823
Epoch 6/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5042 - accuracy: 0.7578 - precision: 0.7359 - recall: 0.7851
Epoch 7/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5024 - accuracy: 0.7582 - precision: 0.7355 - recall: 0.7876
Epoch 8/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5029 - accuracy: 0.7578 - precision: 0.7354 - recall: 0.7893
Epoch 9/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5020 - accuracy: 0.7584 - precision: 0.7354 - recall: 0.7900
Epoch 10/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5025 - accuracy: 0.7584 - precision: 0.7356 - recall: 0.7907
Epoch 11/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5014 - accuracy: 0.7583 - precision: 0.7360 - recall: 0.7908
Epoch 12/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5014 - accuracy: 0.7587 - precision: 0.7363 - recall: 0.7908
Epoch 13/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5013 - accuracy: 0.7588 - precision: 0.7363 - recall: 0.7908
Epoch 14/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5015 - accuracy: 0.7589 - precision: 0.7365 - recall: 0.7910
Epoch 15/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5014 - accuracy: 0.7582 - precision: 0.7366 - recall: 0.7910
Epoch 16/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5013 - accuracy: 0.7586 - precision: 0.7370 - recall: 0.7910
Epoch 17/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5012 - accuracy: 0.7593 - precision: 0.7371 - recall: 0.7910
Epoch 18/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5011 - accuracy: 0.7593 - precision: 0.7372 - recall: 0.7910
Epoch 19/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5012 - accuracy: 0.7590 - precision: 0.7373 - recall: 0.7910
Epoch 20/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5015 - accuracy: 0.7588 - precision: 0.7374 - recall: 0.7910
Epoch 21/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5011 - accuracy: 0.7591 - precision: 0.7375 - recall: 0.7910
Epoch 22/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5012 - accuracy: 0.7591 - precision: 0.7376 - recall: 0.7910
Epoch 23/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.5011 - accuracy: 0.7591 - precision: 0.7380 - recall: 0.7910
Epoch 24/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.5012 - accuracy: 0.7590 - precision: 0.7380 - recall: 0.7909
Epoch 25/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5008 - accuracy: 0.7592 - precision: 0.7380 - recall: 0.7908
Epoch 26/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5011 - accuracy: 0.7591 - precision: 0.7382 - recall: 0.7908
Epoch 27/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5015 - accuracy: 0.7591 - precision: 0.7382 - recall: 0.7909
Epoch 28/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5017 - accuracy: 0.7591 - precision: 0.7382 - recall: 0.7909
Epoch 29/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.5011 - accuracy: 0.7590 - precision: 0.7383 - recall: 0.7909
Epoch 30/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5016 - accuracy: 0.7595 - precision: 0.7383 - recall: 0.7909
Epoch 31/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5011 - accuracy: 0.7594 - precision: 0.7383 - recall: 0.7910
Epoch 32/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.5010 - accuracy: 0.7594 - precision: 0.7383 - recall: 0.7910
Epoch 33/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5004 - accuracy: 0.7591 - precision: 0.7383 - recall: 0.7908
Epoch 34/50
583242/583242 [==============================] - 24s 42us/step - loss: 0.4998 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7908
Epoch 35/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4997 - accuracy: 0.7595 - precision: 0.7385 - recall: 0.7908
Epoch 36/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4996 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7909
Epoch 37/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4998 - accuracy: 0.7597 - precision: 0.7385 - recall: 0.7910
Epoch 38/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4994 - accuracy: 0.7600 - precision: 0.7386 - recall: 0.7910
Epoch 39/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4993 - accuracy: 0.7597 - precision: 0.7386 - recall: 0.7910
Epoch 40/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4993 - accuracy: 0.7596 - precision: 0.7386 - recall: 0.7910
Epoch 41/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4991 - accuracy: 0.7595 - precision: 0.7386 - recall: 0.7910
Epoch 42/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4990 - accuracy: 0.7598 - precision: 0.7390 - recall: 0.7911
Epoch 43/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4992 - accuracy: 0.7602 - precision: 0.7390 - recall: 0.7911
Epoch 44/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4989 - accuracy: 0.7600 - precision: 0.7390 - recall: 0.7911
Epoch 45/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4989 - accuracy: 0.7596 - precision: 0.7390 - recall: 0.7912
Epoch 46/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4990 - accuracy: 0.7597 - precision: 0.7390 - recall: 0.7912
Epoch 47/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4989 - accuracy: 0.7598 - precision: 0.7390 - recall: 0.7912
Epoch 48/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4988 - accuracy: 0.7599 - precision: 0.7390 - recall: 0.7912
Epoch 49/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4989 - accuracy: 0.7594 - precision: 0.7390 - recall: 0.7912
Epoch 50/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4987 - accuracy: 0.7595 - precision: 0.7390 - recall: 0.7912
history.history:
{'loss': [0.5197063562570969, 0.5069167435042089, 0.505052310810514, 0.5045703996256553, 0.5035783061056464, 0.5042400776402395, 0.5023701002935536, 0.5029034813449529, 0.5019730064870407, 0.5024835282133941, 0.5013922073245206, 0.5013754922083034, 0.5012789890171375, 0.5014862361811941, 0.5014241488043266, 0.5013094396086357, 0.501234921514897, 0.5011112377348045, 0.5011963141070341, 0.5014862506300896, 0.5010543694229498, 0.5012186267590876, 0.5010935527917781, 0.5012266403034399, 0.5008324550856994, 0.5010786629217954, 0.5014704610891569, 0.5016693063688246, 0.5010954995167042, 0.5015850913394185, 0.5011019096783097, 0.5010346886104597, 0.5003972668133985, 0.49980865002377894, 0.4997023132157189, 0.49959899666993485, 0.49976766408817297, 0.4994079103518976, 0.4992514943840753, 0.4992686724554331, 0.4990626584693744, 0.4989967690365047, 0.49924041012685216, 0.4988955091072715, 0.49887024046801853, 0.49902272866990016, 0.4988908234352664, 0.4987895256512249, 0.4988884628335148, 0.49866190769846036], 'accuracy': [0.74575907, 0.75505537, 0.75628984, 0.75699455, 0.7571763, 0.75775576, 0.75816214, 0.7578295, 0.75835246, 0.7583593, 0.7582959, 0.7587091, 0.758836, 0.758896, 0.758217, 0.7585805, 0.7592835, 0.7593469, 0.75901943, 0.7587605, 0.7591017, 0.7591257, 0.759052, 0.75902456, 0.7591566, 0.7590846, 0.759088, 0.75910854, 0.75896454, 0.75950634, 0.75938976, 0.75939316, 0.7591446, 0.75969493, 0.7595406, 0.75965554, 0.7596778, 0.76000357, 0.7597104, 0.7595801, 0.75946176, 0.7598441, 0.76023847, 0.75996244, 0.75956464, 0.7596847, 0.75976694, 0.7599333, 0.7594103, 0.75950634], 'precision': [0.7152094, 0.7356341, 0.7372046, 0.73686093, 0.7363789, 0.7358781, 0.735467, 0.73542166, 0.73539156, 0.7355871, 0.7360295, 0.73625636, 0.7363274, 0.7365122, 0.73660636, 0.73700386, 0.73706347, 0.7372488, 0.7373056, 0.7373635, 0.7375277, 0.7375832, 0.73798263, 0.7379977, 0.7380421, 0.7382059, 0.7382107, 0.7382247, 0.73826635, 0.7382801, 0.7382844, 0.73829615, 0.7383407, 0.73850435, 0.73851794, 0.73851836, 0.7385217, 0.738557, 0.7385619, 0.7385736, 0.73857456, 0.7389598, 0.7389608, 0.7389645, 0.7389747, 0.7389758, 0.7389786, 0.7390167, 0.73902017, 0.7390304], 'recall': [0.75566846, 0.7667148, 0.77305496, 0.7784087, 0.7822639, 0.785087, 0.7875641, 0.7892572, 0.78996414, 0.7906926, 0.79077524, 0.79075104, 0.7907982, 0.79098225, 0.79101425, 0.79101145, 0.7909758, 0.7909595, 0.79096025, 0.79095054, 0.7909502, 0.7909592, 0.7909592, 0.7909495, 0.7908073, 0.7908383, 0.790947, 0.79094726, 0.790912, 0.7909465, 0.79095924, 0.79096013, 0.79080844, 0.7907922, 0.79080313, 0.79094726, 0.7909812, 0.79100204, 0.79101586, 0.7910191, 0.79102933, 0.7910676, 0.791071, 0.7910817, 0.7912249, 0.7912257, 0.7912362, 0.7912376, 0.7912407, 0.7912402]}
144811/144811 [==============================] - 2s 15us/step
results evaluated:
[0.4999013708340245, 0.7580501437187195, 0.739000141620636, 0.7913260459899902]
predictions:
[[0.44486564 0.55513436]
 [0.5334128  0.46658716]
 [0.8837325  0.11626745]
 ...
 [0.37371585 0.6262842 ]
 [0.2873798  0.7126202 ]
 [0.16403715 0.83596283]]
pred_y:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 10)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_8 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_9 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_10 (Dense)             (None, 2)                 22        
=================================================================
Total params: 472
Trainable params: 472
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[ 5.78226430e-03  1.03361441e+00 -1.48767723e-02 -7.16901572e-02
   4.14380086e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 5.98095958e-01  2.81538334e-01  4.89436463e-01  3.96717282e-02
  -2.29826710e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [-1.16913506e+00 -4.36462010e-01  1.39720029e+00 -4.08879336e-02
  -1.30311285e+00 -3.78993973e-01 -5.26624687e-01  2.08117268e+00
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 6.56356322e-01 -1.95104693e-03 -1.52781648e+00  1.36817203e-01
   1.91674557e+00 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]
 [-9.45803670e-01 -5.86628460e-01  6.91161757e-01 -2.09115463e-01
   1.99252634e-01 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[ 0.73403681  1.84104089 -0.62005265 -0.05747375  0.41438009 -0.37899397
  -0.52662469 -0.48049833 -0.54483795 -0.34361846  2.52903353]
 [ 1.98663462  0.45632223  0.18684852 -0.19489905 -0.44436638 -0.37899397
  -0.52662469  2.08117268 -0.54483795 -0.34361846 -0.39540796]
 [-0.88754331 -0.55387429  0.89288705  0.05862694 -1.73277997 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [ 1.15156941  0.71293972  1.90151352 -0.1261864   0.19925263 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [-1.09145458 -0.64757609  0.18684852 -0.31336914 -0.87403351 -0.37899397
  -0.52662469 -0.48049833 -0.54483795  2.91020452 -0.39540796]]
test_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288321, 294921]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71450, 73361]))
Epoch 1/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5064 - accuracy: 0.7552 - precision_1: 0.7257 - recall_1: 0.7863
Epoch 2/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4981 - accuracy: 0.7604 - precision_1: 0.7349 - recall_1: 0.7955
Epoch 3/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4970 - accuracy: 0.7609 - precision_1: 0.7353 - recall_1: 0.7988
Epoch 4/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4960 - accuracy: 0.7612 - precision_1: 0.7357 - recall_1: 0.8006
Epoch 5/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4959 - accuracy: 0.7616 - precision_1: 0.7361 - recall_1: 0.8011
Epoch 6/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4955 - accuracy: 0.7616 - precision_1: 0.7362 - recall_1: 0.8018
Epoch 7/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4951 - accuracy: 0.7622 - precision_1: 0.7368 - recall_1: 0.8015
Epoch 8/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4953 - accuracy: 0.7618 - precision_1: 0.7372 - recall_1: 0.8017
Epoch 9/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4952 - accuracy: 0.7617 - precision_1: 0.7372 - recall_1: 0.8016
Epoch 10/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4950 - accuracy: 0.7619 - precision_1: 0.7375 - recall_1: 0.8009
Epoch 11/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4949 - accuracy: 0.7618 - precision_1: 0.7376 - recall_1: 0.8008
Epoch 12/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4952 - accuracy: 0.7618 - precision_1: 0.7379 - recall_1: 0.8010
Epoch 13/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7621 - precision_1: 0.7379 - recall_1: 0.8015
Epoch 14/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4949 - accuracy: 0.7619 - precision_1: 0.7380 - recall_1: 0.8014
Epoch 15/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4948 - accuracy: 0.7620 - precision_1: 0.7382 - recall_1: 0.8010
Epoch 16/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4947 - accuracy: 0.7620 - precision_1: 0.7383 - recall_1: 0.8011
Epoch 17/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4947 - accuracy: 0.7622 - precision_1: 0.7382 - recall_1: 0.8015
Epoch 18/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4947 - accuracy: 0.7622 - precision_1: 0.7382 - recall_1: 0.8017
Epoch 19/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4949 - accuracy: 0.7621 - precision_1: 0.7383 - recall_1: 0.8017
Epoch 20/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4946 - accuracy: 0.7623 - precision_1: 0.7382 - recall_1: 0.8018
Epoch 21/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4946 - accuracy: 0.7627 - precision_1: 0.7383 - recall_1: 0.8018
Epoch 22/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4949 - accuracy: 0.7625 - precision_1: 0.7383 - recall_1: 0.8017
Epoch 23/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4944 - accuracy: 0.7624 - precision_1: 0.7383 - recall_1: 0.8018
Epoch 24/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4944 - accuracy: 0.7621 - precision_1: 0.7383 - recall_1: 0.8018
Epoch 25/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4944 - accuracy: 0.7627 - precision_1: 0.7384 - recall_1: 0.8018
Epoch 26/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4945 - accuracy: 0.7625 - precision_1: 0.7385 - recall_1: 0.8017
Epoch 27/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4943 - accuracy: 0.7623 - precision_1: 0.7385 - recall_1: 0.8016
Epoch 28/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4945 - accuracy: 0.7621 - precision_1: 0.7386 - recall_1: 0.8015
Epoch 29/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4944 - accuracy: 0.7623 - precision_1: 0.7390 - recall_1: 0.8015
Epoch 30/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4945 - accuracy: 0.7625 - precision_1: 0.7388 - recall_1: 0.8015
Epoch 31/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4944 - accuracy: 0.7620 - precision_1: 0.7386 - recall_1: 0.8017
Epoch 32/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4943 - accuracy: 0.7622 - precision_1: 0.7390 - recall_1: 0.8016
Epoch 33/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4944 - accuracy: 0.7624 - precision_1: 0.7390 - recall_1: 0.8015
Epoch 34/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4941 - accuracy: 0.7629 - precision_1: 0.7390 - recall_1: 0.8015
Epoch 35/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4943 - accuracy: 0.7623 - precision_1: 0.7390 - recall_1: 0.8011
Epoch 36/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4944 - accuracy: 0.7625 - precision_1: 0.7390 - recall_1: 0.8011
Epoch 37/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4945 - accuracy: 0.7624 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 38/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4942 - accuracy: 0.7627 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 39/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4942 - accuracy: 0.7627 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 40/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4943 - accuracy: 0.7623 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 41/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4941 - accuracy: 0.7624 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 42/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4943 - accuracy: 0.7626 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 43/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4942 - accuracy: 0.7627 - precision_1: 0.7392 - recall_1: 0.8010
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4943 - accuracy: 0.7626 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 45/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4940 - accuracy: 0.7622 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 46/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4941 - accuracy: 0.7625 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 47/50
583242/583242 [==============================] - 25s 43us/step - loss: 0.4941 - accuracy: 0.7627 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 48/50
583242/583242 [==============================] - 25s 43us/step - loss: 0.4941 - accuracy: 0.7622 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 49/50
583242/583242 [==============================] - 32s 55us/step - loss: 0.4941 - accuracy: 0.7626 - precision_1: 0.7393 - recall_1: 0.8010
Epoch 50/50
583242/583242 [==============================] - 28s 48us/step - loss: 0.4942 - accuracy: 0.7628 - precision_1: 0.7393 - recall_1: 0.8010
history.history:
{'loss': [0.5064069677456585, 0.49805347183620496, 0.49702876759596737, 0.4959707875924477, 0.49592285372407563, 0.49554393133599905, 0.4951304415112264, 0.49527078048823187, 0.4952351885148325, 0.494970390544063, 0.49488562692233284, 0.4951600533420168, 0.4949931953706568, 0.49490582880889167, 0.4947524423118655, 0.49468404936352905, 0.4947236841770265, 0.49472558422369634, 0.49485143966465606, 0.49457619187710233, 0.4946290029793692, 0.49489159141126493, 0.49441795136364597, 0.4943856615209033, 0.49436300074637907, 0.4945426619800426, 0.49433123292853104, 0.49450664008882017, 0.4944008646852184, 0.49445560099537594, 0.49437943508024607, 0.49434645140253336, 0.49435961718653626, 0.49405756447602434, 0.4943477133447529, 0.4944252964815621, 0.49448596237091025, 0.4941839537670097, 0.4942065004825855, 0.4943328146335914, 0.4941145162421689, 0.4942854521433346, 0.4942197725079981, 0.4942691271679389, 0.4940448882667111, 0.4940725946901723, 0.4940574032667792, 0.4941089073137949, 0.4940959085680579, 0.49418336522926615], 'accuracy': [0.755196, 0.76043737, 0.760866, 0.76121575, 0.7616478, 0.76155865, 0.76224107, 0.76176614, 0.76168555, 0.76186556, 0.7617627, 0.761797, 0.7621296, 0.7619256, 0.7620456, 0.7620113, 0.7622016, 0.7622119, 0.7621279, 0.76230276, 0.76265424, 0.7624725, 0.76236624, 0.7621056, 0.76267654, 0.76254624, 0.7622805, 0.76207644, 0.7622839, 0.76247597, 0.76195985, 0.76220506, 0.76236963, 0.76289946, 0.7623097, 0.7624708, 0.7623988, 0.7626628, 0.76268685, 0.76234734, 0.7624177, 0.7625583, 0.7627074, 0.7625668, 0.7621828, 0.7625085, 0.762728, 0.7621965, 0.76261485, 0.762764], 'precision_1': [0.7257257, 0.73486704, 0.73529017, 0.7357034, 0.7361268, 0.73624504, 0.73680806, 0.7371867, 0.73724407, 0.737538, 0.737608, 0.7379481, 0.73790175, 0.73800296, 0.7382103, 0.73827636, 0.73822546, 0.73822093, 0.73827654, 0.738224, 0.7382775, 0.7382855, 0.73829657, 0.73829085, 0.738379, 0.73850775, 0.73852164, 0.73857766, 0.73895717, 0.7387997, 0.7385779, 0.7389568, 0.7389645, 0.73897904, 0.73903036, 0.73903453, 0.73918384, 0.73919755, 0.73920107, 0.73920125, 0.73921895, 0.73924357, 0.7392406, 0.7392531, 0.73925704, 0.7392566, 0.7392576, 0.73926157, 0.7392626, 0.7392721], 'recall_1': [0.7863138, 0.79545677, 0.7988432, 0.8005849, 0.80110836, 0.8017764, 0.8015282, 0.8017292, 0.8015629, 0.8009051, 0.800792, 0.8010014, 0.80146325, 0.80138415, 0.80104125, 0.80106986, 0.8015234, 0.801742, 0.8016842, 0.8017623, 0.8017512, 0.80174494, 0.8017578, 0.8018124, 0.80175704, 0.8016945, 0.8015588, 0.8014889, 0.80153924, 0.8015334, 0.80169094, 0.8015756, 0.80153954, 0.80150974, 0.80105543, 0.8011482, 0.8010423, 0.8010197, 0.801009, 0.80104476, 0.8010428, 0.8010006, 0.8010454, 0.80104107, 0.80103695, 0.80101085, 0.8010085, 0.800967, 0.8009867, 0.8009753]}
144811/144811 [==============================] - 3s 20us/step
results evaluated:
[0.49271500622413417, 0.7617031931877136, 0.7393324375152588, 0.8010210990905762]
predictions:
[[0.4200579  0.57994217]
 [0.57714045 0.42285958]
 [0.7602646  0.23973542]
 ...
 [0.4154699  0.58453006]
 [0.2416697  0.7583303 ]
 [0.15154183 0.8484581 ]]
pred_y:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_12 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_13 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_14 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_15 (Dense)             (None, 2)                 42        
=================================================================
Total params: 1,542
Trainable params: 1,542
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[ 5.78226430e-03  1.03361441e+00 -1.48767723e-02 -7.16901572e-02
   4.14380086e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 5.98095958e-01  2.81538334e-01  4.89436463e-01  3.96717282e-02
  -2.29826710e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [-1.16913506e+00 -4.36462010e-01  1.39720029e+00 -4.08879336e-02
  -1.30311285e+00 -3.78993973e-01 -5.26624687e-01  2.08117268e+00
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 6.56356322e-01 -1.95104693e-03 -1.52781648e+00  1.36817203e-01
   1.91674557e+00 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]
 [-9.45803670e-01 -5.86628460e-01  6.91161757e-01 -2.09115463e-01
   1.99252634e-01 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[ 0.73403681  1.84104089 -0.62005265 -0.05747375  0.41438009 -0.37899397
  -0.52662469 -0.48049833 -0.54483795 -0.34361846  2.52903353]
 [ 1.98663462  0.45632223  0.18684852 -0.19489905 -0.44436638 -0.37899397
  -0.52662469  2.08117268 -0.54483795 -0.34361846 -0.39540796]
 [-0.88754331 -0.55387429  0.89288705  0.05862694 -1.73277997 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [ 1.15156941  0.71293972  1.90151352 -0.1261864   0.19925263 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [-1.09145458 -0.64757609  0.18684852 -0.31336914 -0.87403351 -0.37899397
  -0.52662469 -0.48049833 -0.54483795  2.91020452 -0.39540796]]
test_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288321, 294921]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71450, 73361]))
Epoch 1/50
583242/583242 [==============================] - 27s 46us/step - loss: 0.5056 - accuracy: 0.7565 - precision_2: 0.7269 - recall_2: 0.7935
Epoch 2/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5010 - accuracy: 0.7594 - precision_2: 0.7325 - recall_2: 0.8018
Epoch 3/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4989 - accuracy: 0.7598 - precision_2: 0.7342 - recall_2: 0.8009
Epoch 4/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4972 - accuracy: 0.7605 - precision_2: 0.7350 - recall_2: 0.8009
Epoch 5/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4965 - accuracy: 0.7607 - precision_2: 0.7352 - recall_2: 0.8017
Epoch 6/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4981 - accuracy: 0.7606 - precision_2: 0.7354 - recall_2: 0.8019
Epoch 7/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4970 - accuracy: 0.7608 - precision_2: 0.7356 - recall_2: 0.8019
Epoch 8/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4966 - accuracy: 0.7614 - precision_2: 0.7356 - recall_2: 0.8024
Epoch 9/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4970 - accuracy: 0.7612 - precision_2: 0.7360 - recall_2: 0.8027
Epoch 10/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4968 - accuracy: 0.7612 - precision_2: 0.7361 - recall_2: 0.8027
Epoch 11/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4957 - accuracy: 0.7612 - precision_2: 0.7363 - recall_2: 0.8025
Epoch 12/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5009 - accuracy: 0.7610 - precision_2: 0.7364 - recall_2: 0.8020
Epoch 13/50
583242/583242 [==============================] - 24s 41us/step - loss: 0.4969 - accuracy: 0.7612 - precision_2: 0.7366 - recall_2: 0.8019
Epoch 14/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4959 - accuracy: 0.7610 - precision_2: 0.7366 - recall_2: 0.8020
Epoch 15/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4954 - accuracy: 0.7614 - precision_2: 0.7370 - recall_2: 0.8018
Epoch 16/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4973 - accuracy: 0.7617 - precision_2: 0.7371 - recall_2: 0.8018
Epoch 17/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4983 - accuracy: 0.7607 - precision_2: 0.7372 - recall_2: 0.8017
Epoch 18/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4950 - accuracy: 0.7616 - precision_2: 0.7373 - recall_2: 0.8013
Epoch 19/50
583242/583242 [==============================] - 22s 39us/step - loss: 0.4952 - accuracy: 0.7620 - precision_2: 0.7374 - recall_2: 0.8010
Epoch 20/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.5071 - accuracy: 0.7616 - precision_2: 0.7375 - recall_2: 0.8009
Epoch 21/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4959 - accuracy: 0.7621 - precision_2: 0.7375 - recall_2: 0.8009
Epoch 22/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4952 - accuracy: 0.7622 - precision_2: 0.7378 - recall_2: 0.8008
Epoch 23/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4958 - accuracy: 0.7618 - precision_2: 0.7380 - recall_2: 0.8008
Epoch 24/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.5001 - accuracy: 0.7611 - precision_2: 0.7380 - recall_2: 0.8008
Epoch 25/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4961 - accuracy: 0.7621 - precision_2: 0.7382 - recall_2: 0.8007
Epoch 26/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7623 - precision_2: 0.7382 - recall_2: 0.8006
Epoch 27/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4948 - accuracy: 0.7623 - precision_2: 0.7383 - recall_2: 0.8005
Epoch 28/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4952 - accuracy: 0.7621 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 29/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4973 - accuracy: 0.7621 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 30/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4954 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 31/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4967 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 32/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4969 - accuracy: 0.7616 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 33/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4997 - accuracy: 0.7621 - precision_2: 0.7383 - recall_2: 0.8008
Epoch 34/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4955 - accuracy: 0.7619 - precision_2: 0.7383 - recall_2: 0.8007
Epoch 35/50
583242/583242 [==============================] - 23s 39us/step - loss: 0.4947 - accuracy: 0.7622 - precision_2: 0.7384 - recall_2: 0.8007
Epoch 36/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4953 - accuracy: 0.7619 - precision_2: 0.7385 - recall_2: 0.8006
Epoch 37/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4950 - accuracy: 0.7619 - precision_2: 0.7385 - recall_2: 0.8003
Epoch 38/50
583242/583242 [==============================] - 22s 37us/step - loss: 0.4978 - accuracy: 0.7625 - precision_2: 0.7389 - recall_2: 0.8001
Epoch 39/50
583242/583242 [==============================] - 22s 38us/step - loss: 0.4943 - accuracy: 0.7623 - precision_2: 0.7390 - recall_2: 0.8000
Epoch 40/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4941 - accuracy: 0.7623 - precision_2: 0.7390 - recall_2: 0.7999
Epoch 41/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4945 - accuracy: 0.7623 - precision_2: 0.7390 - recall_2: 0.7998
Epoch 42/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4946 - accuracy: 0.7625 - precision_2: 0.7392 - recall_2: 0.7998
Epoch 43/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4938 - accuracy: 0.7626 - precision_2: 0.7392 - recall_2: 0.7998
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4944 - accuracy: 0.7630 - precision_2: 0.7393 - recall_2: 0.7997
Epoch 45/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4943 - accuracy: 0.7627 - precision_2: 0.7393 - recall_2: 0.7996
Epoch 46/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4936 - accuracy: 0.7630 - precision_2: 0.7393 - recall_2: 0.7991
Epoch 47/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4937 - accuracy: 0.7624 - precision_2: 0.7393 - recall_2: 0.7991
Epoch 48/50
583242/583242 [==============================] - 24s 41us/step - loss: 0.4936 - accuracy: 0.7629 - precision_2: 0.7395 - recall_2: 0.7991
Epoch 49/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4938 - accuracy: 0.7624 - precision_2: 0.7395 - recall_2: 0.7990
Epoch 50/50
583242/583242 [==============================] - 21s 37us/step - loss: 0.4944 - accuracy: 0.7626 - precision_2: 0.7395 - recall_2: 0.7990
history.history:
{'loss': [0.5055836783974689, 0.50102208241337, 0.4988549371440437, 0.4971693764486221, 0.4965297489937614, 0.49811366997336093, 0.49698614444920247, 0.4966080705947792, 0.4970184165353936, 0.4968295731498008, 0.49565732814091956, 0.5009365101483227, 0.4969463115742906, 0.4958667321203275, 0.4954309401527647, 0.4972647354467924, 0.49827746225179004, 0.4949623513759534, 0.4952026374765084, 0.5071289857445864, 0.495891122006341, 0.49518596052291197, 0.4957540263959046, 0.5000919788683115, 0.49605947268590433, 0.49519536870059394, 0.4948240140628904, 0.4951654636623332, 0.4973457240822401, 0.4953525847981419, 0.4967349542500456, 0.4969282800090218, 0.4996561904671634, 0.49551759873490003, 0.4946659964701841, 0.4953313270665039, 0.49500753750466164, 0.4977935884416286, 0.49427585549466246, 0.4941225005583326, 0.4944795411170169, 0.4945750932251327, 0.4938311653036058, 0.4944031147845234, 0.4942568335477188, 0.49362878719566644, 0.4937436214810719, 0.49362808140914977, 0.49384957685729947, 0.49440117285378754], 'accuracy': [0.75648874, 0.7593966, 0.7598167, 0.7605385, 0.76073223, 0.7605779, 0.760794, 0.76140434, 0.76120716, 0.7612329, 0.7612192, 0.76095515, 0.7612449, 0.76095515, 0.76142496, 0.7616873, 0.7607357, 0.76160324, 0.76202846, 0.76159126, 0.76211417, 0.7621519, 0.76183987, 0.76107347, 0.7620816, 0.7622839, 0.7622651, 0.7621245, 0.76207477, 0.76193243, 0.7617558, 0.7616375, 0.76207477, 0.76188785, 0.7621656, 0.7618587, 0.761941, 0.7624537, 0.7622702, 0.7622925, 0.7623062, 0.76246053, 0.7626337, 0.76298344, 0.7626663, 0.7629526, 0.76243824, 0.7628909, 0.7624451, 0.76264226], 'precision_2': [0.7268899, 0.7325245, 0.73415405, 0.7350423, 0.7352495, 0.73536557, 0.7355812, 0.7356443, 0.7359877, 0.73610663, 0.7363091, 0.7364383, 0.73655266, 0.73656946, 0.7370061, 0.73706335, 0.7372304, 0.73728067, 0.73735946, 0.7375266, 0.7375386, 0.7377548, 0.7379837, 0.73804253, 0.738207, 0.7382222, 0.73828024, 0.73828125, 0.738281, 0.7382812, 0.73828363, 0.7382883, 0.7382953, 0.73833704, 0.7383517, 0.73850626, 0.7385215, 0.7388514, 0.73896426, 0.7389784, 0.73902017, 0.73918694, 0.7392178, 0.73925316, 0.7392615, 0.73931354, 0.73933095, 0.7394813, 0.7394945, 0.7395359], 'recall_2': [0.7934798, 0.80176926, 0.80091184, 0.80090237, 0.80168, 0.80188686, 0.801876, 0.80241525, 0.8026602, 0.80273694, 0.80253, 0.8020449, 0.80194247, 0.8019847, 0.8017907, 0.8017554, 0.80171067, 0.80134577, 0.80100405, 0.800884, 0.80094326, 0.800836, 0.8008498, 0.80078113, 0.800718, 0.8005755, 0.8005219, 0.8007084, 0.8007467, 0.8007769, 0.8007684, 0.8007776, 0.8007684, 0.8007301, 0.8007168, 0.80056304, 0.8003164, 0.80007464, 0.8000228, 0.79987216, 0.79984313, 0.79980457, 0.79975307, 0.7996587, 0.79958564, 0.7991064, 0.7990897, 0.7990822, 0.7990489, 0.7989698]}
144811/144811 [==============================] - 2s 16us/step
results evaluated:
[0.5012056275688418, 0.7598870396614075, 0.7397311925888062, 0.7989093065261841]
predictions:
[[0.42877746 0.5712226 ]
 [0.80317384 0.19682613]
 [0.9162343  0.08376567]
 ...
 [0.44584945 0.5541505 ]
 [0.31160504 0.688395  ]
 [0.12524424 0.8747557 ]]
pred_y:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_17 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_18 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_19 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_20 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_21 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_22 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_23 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_24 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_25 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_26 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_27 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_28 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_29 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_30 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_31 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_32 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_33 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_34 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_35 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_36 (Dense)             (None, 2)                 42        
=================================================================
Total params: 8,262
Trainable params: 8,262
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[ 5.78226430e-03  1.03361441e+00 -1.48767723e-02 -7.16901572e-02
   4.14380086e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 5.98095958e-01  2.81538334e-01  4.89436463e-01  3.96717282e-02
  -2.29826710e-01 -3.78993973e-01  1.89888553e+00 -4.80498332e-01
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [-1.16913506e+00 -4.36462010e-01  1.39720029e+00 -4.08879336e-02
  -1.30311285e+00 -3.78993973e-01 -5.26624687e-01  2.08117268e+00
  -5.44837946e-01 -3.43618462e-01 -3.95407964e-01]
 [ 6.56356322e-01 -1.95104693e-03 -1.52781648e+00  1.36817203e-01
   1.91674557e+00 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]
 [-9.45803670e-01 -5.86628460e-01  6.91161757e-01 -2.09115463e-01
   1.99252634e-01 -3.78993973e-01 -5.26624687e-01 -4.80498332e-01
  -5.44837946e-01  2.91020452e+00 -3.95407964e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[ 0.73403681  1.84104089 -0.62005265 -0.05747375  0.41438009 -0.37899397
  -0.52662469 -0.48049833 -0.54483795 -0.34361846  2.52903353]
 [ 1.98663462  0.45632223  0.18684852 -0.19489905 -0.44436638 -0.37899397
  -0.52662469  2.08117268 -0.54483795 -0.34361846 -0.39540796]
 [-0.88754331 -0.55387429  0.89288705  0.05862694 -1.73277997 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [ 1.15156941  0.71293972  1.90151352 -0.1261864   0.19925263 -0.37899397
   1.89888553 -0.48049833 -0.54483795 -0.34361846 -0.39540796]
 [-1.09145458 -0.64757609  0.18684852 -0.31336914 -0.87403351 -0.37899397
  -0.52662469 -0.48049833 -0.54483795  2.91020452 -0.39540796]]
test_y BEFORE fitting:
[[1. 0.]
 [0. 1.]
 [0. 1.]
 [0. 1.]
 [1. 0.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288321, 294921]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71450, 73361]))
Epoch 1/50
583242/583242 [==============================] - 50s 86us/step - loss: 0.7112 - accuracy: 0.6939 - precision_3: 0.6722 - recall_3: 0.7596
Epoch 2/50
583242/583242 [==============================] - 62s 106us/step - loss: 8.9181 - accuracy: 0.5541 - precision_3: 0.6328 - recall_3: 0.6834
Epoch 3/50
583242/583242 [==============================] - 52s 89us/step - loss: 2177.9732 - accuracy: 0.6216 - precision_3: 0.6152 - recall_3: 0.6666
Epoch 4/50
583242/583242 [==============================] - 48s 83us/step - loss: 12.2450 - accuracy: 0.5722 - precision_3: 0.6036 - recall_3: 0.6609
Epoch 5/50
583242/583242 [==============================] - 48s 82us/step - loss: 2955.3437 - accuracy: 0.5351 - precision_3: 0.5921 - recall_3: 0.6343
Epoch 6/50
583242/583242 [==============================] - 48s 82us/step - loss: 11625.5248 - accuracy: 0.5203 - precision_3: 0.5794 - recall_3: 0.6137
Epoch 7/50
583242/583242 [==============================] - 48s 82us/step - loss: 260074.6580 - accuracy: 0.5123 - precision_3: 0.5692 - recall_3: 0.5990
Epoch 8/50
583242/583242 [==============================] - 48s 83us/step - loss: 14938.0873 - accuracy: 0.5075 - precision_3: 0.5608 - recall_3: 0.5866
Epoch 9/50
583242/583242 [==============================] - 68s 117us/step - loss: 46535.0060 - accuracy: 0.5226 - precision_3: 0.5549 - recall_3: 0.5786
Epoch 10/50
583242/583242 [==============================] - 62s 107us/step - loss: 25457.4565 - accuracy: 0.5178 - precision_3: 0.5509 - recall_3: 0.5706
Epoch 11/50
583242/583242 [==============================] - 52s 90us/step - loss: 230588.4862 - accuracy: 0.5241 - precision_3: 0.5478 - recall_3: 0.5655
Epoch 12/50
583242/583242 [==============================] - 51s 88us/step - loss: 42294.1207 - accuracy: 0.5242 - precision_3: 0.5450 - recall_3: 0.5613
Epoch 13/50
583242/583242 [==============================] - 51s 88us/step - loss: 349033934.3079 - accuracy: 0.5297 - precision_3: 0.5435 - recall_3: 0.5591
Epoch 14/50
583242/583242 [==============================] - 51s 87us/step - loss: 14523465.0908 - accuracy: 0.5175 - precision_3: 0.5415 - recall_3: 0.5568
Epoch 15/50
583242/583242 [==============================] - 53s 91us/step - loss: 49676.9901 - accuracy: 0.5173 - precision_3: 0.5391 - recall_3: 0.5526
Epoch 16/50
583242/583242 [==============================] - 52s 90us/step - loss: 1794775395.0419 - accuracy: 0.5449 - precision_3: 0.5388 - recall_3: 0.5494
Epoch 17/50
583242/583242 [==============================] - 49s 84us/step - loss: 782024.3055 - accuracy: 0.5373 - precision_3: 0.5385 - recall_3: 0.5479
Epoch 18/50
583242/583242 [==============================] - 49s 84us/step - loss: 4050156.4734 - accuracy: 0.5145 - precision_3: 0.5376 - recall_3: 0.5438
Epoch 19/50
583242/583242 [==============================] - 49s 84us/step - loss: 2287700.4573 - accuracy: 0.5156 - precision_3: 0.5364 - recall_3: 0.5418
Epoch 20/50
583242/583242 [==============================] - 49s 84us/step - loss: 2416123.2503 - accuracy: 0.5141 - precision_3: 0.5347 - recall_3: 0.5396
Epoch 21/50
583242/583242 [==============================] - 49s 83us/step - loss: 11954338.9925 - accuracy: 0.5358 - precision_3: 0.5343 - recall_3: 0.5387
Epoch 22/50
583242/583242 [==============================] - 60s 104us/step - loss: 29702564.3804 - accuracy: 0.5200 - precision_3: 0.5337 - recall_3: 0.5367
Epoch 23/50
583242/583242 [==============================] - 51s 88us/step - loss: 453290151.3653 - accuracy: 0.5112 - precision_3: 0.5327 - recall_3: 0.5343
Epoch 24/50
583242/583242 [==============================] - 51s 87us/step - loss: 877036210.6290 - accuracy: 0.5177 - precision_3: 0.5317 - recall_3: 0.5328
Epoch 25/50
583242/583242 [==============================] - 50s 85us/step - loss: 288368114.6322 - accuracy: 0.5089 - precision_3: 0.5308 - recall_3: 0.5311
Epoch 26/50
583242/583242 [==============================] - 49s 84us/step - loss: 221472773607.0898 - accuracy: 0.5171 - precision_3: 0.5298 - recall_3: 0.5291
Epoch 27/50
583242/583242 [==============================] - 49s 83us/step - loss: 4739580.3608 - accuracy: 0.5121 - precision_3: 0.5293 - recall_3: 0.5278
Epoch 28/50
583242/583242 [==============================] - 49s 84us/step - loss: 433255.2522 - accuracy: 0.5153 - precision_3: 0.5285 - recall_3: 0.5260
Epoch 29/50
583242/583242 [==============================] - 53s 91us/step - loss: 64518325481.6234 - accuracy: 0.5380 - precision_3: 0.5281 - recall_3: 0.5238
Epoch 30/50
583242/583242 [==============================] - 50s 86us/step - loss: 149618.0035 - accuracy: 0.5248 - precision_3: 0.5284 - recall_3: 0.5230
Epoch 31/50
583242/583242 [==============================] - 51s 88us/step - loss: 1623994636.9201 - accuracy: 0.5238 - precision_3: 0.5282 - recall_3: 0.5211
Epoch 32/50
583242/583242 [==============================] - 51s 88us/step - loss: 3356856.9830 - accuracy: 0.5273 - precision_3: 0.5279 - recall_3: 0.5196
Epoch 33/50
583242/583242 [==============================] - 50s 87us/step - loss: 9626714.7194 - accuracy: 0.5138 - precision_3: 0.5276 - recall_3: 0.5185
Epoch 34/50
583242/583242 [==============================] - 50s 86us/step - loss: 2105066.9291 - accuracy: 0.5223 - precision_3: 0.5273 - recall_3: 0.5166
Epoch 35/50
583242/583242 [==============================] - 50s 85us/step - loss: 9378967259.4398 - accuracy: 0.5436 - precision_3: 0.5272 - recall_3: 0.5168
Epoch 36/50
583242/583242 [==============================] - 50s 86us/step - loss: 14484903.9064 - accuracy: 0.5247 - precision_3: 0.5273 - recall_3: 0.5168
Epoch 37/50
583242/583242 [==============================] - 49s 85us/step - loss: 162707553.0644 - accuracy: 0.5117 - precision_3: 0.5269 - recall_3: 0.5155
Epoch 38/50
583242/583242 [==============================] - 50s 86us/step - loss: 8426607.3463 - accuracy: 0.5266 - precision_3: 0.5267 - recall_3: 0.5142
Epoch 39/50
583242/583242 [==============================] - 52s 89us/step - loss: 3147980783.3049 - accuracy: 0.5178 - precision_3: 0.5264 - recall_3: 0.5135
Epoch 40/50
583242/583242 [==============================] - 52s 89us/step - loss: 204049459.4782 - accuracy: 0.5213 - precision_3: 0.5263 - recall_3: 0.5135
Epoch 41/50
583242/583242 [==============================] - 56s 96us/step - loss: 41426558.0720 - accuracy: 0.5085 - precision_3: 0.5257 - recall_3: 0.5135
Epoch 42/50
583242/583242 [==============================] - 50s 86us/step - loss: 5231549520.2211 - accuracy: 0.5076 - precision_3: 0.5250 - recall_3: 0.5136
Epoch 43/50
583242/583242 [==============================] - 50s 85us/step - loss: 439404750.3798 - accuracy: 0.5085 - precision_3: 0.5245 - recall_3: 0.5131
Epoch 44/50
583242/583242 [==============================] - 50s 85us/step - loss: 4476564284.5591 - accuracy: 0.5297 - precision_3: 0.5244 - recall_3: 0.5132
Epoch 45/50
583242/583242 [==============================] - 50s 86us/step - loss: 618883041.4727 - accuracy: 0.5240 - precision_3: 0.5243 - recall_3: 0.5136
Epoch 46/50
583242/583242 [==============================] - 51s 87us/step - loss: 130741414.5818 - accuracy: 0.5179 - precision_3: 0.5240 - recall_3: 0.5139
Epoch 47/50
583242/583242 [==============================] - 54s 93us/step - loss: 7683539571782.0420 - accuracy: 0.5304 - precision_3: 0.5238 - recall_3: 0.5144
Epoch 48/50
583242/583242 [==============================] - 52s 89us/step - loss: 91812652.2061 - accuracy: 0.5075 - precision_3: 0.5236 - recall_3: 0.5151
Epoch 49/50
583242/583242 [==============================] - 53s 90us/step - loss: 3793538174.3360 - accuracy: 0.5310 - precision_3: 0.5234 - recall_3: 0.5155
Epoch 50/50
583242/583242 [==============================] - 52s 89us/step - loss: 2955576444.6746 - accuracy: 0.5302 - precision_3: 0.5234 - recall_3: 0.5155
history.history:
{'loss': [0.7112257181736606, 8.918127008474194, 2177.9731882878373, 12.24499801657092, 2955.3437077107733, 11625.52479528444, 260074.65803569916, 14938.087291447446, 46535.00598173006, 25457.456474923423, 230588.48621812748, 42294.12074849235, 349033934.3079036, 14523465.090847408, 49676.9900650268, 1794775395.0418572, 782024.3055254164, 4050156.4734323537, 2287700.4572670804, 2416123.250317252, 11954338.992483158, 29702564.380418833, 453290151.36528, 877036210.6289724, 288368114.6322119, 221472773607.08975, 4739580.360785636, 433255.25222741737, 64518325481.62335, 149618.0034836978, 1623994636.9200573, 3356856.9830339155, 9626714.719395556, 2105066.929128561, 9378967259.439835, 14484903.906414693, 162707553.06439656, 8426607.346309736, 3147980783.3048673, 204049459.47823203, 41426558.07198837, 5231549520.221121, 439404750.37980086, 4476564284.559051, 618883041.4726921, 130741414.58179837, 7683539571782.042, 91812652.20605405, 3793538174.336028, 2955576444.6746426], 'accuracy': [0.6939384, 0.5540856, 0.62157047, 0.572181, 0.53511924, 0.5203432, 0.5123054, 0.5075183, 0.5225738, 0.51784164, 0.5240518, 0.52417177, 0.5296738, 0.51748157, 0.5172501, 0.54494363, 0.5372967, 0.5145034, 0.51560074, 0.51410395, 0.5357793, 0.51997113, 0.51118404, 0.5176822, 0.5089294, 0.51708555, 0.5121202, 0.5153144, 0.5380305, 0.5247753, 0.5237603, 0.52733, 0.5138262, 0.52227205, 0.54359424, 0.524729, 0.5117413, 0.526555, 0.51781595, 0.5212588, 0.5084716, 0.507556, 0.50848705, 0.5296772, 0.52395403, 0.5178828, 0.53035104, 0.50751144, 0.53104544, 0.5301967], 'precision_3': [0.6722386, 0.6328295, 0.615203, 0.6036221, 0.5920996, 0.57937944, 0.56917, 0.5608457, 0.5548563, 0.5508669, 0.5478444, 0.5450117, 0.54353213, 0.5415029, 0.5390914, 0.53881824, 0.5385241, 0.5376124, 0.5364033, 0.5347414, 0.5342563, 0.5337483, 0.53274703, 0.5317384, 0.53078896, 0.5298126, 0.5293245, 0.5284589, 0.5281406, 0.5283561, 0.5281823, 0.5279354, 0.52760065, 0.5273391, 0.5272391, 0.52734613, 0.52689224, 0.5267012, 0.52640855, 0.52634627, 0.52566624, 0.52497697, 0.5245488, 0.52439237, 0.5243442, 0.5239953, 0.52384174, 0.5235752, 0.52341807, 0.52343154], 'recall_3': [0.75956774, 0.68340284, 0.666556, 0.6609458, 0.6343318, 0.6136963, 0.5990408, 0.58656263, 0.57858264, 0.5706347, 0.5654512, 0.5612879, 0.5591346, 0.5568278, 0.5526234, 0.5494, 0.54791677, 0.5437518, 0.5418242, 0.5396234, 0.53871024, 0.53668076, 0.53426987, 0.5327714, 0.5310719, 0.52912384, 0.52783227, 0.5259869, 0.5238111, 0.52302235, 0.5211388, 0.51961356, 0.5185094, 0.51659715, 0.51681954, 0.51684904, 0.5155037, 0.51422054, 0.51354337, 0.5134812, 0.51347506, 0.51357985, 0.5131403, 0.51322246, 0.51363826, 0.5139238, 0.5144166, 0.5150959, 0.5154819, 0.51545465]}
144811/144811 [==============================] - 4s 26us/step
results evaluated:
[2918639.5370569224, 0.5065982341766357, 0.5234428644180298, 0.5144236087799072]
predictions:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [1. 0.]
 [1. 0.]]
pred_y:
[[0. 1.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [0. 1.]
 [0. 1.]]