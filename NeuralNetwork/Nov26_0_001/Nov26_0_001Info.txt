_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 5)                 60        
_________________________________________________________________
dense_2 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_4 (Dense)              (None, 5)                 30        
_________________________________________________________________
dense_5 (Dense)              (None, 2)                 12        
=================================================================
Total params: 162
Trainable params: 162
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.18308677e-02 -1.83090326e-01  8.93921614e-01 -1.38253894e-03
  -2.29714010e-01  2.63671230e+00 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-4.79774948e-01 -5.67746610e-02  2.87845387e-01 -1.14050112e-01
  -1.51240296e-02 -3.79260187e-01  1.89914415e+00 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-7.03172897e-01  3.79667924e-01 -1.16205432e-01 -8.11887362e-02
  -6.59481890e-01 -3.79260187e-01 -5.26552974e-01  2.08072789e+00
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [ 1.41854127e-01  1.25501966e-01 -8.23294363e-01  7.37291761e-02
   1.99465951e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01  2.91139377e+00 -3.94941094e-01]
 [ 8.99464562e-01 -1.08025906e-01  3.88858091e-01 -2.26717684e-01
  -4.26078541e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
   1.83447261e+00 -3.43478100e-01 -3.94941094e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.50891381 -0.5332763  -0.82329436  0.0408678   0.62923383 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [ 0.05443754 -0.04748752  0.79290891  0.09720159  0.19946595 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-0.84886721  0.34389524 -0.82329436 -0.06241081  0.62923383  2.6367123
  -0.52655297 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-1.18882061 -0.64300212 -1.95463665  0.13006296  1.27359169 -0.37926019
  -0.52655297 -0.48060105 -0.5451158   2.91139377 -0.39494109]
 [-1.2762372  -0.653427    0.5908835   0.07842366 -0.01512403 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288578, 294664]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71196, 73615]))
Epoch 1/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5099 - accuracy: 0.7531 - precision: 0.7210 - recall: 0.7868 - f1_score: 0.0000e+00
Epoch 2/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5043 - accuracy: 0.7569 - precision: 0.7342 - recall: 0.7894 - f1_score: 0.0000e+00
Epoch 3/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5035 - accuracy: 0.7566 - precision: 0.7344 - recall: 0.7920 - f1_score: 0.0000e+00
Epoch 4/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5025 - accuracy: 0.7575 - precision: 0.7326 - recall: 0.7970 - f1_score: 0.0000e+00
Epoch 5/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5017 - accuracy: 0.7585 - precision: 0.7331 - recall: 0.7980 - f1_score: 0.0000e+00
Epoch 6/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5015 - accuracy: 0.7582 - precision: 0.7334 - recall: 0.7984 - f1_score: 0.0000e+00
Epoch 7/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5009 - accuracy: 0.7587 - precision: 0.7335 - recall: 0.7992 - f1_score: 0.0000e+00
Epoch 8/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5009 - accuracy: 0.7586 - precision: 0.7334 - recall: 0.8002 - f1_score: 4.2734e-07
Epoch 9/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5004 - accuracy: 0.7588 - precision: 0.7332 - recall: 0.8014 - f1_score: 3.9971e-07
Epoch 10/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5003 - accuracy: 0.7593 - precision: 0.7332 - recall: 0.8021 - f1_score: 3.5756e-07
Epoch 11/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5001 - accuracy: 0.7590 - precision: 0.7331 - recall: 0.8027 - f1_score: 3.2345e-07
Epoch 12/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5002 - accuracy: 0.7592 - precision: 0.7332 - recall: 0.8027 - f1_score: 2.9531e-07
Epoch 13/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5000 - accuracy: 0.7593 - precision: 0.7333 - recall: 0.8028 - f1_score: 2.7163e-07
Epoch 14/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.5000 - accuracy: 0.7588 - precision: 0.7332 - recall: 0.8036 - f1_score: 2.5150e-07
Epoch 15/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4999 - accuracy: 0.7591 - precision: 0.7331 - recall: 0.8037 - f1_score: 2.3414e-07
Epoch 16/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4999 - accuracy: 0.7587 - precision: 0.7328 - recall: 0.8043 - f1_score: 2.1902e-07
Epoch 17/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4998 - accuracy: 0.7586 - precision: 0.7327 - recall: 0.8047 - f1_score: 2.0574e-07
Epoch 18/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4995 - accuracy: 0.7587 - precision: 0.7325 - recall: 0.8053 - f1_score: 5.1220e-07
Epoch 19/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4998 - accuracy: 0.7588 - precision: 0.7324 - recall: 0.8056 - f1_score: 5.5820e-07
Epoch 20/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4995 - accuracy: 0.7587 - precision: 0.7324 - recall: 0.8059 - f1_score: 5.2956e-07
Epoch 21/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4996 - accuracy: 0.7589 - precision: 0.7324 - recall: 0.8064 - f1_score: 5.0372e-07
Epoch 22/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4995 - accuracy: 0.7590 - precision: 0.7323 - recall: 0.8065 - f1_score: 4.8028e-07
Epoch 23/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4995 - accuracy: 0.7591 - precision: 0.7324 - recall: 0.8066 - f1_score: 4.5893e-07
Epoch 24/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4994 - accuracy: 0.7592 - precision: 0.7323 - recall: 0.8066 - f1_score: 4.3939e-07
Epoch 25/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4996 - accuracy: 0.7590 - precision: 0.7323 - recall: 0.8067 - f1_score: 4.2145e-07
Epoch 26/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4996 - accuracy: 0.7592 - precision: 0.7324 - recall: 0.8066 - f1_score: 4.0492e-07
Epoch 27/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4993 - accuracy: 0.7588 - precision: 0.7323 - recall: 0.8068 - f1_score: 4.7511e-07
Epoch 28/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4995 - accuracy: 0.7589 - precision: 0.7322 - recall: 0.8073 - f1_score: 4.9889e-07
Epoch 29/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4992 - accuracy: 0.7588 - precision: 0.7322 - recall: 0.8074 - f1_score: 4.8138e-07
Epoch 30/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4994 - accuracy: 0.7592 - precision: 0.7322 - recall: 0.8074 - f1_score: 4.6506e-07
Epoch 31/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4994 - accuracy: 0.7589 - precision: 0.7322 - recall: 0.8074 - f1_score: 4.4981e-07
Epoch 32/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4995 - accuracy: 0.7587 - precision: 0.7322 - recall: 0.8074 - f1_score: 4.3552e-07
Epoch 33/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4994 - accuracy: 0.7596 - precision: 0.7323 - recall: 0.8074 - f1_score: 4.2212e-07
Epoch 34/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4994 - accuracy: 0.7591 - precision: 0.7324 - recall: 0.8069 - f1_score: 4.0952e-07
Epoch 35/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4994 - accuracy: 0.7592 - precision: 0.7324 - recall: 0.8068 - f1_score: 3.9765e-07
Epoch 36/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4992 - accuracy: 0.7595 - precision: 0.7324 - recall: 0.8069 - f1_score: 3.8644e-07
Epoch 37/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4993 - accuracy: 0.7594 - precision: 0.7324 - recall: 0.8069 - f1_score: 3.8666e-07
Epoch 38/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4994 - accuracy: 0.7593 - precision: 0.7324 - recall: 0.8067 - f1_score: 4.5825e-07
Epoch 39/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4993 - accuracy: 0.7590 - precision: 0.7324 - recall: 0.8067 - f1_score: 4.4633e-07
Epoch 40/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4995 - accuracy: 0.7593 - precision: 0.7325 - recall: 0.8067 - f1_score: 4.3503e-07
Epoch 41/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4992 - accuracy: 0.7592 - precision: 0.7324 - recall: 0.8069 - f1_score: 4.2430e-07
Epoch 42/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4994 - accuracy: 0.7589 - precision: 0.7324 - recall: 0.8069 - f1_score: 4.1407e-07
Epoch 43/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4995 - accuracy: 0.7589 - precision: 0.7324 - recall: 0.8069 - f1_score: 4.0433e-07
Epoch 44/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4992 - accuracy: 0.7595 - precision: 0.7324 - recall: 0.8069 - f1_score: 3.9503e-07
Epoch 45/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.4995 - accuracy: 0.7591 - precision: 0.7325 - recall: 0.8069 - f1_score: 3.8615e-07
Epoch 46/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.4991 - accuracy: 0.7591 - precision: 0.7325 - recall: 0.8068 - f1_score: 3.7767e-07
Epoch 47/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4991 - accuracy: 0.7596 - precision: 0.7325 - recall: 0.8069 - f1_score: 3.6955e-07
Epoch 48/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4990 - accuracy: 0.7593 - precision: 0.7325 - recall: 0.8069 - f1_score: 3.6176e-07
Epoch 49/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4991 - accuracy: 0.7597 - precision: 0.7325 - recall: 0.8073 - f1_score: 3.5430e-07
Epoch 50/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4993 - accuracy: 0.7594 - precision: 0.7325 - recall: 0.8073 - f1_score: 3.4715e-07
history.history:
{'loss': [0.5099027304477648, 0.5043134360214883, 0.5035042066109778, 0.5024744915975137, 0.501678752226152, 0.5015310223134594, 0.5009110173715813, 0.500883395450015, 0.5004412672996822, 0.5002858068774761, 0.5000952197423348, 0.5001799588831588, 0.5000258045614946, 0.5000208452916797, 0.49989212045766157, 0.49992626375803445, 0.4998265417347561, 0.49951852909097827, 0.4998355918271429, 0.4994932741170887, 0.49958542403207595, 0.4995167685415294, 0.49948547941251786, 0.4994407970024437, 0.49955655719649517, 0.4996328595714967, 0.4993478444229785, 0.49951673056490414, 0.4992209150688965, 0.4993916899312789, 0.499440912545167, 0.49952162117515375, 0.4993888172799248, 0.4993527128755298, 0.49938053350847317, 0.4992446635292116, 0.4993091497482222, 0.49938120138458725, 0.49925141624895325, 0.4994779964118636, 0.49921968288985846, 0.49938518124350484, 0.49945431769499804, 0.49921797371852966, 0.49949338989997116, 0.49912325022194415, 0.499128167721692, 0.4990074127021071, 0.49907880603448945, 0.4993007938086331], 'accuracy': [0.7530991, 0.7569208, 0.75660187, 0.7575466, 0.7584639, 0.75822383, 0.7587091, 0.7586062, 0.75876737, 0.75928, 0.7590126, 0.7592183, 0.7592612, 0.7587931, 0.759136, 0.7587142, 0.758553, 0.7587108, 0.75878626, 0.7586731, 0.7589354, 0.75898, 0.7591377, 0.7591943, 0.75896114, 0.7591823, 0.7588325, 0.75887537, 0.7588188, 0.7592029, 0.75885826, 0.7586834, 0.75959724, 0.7591034, 0.7592166, 0.75953037, 0.75943774, 0.7592697, 0.75902456, 0.7593023, 0.75921315, 0.75892514, 0.75886685, 0.7594789, 0.75905025, 0.75906056, 0.75961095, 0.7593006, 0.7596812, 0.7593555], 'precision': [0.7209789, 0.7341763, 0.7344099, 0.7326161, 0.73307174, 0.733379, 0.7334676, 0.7334159, 0.7331775, 0.7331521, 0.7331091, 0.7331738, 0.7333071, 0.7331705, 0.73312145, 0.7327602, 0.73269725, 0.73249793, 0.7324395, 0.73242056, 0.73236334, 0.7323498, 0.73235285, 0.73234904, 0.7323478, 0.73236537, 0.7323183, 0.73219526, 0.7321797, 0.73217934, 0.7321942, 0.7321959, 0.732342, 0.7323516, 0.7324067, 0.7324206, 0.7324208, 0.73243594, 0.73244053, 0.7324784, 0.73243713, 0.732436, 0.7324371, 0.73243654, 0.7324783, 0.7324815, 0.7324792, 0.7324789, 0.7324789, 0.7324783], 'recall': [0.7868256, 0.7893731, 0.7919834, 0.796988, 0.79798144, 0.7983962, 0.79921824, 0.8002165, 0.80142725, 0.80213076, 0.8026722, 0.802685, 0.8028072, 0.8036205, 0.80372804, 0.8042712, 0.80466604, 0.805323, 0.8055761, 0.80585724, 0.80636835, 0.80650234, 0.80658305, 0.8066374, 0.8066824, 0.8066377, 0.806801, 0.8073413, 0.8073965, 0.8074091, 0.8073965, 0.8073976, 0.8074005, 0.80692536, 0.8067828, 0.8068571, 0.8068615, 0.80670726, 0.8066996, 0.80669224, 0.8068509, 0.806876, 0.80687684, 0.8068987, 0.80685765, 0.8068489, 0.8068943, 0.8068994, 0.8073225, 0.8073479], 'f1_score': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.273447e-07, 3.9970945e-07, 3.5756347e-07, 3.2345318e-07, 2.953113e-07, 2.7163463e-07, 2.5150192e-07, 2.3414158e-07, 2.190207e-07, 2.0574399e-07, 5.121974e-07, 5.582038e-07, 5.2956324e-07, 5.037175e-07, 4.8027715e-07, 4.5892568e-07, 4.3939212e-07, 4.2145382e-07, 4.0492418e-07, 4.7511446e-07, 4.9888746e-07, 4.81379e-07, 4.6505662e-07, 4.4980774e-07, 4.3551938e-07, 4.2212267e-07, 4.0952114e-07, 3.9764885e-07, 3.8644487e-07, 3.8665763e-07, 4.582483e-07, 4.4633433e-07, 4.350323e-07, 4.2429753e-07, 4.1407446e-07, 4.043282e-07, 3.9503277e-07, 3.8615497e-07, 3.7766685e-07, 3.6954503e-07, 3.6176195e-07, 3.5430395e-07, 3.4714643e-07]}
144811/144811 [==============================] - 2s 13us/step
results evaluated:
[0.4985604660522097, 0.7557920217514038, 0.7325029969215393, 0.8071302175521851, 3.428156958307227e-07]
1615/1615 [==============================] - 0s 13us/step
auto results evaluated:
[0.5477016767850232, 0.900928795337677, 0.7325360178947449, 0.8071702718734741, 3.4195994658148265e-07]
predictions:
[[0.9465944  0.0534056 ]
 [0.21628577 0.7837142 ]
 [0.9396375  0.06036255]
 ...
 [0.66841507 0.33158487]
 [0.96326405 0.036736  ]
 [0.37789854 0.6221015 ]]
pred_y:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 ...
 [0. 1.]
 [1. 0.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_6 (Dense)              (None, 10)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_8 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_9 (Dense)              (None, 10)                110       
_________________________________________________________________
dense_10 (Dense)             (None, 2)                 22        
=================================================================
Total params: 472
Trainable params: 472
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.18308677e-02 -1.83090326e-01  8.93921614e-01 -1.38253894e-03
  -2.29714010e-01  2.63671230e+00 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-4.79774948e-01 -5.67746610e-02  2.87845387e-01 -1.14050112e-01
  -1.51240296e-02 -3.79260187e-01  1.89914415e+00 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-7.03172897e-01  3.79667924e-01 -1.16205432e-01 -8.11887362e-02
  -6.59481890e-01 -3.79260187e-01 -5.26552974e-01  2.08072789e+00
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [ 1.41854127e-01  1.25501966e-01 -8.23294363e-01  7.37291761e-02
   1.99465951e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01  2.91139377e+00 -3.94941094e-01]
 [ 8.99464562e-01 -1.08025906e-01  3.88858091e-01 -2.26717684e-01
  -4.26078541e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
   1.83447261e+00 -3.43478100e-01 -3.94941094e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.50891381 -0.5332763  -0.82329436  0.0408678   0.62923383 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [ 0.05443754 -0.04748752  0.79290891  0.09720159  0.19946595 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-0.84886721  0.34389524 -0.82329436 -0.06241081  0.62923383  2.6367123
  -0.52655297 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-1.18882061 -0.64300212 -1.95463665  0.13006296  1.27359169 -0.37926019
  -0.52655297 -0.48060105 -0.5451158   2.91139377 -0.39494109]
 [-1.2762372  -0.653427    0.5908835   0.07842366 -0.01512403 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288578, 294664]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71196, 73615]))
Epoch 1/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.5069 - accuracy: 0.7544 - precision_1: 0.7293 - recall_1: 0.7743 - f1_score: 0.0000e+00     
Epoch 2/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4990 - accuracy: 0.7596 - precision_1: 0.7375 - recall_1: 0.7862 - f1_score: 0.0000e+00
Epoch 3/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4970 - accuracy: 0.7609 - precision_1: 0.7390 - recall_1: 0.7891 - f1_score: 0.0000e+00
Epoch 4/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4962 - accuracy: 0.7616 - precision_1: 0.7394 - recall_1: 0.7909 - f1_score: 0.0000e+00
Epoch 5/50
583242/583242 [==============================] - 18s 31us/step - loss: 0.4955 - accuracy: 0.7619 - precision_1: 0.7399 - recall_1: 0.7926 - f1_score: 1.0514e-06
Epoch 6/50
583242/583242 [==============================] - 18s 30us/step - loss: 0.4950 - accuracy: 0.7622 - precision_1: 0.7401 - recall_1: 0.7936 - f1_score: 3.5728e-06
Epoch 7/50
583242/583242 [==============================] - 18s 30us/step - loss: 0.4948 - accuracy: 0.7620 - precision_1: 0.7402 - recall_1: 0.7940 - f1_score: 3.1831e-06
Epoch 8/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4949 - accuracy: 0.7626 - precision_1: 0.7403 - recall_1: 0.7946 - f1_score: 2.7571e-06
Epoch 9/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4947 - accuracy: 0.7626 - precision_1: 0.7406 - recall_1: 0.7949 - f1_score: 2.4320e-06
Epoch 10/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4946 - accuracy: 0.7626 - precision_1: 0.7409 - recall_1: 0.7949 - f1_score: 2.9600e-06
Epoch 11/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4944 - accuracy: 0.7623 - precision_1: 0.7410 - recall_1: 0.7951 - f1_score: 2.9384e-06
Epoch 12/50
583242/583242 [==============================] - 16s 28us/step - loss: 0.4945 - accuracy: 0.7624 - precision_1: 0.7410 - recall_1: 0.7954 - f1_score: 2.6825e-06
Epoch 13/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4943 - accuracy: 0.7625 - precision_1: 0.7411 - recall_1: 0.7956 - f1_score: 2.4677e-06
Epoch 14/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4944 - accuracy: 0.7632 - precision_1: 0.7412 - recall_1: 0.7958 - f1_score: 2.2847e-06
Epoch 15/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7628 - precision_1: 0.7412 - recall_1: 0.7959 - f1_score: 2.1270e-06
Epoch 16/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4944 - accuracy: 0.7630 - precision_1: 0.7412 - recall_1: 0.7959 - f1_score: 2.0940e-06
Epoch 17/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4943 - accuracy: 0.7629 - precision_1: 0.7413 - recall_1: 0.7960 - f1_score: 2.0791e-06
Epoch 18/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7626 - precision_1: 0.7413 - recall_1: 0.7961 - f1_score: 2.0943e-06
Epoch 19/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4942 - accuracy: 0.7630 - precision_1: 0.7413 - recall_1: 0.7966 - f1_score: 2.1625e-06
Epoch 20/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4943 - accuracy: 0.7630 - precision_1: 0.7414 - recall_1: 0.7966 - f1_score: 2.2098e-06
Epoch 21/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4942 - accuracy: 0.7625 - precision_1: 0.7414 - recall_1: 0.7967 - f1_score: 2.1735e-06
Epoch 22/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4944 - accuracy: 0.7625 - precision_1: 0.7414 - recall_1: 0.7967 - f1_score: 2.2385e-06
Epoch 23/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4943 - accuracy: 0.7627 - precision_1: 0.7414 - recall_1: 0.7967 - f1_score: 2.6654e-06
Epoch 24/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7629 - precision_1: 0.7414 - recall_1: 0.7968 - f1_score: 2.6272e-06
Epoch 25/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7627 - precision_1: 0.7414 - recall_1: 0.7968 - f1_score: 2.5199e-06
Epoch 26/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7630 - precision_1: 0.7415 - recall_1: 0.7969 - f1_score: 2.7773e-06
Epoch 27/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4942 - accuracy: 0.7628 - precision_1: 0.7415 - recall_1: 0.7969 - f1_score: 2.9728e-06
Epoch 28/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7628 - precision_1: 0.7415 - recall_1: 0.7969 - f1_score: 2.9797e-06
Epoch 29/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7628 - precision_1: 0.7415 - recall_1: 0.7969 - f1_score: 3.0665e-06
Epoch 30/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7629 - precision_1: 0.7418 - recall_1: 0.7969 - f1_score: 3.1222e-06
Epoch 31/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4942 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 3.0327e-06
Epoch 32/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.9364e-06
Epoch 33/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7630 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.8460e-06
Epoch 34/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4939 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.7610e-06
Epoch 35/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4939 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.6810e-06
Epoch 36/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4939 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.6055e-06
Epoch 37/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7628 - precision_1: 0.7419 - recall_1: 0.7969 - f1_score: 2.8012e-06
Epoch 38/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7630 - precision_1: 0.7420 - recall_1: 0.7969 - f1_score: 2.7778e-06
Epoch 39/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7629 - precision_1: 0.7420 - recall_1: 0.7969 - f1_score: 2.7550e-06
Epoch 40/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4938 - accuracy: 0.7630 - precision_1: 0.7420 - recall_1: 0.7969 - f1_score: 2.8840e-06
Epoch 41/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4939 - accuracy: 0.7627 - precision_1: 0.7420 - recall_1: 0.7969 - f1_score: 2.9542e-06
Epoch 42/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4939 - accuracy: 0.7626 - precision_1: 0.7420 - recall_1: 0.7969 - f1_score: 2.8830e-06
Epoch 43/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4940 - accuracy: 0.7629 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 2.8955e-06
Epoch 44/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4942 - accuracy: 0.7628 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 2.9878e-06
Epoch 45/50
583242/583242 [==============================] - 16s 27us/step - loss: 0.4941 - accuracy: 0.7622 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 3.0555e-06
Epoch 46/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4940 - accuracy: 0.7632 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 3.0056e-06
Epoch 47/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4940 - accuracy: 0.7628 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 2.9604e-06
Epoch 48/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4938 - accuracy: 0.7625 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 2.9696e-06
Epoch 49/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4942 - accuracy: 0.7628 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 3.3065e-06
Epoch 50/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4940 - accuracy: 0.7625 - precision_1: 0.7421 - recall_1: 0.7969 - f1_score: 3.5445e-06
history.history:
{'loss': [0.5069368089483713, 0.49898115294169343, 0.49696449627894546, 0.49615376385270227, 0.49548722420538804, 0.4950154935085629, 0.49480060987922914, 0.49488114727562477, 0.4947142773212419, 0.4946130277337735, 0.4943557993902976, 0.4944602958086566, 0.4943166480474613, 0.4943698134106674, 0.4940156983921096, 0.4943755133623309, 0.4943440192774972, 0.4940772304630289, 0.4941895108819112, 0.494266180564729, 0.49416899500879957, 0.49441634726083034, 0.49426594696869935, 0.49414904312916413, 0.49408710083326945, 0.494099891381342, 0.4941726085182893, 0.494023537200737, 0.4941011061635563, 0.4940352455040036, 0.49415280722632315, 0.49405175631640524, 0.4939676850767332, 0.49388675179051417, 0.49394604016814436, 0.49390152721111097, 0.49408856094032516, 0.4939985584355452, 0.49405119947538007, 0.49381121828090857, 0.49392076118079037, 0.49390118132099625, 0.4940373488920128, 0.49416347964449686, 0.4940739102032163, 0.4939814416750864, 0.4940437080160132, 0.49380758628876903, 0.4941897711834874, 0.49397566909969837], 'accuracy': [0.7543884, 0.7595955, 0.76094484, 0.7616427, 0.76188785, 0.7621759, 0.7620319, 0.76259595, 0.7625874, 0.76255655, 0.76231134, 0.76244164, 0.76245195, 0.76324236, 0.7627897, 0.76304173, 0.76290286, 0.76255655, 0.7630229, 0.7629783, 0.76251197, 0.76254624, 0.76268, 0.76293546, 0.7626697, 0.7629543, 0.7627966, 0.762776, 0.7628017, 0.76294917, 0.762776, 0.7628429, 0.7629749, 0.7627537, 0.76276743, 0.76275545, 0.7627537, 0.76300573, 0.76294404, 0.76296115, 0.7626851, 0.762608, 0.76291317, 0.76278114, 0.76216733, 0.76321834, 0.7627571, 0.7624862, 0.76279145, 0.76253253], 'precision_1': [0.72931445, 0.7375025, 0.73897576, 0.73944056, 0.7399045, 0.74011964, 0.7402271, 0.7402841, 0.74063915, 0.74091625, 0.7409843, 0.7410026, 0.74114126, 0.7411545, 0.7412063, 0.74122983, 0.7412702, 0.74127054, 0.7412711, 0.74137276, 0.7414339, 0.74143386, 0.7414368, 0.7414412, 0.7414479, 0.74147683, 0.74148947, 0.7414929, 0.7415057, 0.74175644, 0.74189264, 0.7418959, 0.7419075, 0.7419472, 0.741948, 0.74194825, 0.7419498, 0.74195164, 0.7419645, 0.7419612, 0.7419646, 0.74200135, 0.7420605, 0.7421057, 0.7421158, 0.74211705, 0.7421173, 0.7421218, 0.74211955, 0.7421275], 'recall_1': [0.7743375, 0.7862057, 0.7891023, 0.7908802, 0.7925674, 0.7935824, 0.79398674, 0.7946344, 0.79490274, 0.7949221, 0.79512316, 0.79542226, 0.79564303, 0.7958393, 0.7959112, 0.7959017, 0.79597116, 0.7961214, 0.79659355, 0.79664797, 0.79665095, 0.7966884, 0.7966704, 0.79681206, 0.7968224, 0.7968583, 0.7968617, 0.7968706, 0.7968633, 0.79687494, 0.7968714, 0.79687476, 0.7968684, 0.79686177, 0.7968652, 0.7968718, 0.79687476, 0.7968742, 0.79687065, 0.79688793, 0.79687834, 0.79687524, 0.79687923, 0.7968791, 0.7968788, 0.7968751, 0.7968783, 0.7968761, 0.7968786, 0.7968783], 'f1_score': [0.0, 0.0, 0.0, 0.0, 1.0514299e-06, 3.5727878e-06, 3.1830791e-06, 2.7571182e-06, 2.4319836e-06, 2.9599987e-06, 2.938368e-06, 2.682483e-06, 2.4676633e-06, 2.2847005e-06, 2.127018e-06, 2.0940136e-06, 2.0791097e-06, 2.0942546e-06, 2.1625299e-06, 2.2097652e-06, 2.1734736e-06, 2.238541e-06, 2.6654398e-06, 2.6272346e-06, 2.5199463e-06, 2.7772978e-06, 2.9728135e-06, 2.979729e-06, 3.0664507e-06, 3.1222316e-06, 3.0326669e-06, 2.9363916e-06, 2.8460156e-06, 2.761036e-06, 2.681019e-06, 2.6054809e-06, 2.8012284e-06, 2.777809e-06, 2.755036e-06, 2.8840384e-06, 2.954164e-06, 2.8829868e-06, 2.895535e-06, 2.9877551e-06, 3.0554936e-06, 3.0055837e-06, 2.9603714e-06, 2.9696096e-06, 3.3065296e-06, 3.544456e-06]}
144811/144811 [==============================] - 2s 13us/step
results evaluated:
[0.4926235782721536, 0.76186203956604, 0.7419940233230591, 0.7968794107437134, 3.6198207453708164e-06]
1615/1615 [==============================] - 0s 15us/step
auto results evaluated:
[0.8103726393667168, 0.900928795337677, 0.7420122027397156, 0.7968800663948059, 3.610738986026263e-06]
predictions:
[[0.9408467  0.05915328]
 [0.23356958 0.76643044]
 [0.9084165  0.09158351]
 ...
 [0.59231794 0.40768206]
 [0.94897157 0.05102841]
 [0.48758477 0.51241523]]
pred_y:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 ...
 [0. 1.]
 [1. 0.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_11 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_12 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_13 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_14 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_15 (Dense)             (None, 2)                 42        
=================================================================
Total params: 1,542
Trainable params: 1,542
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.18308677e-02 -1.83090326e-01  8.93921614e-01 -1.38253894e-03
  -2.29714010e-01  2.63671230e+00 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-4.79774948e-01 -5.67746610e-02  2.87845387e-01 -1.14050112e-01
  -1.51240296e-02 -3.79260187e-01  1.89914415e+00 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-7.03172897e-01  3.79667924e-01 -1.16205432e-01 -8.11887362e-02
  -6.59481890e-01 -3.79260187e-01 -5.26552974e-01  2.08072789e+00
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [ 1.41854127e-01  1.25501966e-01 -8.23294363e-01  7.37291761e-02
   1.99465951e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01  2.91139377e+00 -3.94941094e-01]
 [ 8.99464562e-01 -1.08025906e-01  3.88858091e-01 -2.26717684e-01
  -4.26078541e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
   1.83447261e+00 -3.43478100e-01 -3.94941094e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.50891381 -0.5332763  -0.82329436  0.0408678   0.62923383 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [ 0.05443754 -0.04748752  0.79290891  0.09720159  0.19946595 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-0.84886721  0.34389524 -0.82329436 -0.06241081  0.62923383  2.6367123
  -0.52655297 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-1.18882061 -0.64300212 -1.95463665  0.13006296  1.27359169 -0.37926019
  -0.52655297 -0.48060105 -0.5451158   2.91139377 -0.39494109]
 [-1.2762372  -0.653427    0.5908835   0.07842366 -0.01512403 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288578, 294664]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71196, 73615]))
Epoch 1/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.5063 - accuracy: 0.7563 - precision_2: 0.7293 - recall_2: 0.7898 - f1_score: 2.4240e-05
Epoch 2/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4996 - accuracy: 0.7598 - precision_2: 0.7329 - recall_2: 0.8019 - f1_score: 2.6459e-05
Epoch 3/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4992 - accuracy: 0.7599 - precision_2: 0.7331 - recall_2: 0.8045 - f1_score: 2.3113e-05
Epoch 4/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4985 - accuracy: 0.7608 - precision_2: 0.7343 - recall_2: 0.8038 - f1_score: 2.8664e-05
Epoch 5/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4967 - accuracy: 0.7611 - precision_2: 0.7350 - recall_2: 0.8036 - f1_score: 2.6034e-05
Epoch 6/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4975 - accuracy: 0.7611 - precision_2: 0.7359 - recall_2: 0.8027 - f1_score: 2.3447e-05
Epoch 7/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4966 - accuracy: 0.7607 - precision_2: 0.7362 - recall_2: 0.8021 - f1_score: 2.3111e-05
Epoch 8/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4967 - accuracy: 0.7611 - precision_2: 0.7363 - recall_2: 0.8026 - f1_score: 2.3749e-05
Epoch 9/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4970 - accuracy: 0.7611 - precision_2: 0.7363 - recall_2: 0.8026 - f1_score: 2.5946e-05
Epoch 10/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4965 - accuracy: 0.7612 - precision_2: 0.7364 - recall_2: 0.8026 - f1_score: 2.6992e-05
Epoch 11/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4958 - accuracy: 0.7611 - precision_2: 0.7364 - recall_2: 0.8031 - f1_score: 2.8088e-05
Epoch 12/50
583242/583242 [==============================] - 19s 32us/step - loss: 0.4958 - accuracy: 0.7615 - precision_2: 0.7363 - recall_2: 0.8031 - f1_score: 3.0322e-05
Epoch 13/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4985 - accuracy: 0.7612 - precision_2: 0.7366 - recall_2: 0.8029 - f1_score: 3.4986e-05
Epoch 14/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4969 - accuracy: 0.7616 - precision_2: 0.7370 - recall_2: 0.8027 - f1_score: 4.2036e-05
Epoch 15/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4960 - accuracy: 0.7616 - precision_2: 0.7372 - recall_2: 0.8025 - f1_score: 5.5170e-05
Epoch 16/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4963 - accuracy: 0.7613 - precision_2: 0.7373 - recall_2: 0.8025 - f1_score: 6.3078e-05
Epoch 17/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4959 - accuracy: 0.7614 - precision_2: 0.7373 - recall_2: 0.8025 - f1_score: 7.6033e-05
Epoch 18/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4954 - accuracy: 0.7615 - precision_2: 0.7373 - recall_2: 0.8020 - f1_score: 7.6758e-05
Epoch 19/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4950 - accuracy: 0.7620 - precision_2: 0.7374 - recall_2: 0.8019 - f1_score: 7.9940e-05
Epoch 20/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4974 - accuracy: 0.7618 - precision_2: 0.7376 - recall_2: 0.8018 - f1_score: 8.3757e-05
Epoch 21/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4949 - accuracy: 0.7622 - precision_2: 0.7380 - recall_2: 0.8017 - f1_score: 8.7769e-05
Epoch 22/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4950 - accuracy: 0.7621 - precision_2: 0.7380 - recall_2: 0.8016 - f1_score: 9.3954e-05
Epoch 23/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4949 - accuracy: 0.7623 - precision_2: 0.7382 - recall_2: 0.8015 - f1_score: 9.8300e-05
Epoch 24/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4951 - accuracy: 0.7621 - precision_2: 0.7383 - recall_2: 0.8011 - f1_score: 1.0740e-04
Epoch 25/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4949 - accuracy: 0.7618 - precision_2: 0.7383 - recall_2: 0.8010 - f1_score: 1.1952e-04
Epoch 26/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4952 - accuracy: 0.7617 - precision_2: 0.7383 - recall_2: 0.8009 - f1_score: 1.2497e-04
Epoch 27/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4952 - accuracy: 0.7621 - precision_2: 0.7384 - recall_2: 0.8008 - f1_score: 1.4134e-04
Epoch 28/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4953 - accuracy: 0.7618 - precision_2: 0.7384 - recall_2: 0.8008 - f1_score: 1.4540e-04
Epoch 29/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4952 - accuracy: 0.7618 - precision_2: 0.7385 - recall_2: 0.8008 - f1_score: 1.4318e-04
Epoch 30/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4949 - accuracy: 0.7619 - precision_2: 0.7385 - recall_2: 0.8008 - f1_score: 1.5246e-04
Epoch 31/50
583242/583242 [==============================] - 17s 29us/step - loss: 0.4955 - accuracy: 0.7620 - precision_2: 0.7385 - recall_2: 0.8008 - f1_score: 1.6316e-04
Epoch 32/50
583242/583242 [==============================] - 18s 32us/step - loss: 0.4948 - accuracy: 0.7621 - precision_2: 0.7385 - recall_2: 0.8010 - f1_score: 1.7476e-04
Epoch 33/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4948 - accuracy: 0.7617 - precision_2: 0.7385 - recall_2: 0.8010 - f1_score: 1.8528e-04
Epoch 34/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4949 - accuracy: 0.7619 - precision_2: 0.7385 - recall_2: 0.8010 - f1_score: 1.9048e-04
Epoch 35/50
583242/583242 [==============================] - 19s 33us/step - loss: 0.4947 - accuracy: 0.7620 - precision_2: 0.7385 - recall_2: 0.8010 - f1_score: 1.9048e-04
Epoch 36/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7619 - precision_2: 0.7386 - recall_2: 0.8010 - f1_score: 1.9448e-04
Epoch 37/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4947 - accuracy: 0.7620 - precision_2: 0.7386 - recall_2: 0.8010 - f1_score: 2.0015e-04
Epoch 38/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4945 - accuracy: 0.7621 - precision_2: 0.7386 - recall_2: 0.8011 - f1_score: 2.0885e-04
Epoch 39/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4948 - accuracy: 0.7618 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.1925e-04
Epoch 40/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4949 - accuracy: 0.7619 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.2494e-04
Epoch 41/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4944 - accuracy: 0.7622 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.3042e-04
Epoch 42/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4943 - accuracy: 0.7621 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.3956e-04
Epoch 43/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4944 - accuracy: 0.7618 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.5364e-04
Epoch 44/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4942 - accuracy: 0.7621 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.6539e-04
Epoch 45/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4945 - accuracy: 0.7628 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.7714e-04
Epoch 46/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4947 - accuracy: 0.7620 - precision_2: 0.7386 - recall_2: 0.8015 - f1_score: 2.9084e-04
Epoch 47/50
583242/583242 [==============================] - 21s 35us/step - loss: 0.4944 - accuracy: 0.7625 - precision_2: 0.7386 - recall_2: 0.8017 - f1_score: 2.9430e-04
Epoch 48/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4944 - accuracy: 0.7620 - precision_2: 0.7386 - recall_2: 0.8017 - f1_score: 2.9722e-04
Epoch 49/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4946 - accuracy: 0.7619 - precision_2: 0.7386 - recall_2: 0.8017 - f1_score: 2.9781e-04
Epoch 50/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4943 - accuracy: 0.7621 - precision_2: 0.7386 - recall_2: 0.8018 - f1_score: 3.0532e-04
history.history:
{'loss': [0.5062854379807635, 0.49957913032487883, 0.4991515142650061, 0.4984985389616794, 0.49665870151461616, 0.49751140181720954, 0.4965790819017196, 0.4966554620894168, 0.4969575146858787, 0.49649273162356244, 0.49582554307056254, 0.4958302312638301, 0.49851937516134165, 0.49687231392918474, 0.49602132611732863, 0.4962675041281594, 0.4958642208004031, 0.4953651888545448, 0.49499392978997175, 0.49740579944588886, 0.49491147648607925, 0.49496309725924087, 0.4948639367600915, 0.49507696612664864, 0.49486921998500655, 0.49516355452047783, 0.49519589420329996, 0.4952846807275226, 0.49516040262215705, 0.4949369332097171, 0.49548072055748194, 0.49483191279807803, 0.4947847715104624, 0.49487165123381943, 0.4947343895065108, 0.49457984380756287, 0.49474591095459863, 0.494468287364955, 0.4948387921744963, 0.49491145379819124, 0.494386490067795, 0.4943407087559856, 0.49444982289837086, 0.49418815924361376, 0.4944831602711669, 0.49465160021697435, 0.4943700053285036, 0.4944436023508109, 0.4946493080672291, 0.49425831055114744], 'accuracy': [0.75631213, 0.7597961, 0.7598835, 0.7607854, 0.76108545, 0.7611095, 0.7607408, 0.76107174, 0.7611026, 0.76119864, 0.7611249, 0.7614541, 0.761214, 0.7616324, 0.7616153, 0.7612535, 0.7613975, 0.76149696, 0.76202846, 0.7618347, 0.7621793, 0.76207477, 0.76230794, 0.7621176, 0.7617987, 0.7617181, 0.76213133, 0.761785, 0.7618141, 0.76187587, 0.76199585, 0.7620987, 0.7617267, 0.761881, 0.7620233, 0.7619256, 0.76200104, 0.7621262, 0.761761, 0.76191527, 0.76215017, 0.76210046, 0.76180387, 0.7621159, 0.7628206, 0.7620216, 0.76246226, 0.76202846, 0.7618501, 0.7620919], 'precision_2': [0.72930133, 0.73285985, 0.7330512, 0.7342748, 0.7349641, 0.7359105, 0.73620135, 0.7363241, 0.7363305, 0.73640066, 0.7363835, 0.7363278, 0.73656154, 0.73700505, 0.73724246, 0.73728585, 0.73728925, 0.7373048, 0.737396, 0.7376115, 0.7379839, 0.73804325, 0.7382202, 0.73827523, 0.73828477, 0.73833674, 0.73835117, 0.7384198, 0.73850775, 0.73852116, 0.73852223, 0.73850846, 0.7385089, 0.7385202, 0.7385227, 0.73856026, 0.7385599, 0.73856026, 0.73856384, 0.7385624, 0.7385634, 0.73856413, 0.73856074, 0.73856235, 0.73857373, 0.7385748, 0.7385736, 0.73857456, 0.7385644, 0.7385606], 'recall_2': [0.78977233, 0.80192864, 0.8045291, 0.8038033, 0.8035981, 0.802693, 0.8020653, 0.8025502, 0.80262506, 0.8026484, 0.80307156, 0.8030699, 0.80293334, 0.8027384, 0.8024834, 0.8024787, 0.8025256, 0.80204505, 0.8018985, 0.8017855, 0.80174404, 0.801552, 0.80149806, 0.80105513, 0.801004, 0.80087316, 0.8008362, 0.8008431, 0.8008251, 0.8007974, 0.8008204, 0.8009996, 0.8010048, 0.80103326, 0.80099976, 0.8010364, 0.80104125, 0.80105454, 0.80147964, 0.80148417, 0.801485, 0.80149686, 0.8015396, 0.8015421, 0.8015218, 0.8015396, 0.80169284, 0.80169344, 0.8017424, 0.80175805], 'f1_score': [2.4239827e-05, 2.6459045e-05, 2.3112541e-05, 2.866378e-05, 2.6034033e-05, 2.3447179e-05, 2.31105e-05, 2.374892e-05, 2.5945554e-05, 2.6991951e-05, 2.8088016e-05, 3.032177e-05, 3.4985896e-05, 4.2035786e-05, 5.516963e-05, 6.307817e-05, 7.603315e-05, 7.675834e-05, 7.994005e-05, 8.3757e-05, 8.776879e-05, 9.3953626e-05, 9.830008e-05, 0.00010740292, 0.000119521676, 0.00012497156, 0.00014133676, 0.00014539786, 0.00014318079, 0.0001524595, 0.0001631621, 0.00017476363, 0.00018528476, 0.00019048396, 0.00019048004, 0.0001944822, 0.00020015129, 0.00020884593, 0.00021925496, 0.00022493882, 0.00023042387, 0.000239561, 0.00025364186, 0.00026539218, 0.00027713645, 0.00029083723, 0.00029430084, 0.00029722045, 0.00029780914, 0.00030532468]}
144811/144811 [==============================] - 2s 13us/step
results evaluated:
[0.5021530247255873, 0.761896550655365, 0.7386994361877441, 0.8017578721046448, 0.00030924484599381685]
1615/1615 [==============================] - 0s 12us/step
auto results evaluated:
[1.3733529414185799, 0.900928795337677, 0.7386536002159119, 0.8017987012863159, 0.00032080677920021117]
predictions:
[[0.9672102  0.03278982]
 [0.11115789 0.8888421 ]
 [0.9747557  0.02524431]
 ...
 [0.71947896 0.28052106]
 [0.96647406 0.03352591]
 [0.48341706 0.5165829 ]]
pred_y:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 ...
 [0. 1.]
 [1. 0.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_4"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_16 (Dense)             (None, 20)                240       
_________________________________________________________________
dense_17 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_18 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_19 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_20 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_21 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_22 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_23 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_24 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_25 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_26 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_27 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_28 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_29 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_30 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_31 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_32 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_33 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_34 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_35 (Dense)             (None, 20)                420       
_________________________________________________________________
dense_36 (Dense)             (None, 2)                 42        
=================================================================
Total params: 8,262
Trainable params: 8,262
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.18308677e-02 -1.83090326e-01  8.93921614e-01 -1.38253894e-03
  -2.29714010e-01  2.63671230e+00 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-4.79774948e-01 -5.67746610e-02  2.87845387e-01 -1.14050112e-01
  -1.51240296e-02 -3.79260187e-01  1.89914415e+00 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-7.03172897e-01  3.79667924e-01 -1.16205432e-01 -8.11887362e-02
  -6.59481890e-01 -3.79260187e-01 -5.26552974e-01  2.08072789e+00
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [ 1.41854127e-01  1.25501966e-01 -8.23294363e-01  7.37291761e-02
   1.99465951e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01  2.91139377e+00 -3.94941094e-01]
 [ 8.99464562e-01 -1.08025906e-01  3.88858091e-01 -2.26717684e-01
  -4.26078541e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
   1.83447261e+00 -3.43478100e-01 -3.94941094e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.50891381 -0.5332763  -0.82329436  0.0408678   0.62923383 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [ 0.05443754 -0.04748752  0.79290891  0.09720159  0.19946595 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-0.84886721  0.34389524 -0.82329436 -0.06241081  0.62923383  2.6367123
  -0.52655297 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-1.18882061 -0.64300212 -1.95463665  0.13006296  1.27359169 -0.37926019
  -0.52655297 -0.48060105 -0.5451158   2.91139377 -0.39494109]
 [-1.2762372  -0.653427    0.5908835   0.07842366 -0.01512403 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288578, 294664]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71196, 73615]))
Epoch 1/50
583242/583242 [==============================] - 38s 66us/step - loss: 7.2439 - accuracy: 0.6950 - precision_3: 0.6683 - recall_3: 0.7362 - f1_score: 0.0014   
Epoch 2/50
583242/583242 [==============================] - 39s 66us/step - loss: 35.1783 - accuracy: 0.5749 - precision_3: 0.6561 - recall_3: 0.6822 - f1_score: 0.0050
Epoch 3/50
583242/583242 [==============================] - 39s 66us/step - loss: 645.4361 - accuracy: 0.5276 - precision_3: 0.6087 - recall_3: 0.6254 - f1_score: 0.0333
Epoch 4/50
583242/583242 [==============================] - 37s 64us/step - loss: 90.4650 - accuracy: 0.5351 - precision_3: 0.5851 - recall_3: 0.5908 - f1_score: 0.0666
Epoch 5/50
583242/583242 [==============================] - 38s 64us/step - loss: 8018.2491 - accuracy: 0.5252 - precision_3: 0.5721 - recall_3: 0.5732 - f1_score: 0.1169
Epoch 6/50
583242/583242 [==============================] - 38s 64us/step - loss: 478.7985 - accuracy: 0.5423 - precision_3: 0.5643 - recall_3: 0.5683 - f1_score: 0.1378
Epoch 7/50
583242/583242 [==============================] - 38s 64us/step - loss: 1509.5105 - accuracy: 0.5333 - precision_3: 0.5587 - recall_3: 0.5683 - f1_score: 0.1516
Epoch 8/50
583242/583242 [==============================] - 38s 65us/step - loss: 319634.3337 - accuracy: 0.5109 - precision_3: 0.5533 - recall_3: 0.5633 - f1_score: 0.1935
Epoch 9/50
583242/583242 [==============================] - 39s 67us/step - loss: 33787.1754 - accuracy: 0.5149 - precision_3: 0.5475 - recall_3: 0.5567 - f1_score: 0.2318
Epoch 10/50
583242/583242 [==============================] - 37s 64us/step - loss: 408294.2508 - accuracy: 0.5301 - precision_3: 0.5448 - recall_3: 0.5517 - f1_score: 0.2729
Epoch 11/50
583242/583242 [==============================] - 37s 64us/step - loss: 355733.6003 - accuracy: 0.5144 - precision_3: 0.5422 - recall_3: 0.5466 - f1_score: 0.3060
Epoch 12/50
583242/583242 [==============================] - 37s 64us/step - loss: 636140.3487 - accuracy: 0.5159 - precision_3: 0.5398 - recall_3: 0.5412 - f1_score: 0.3305
Epoch 13/50
583242/583242 [==============================] - 37s 64us/step - loss: 51432.5306 - accuracy: 0.5274 - precision_3: 0.5375 - recall_3: 0.5383 - f1_score: 0.3496
Epoch 14/50
583242/583242 [==============================] - 37s 64us/step - loss: 14311.8192 - accuracy: 0.5273 - precision_3: 0.5366 - recall_3: 0.5364 - f1_score: 0.3656
Epoch 15/50
583242/583242 [==============================] - 37s 64us/step - loss: 139113.1925 - accuracy: 0.5098 - precision_3: 0.5351 - recall_3: 0.5344 - f1_score: 0.3778
Epoch 16/50
583242/583242 [==============================] - 38s 65us/step - loss: 49942.6847 - accuracy: 0.5223 - precision_3: 0.5337 - recall_3: 0.5313 - f1_score: 0.3880
Epoch 17/50
583242/583242 [==============================] - 42s 72us/step - loss: 56158.0810 - accuracy: 0.5124 - precision_3: 0.5323 - recall_3: 0.5306 - f1_score: 0.3965
Epoch 18/50
583242/583242 [==============================] - 42s 72us/step - loss: 173590404.1780 - accuracy: 0.5343 - precision_3: 0.5314 - recall_3: 0.5301 - f1_score: 0.4044
Epoch 19/50
583242/583242 [==============================] - 42s 71us/step - loss: 560910.1915 - accuracy: 0.5062 - precision_3: 0.5308 - recall_3: 0.5282 - f1_score: 0.4122
Epoch 20/50
583242/583242 [==============================] - 42s 72us/step - loss: 92991301.5286 - accuracy: 0.5601 - precision_3: 0.5307 - recall_3: 0.5274 - f1_score: 0.4195
Epoch 21/50
583242/583242 [==============================] - 42s 72us/step - loss: 224641547.0080 - accuracy: 0.5199 - precision_3: 0.5310 - recall_3: 0.5256 - f1_score: 0.4263
Epoch 22/50
583242/583242 [==============================] - 42s 72us/step - loss: 2398384.9335 - accuracy: 0.5283 - precision_3: 0.5307 - recall_3: 0.5212 - f1_score: 0.4315
Epoch 23/50
583242/583242 [==============================] - 42s 72us/step - loss: 3953376.3869 - accuracy: 0.5308 - precision_3: 0.5303 - recall_3: 0.5210 - f1_score: 0.4365
Epoch 24/50
583242/583242 [==============================] - 39s 68us/step - loss: 248852.0730 - accuracy: 0.5083 - precision_3: 0.5297 - recall_3: 0.5207 - f1_score: 0.4406
Epoch 25/50
583242/583242 [==============================] - 37s 63us/step - loss: 396471.2549 - accuracy: 0.5274 - precision_3: 0.5290 - recall_3: 0.5193 - f1_score: 0.4440
Epoch 26/50
583242/583242 [==============================] - 37s 63us/step - loss: 230258.9520 - accuracy: 0.5226 - precision_3: 0.5287 - recall_3: 0.5189 - f1_score: 0.4476
Epoch 27/50
583242/583242 [==============================] - 36s 63us/step - loss: 139179.6252 - accuracy: 0.5236 - precision_3: 0.5284 - recall_3: 0.5181 - f1_score: 0.4508
Epoch 28/50
583242/583242 [==============================] - 36s 62us/step - loss: 4428910.0318 - accuracy: 0.5281 - precision_3: 0.5280 - recall_3: 0.5186 - f1_score: 0.4538
Epoch 29/50
583242/583242 [==============================] - 36s 62us/step - loss: 7301750.5850 - accuracy: 0.5248 - precision_3: 0.5279 - recall_3: 0.5186 - f1_score: 0.4568
Epoch 30/50
583242/583242 [==============================] - 36s 62us/step - loss: 46284078.1098 - accuracy: 0.5087 - precision_3: 0.5273 - recall_3: 0.5180 - f1_score: 0.4589
Epoch 31/50
583242/583242 [==============================] - 41s 71us/step - loss: 171546.4774 - accuracy: 0.5191 - precision_3: 0.5266 - recall_3: 0.5168 - f1_score: 0.4608
Epoch 32/50
583242/583242 [==============================] - 45s 77us/step - loss: 5307706.4684 - accuracy: 0.5271 - precision_3: 0.5265 - recall_3: 0.5175 - f1_score: 0.4631
Epoch 33/50
583242/583242 [==============================] - 45s 77us/step - loss: 536779987.3052 - accuracy: 0.5115 - precision_3: 0.5259 - recall_3: 0.5177 - f1_score: 0.4648
Epoch 34/50
583242/583242 [==============================] - 45s 78us/step - loss: 169350.7274 - accuracy: 0.5226 - precision_3: 0.5255 - recall_3: 0.5183 - f1_score: 0.4665
Epoch 35/50
583242/583242 [==============================] - 45s 78us/step - loss: 721152.4599 - accuracy: 0.5158 - precision_3: 0.5253 - recall_3: 0.5191 - f1_score: 0.4683
Epoch 36/50
583242/583242 [==============================] - 45s 78us/step - loss: 19543386.4304 - accuracy: 0.5070 - precision_3: 0.5246 - recall_3: 0.5192 - f1_score: 0.4694
Epoch 37/50
583242/583242 [==============================] - 46s 78us/step - loss: 33490502.4476 - accuracy: 0.5057 - precision_3: 0.5239 - recall_3: 0.5195 - f1_score: 0.4706
Epoch 38/50
583242/583242 [==============================] - 45s 78us/step - loss: 110212.4673 - accuracy: 0.5179 - precision_3: 0.5235 - recall_3: 0.5191 - f1_score: 0.4718
Epoch 39/50
583242/583242 [==============================] - 45s 78us/step - loss: 2718991727.7731 - accuracy: 0.5091 - precision_3: 0.5230 - recall_3: 0.5191 - f1_score: 0.4729
Epoch 40/50
583242/583242 [==============================] - 45s 78us/step - loss: 54111064.4128 - accuracy: 0.5360 - precision_3: 0.5229 - recall_3: 0.5194 - f1_score: 0.4741
Epoch 41/50
583242/583242 [==============================] - 46s 78us/step - loss: 10964253.2240 - accuracy: 0.5246 - precision_3: 0.5229 - recall_3: 0.5190 - f1_score: 0.4756
Epoch 42/50
583242/583242 [==============================] - 45s 78us/step - loss: 216119988.5482 - accuracy: 0.5258 - precision_3: 0.5229 - recall_3: 0.5190 - f1_score: 0.4770
Epoch 43/50
583242/583242 [==============================] - 44s 75us/step - loss: 891146.9613 - accuracy: 0.5030 - precision_3: 0.5226 - recall_3: 0.5189 - f1_score: 0.4779
Epoch 44/50
583242/583242 [==============================] - 42s 72us/step - loss: 531332.0769 - accuracy: 0.5116 - precision_3: 0.5221 - recall_3: 0.5190 - f1_score: 0.4788
Epoch 45/50
583242/583242 [==============================] - 37s 63us/step - loss: 1684308.5545 - accuracy: 0.5044 - precision_3: 0.5216 - recall_3: 0.5194 - f1_score: 0.4793
Epoch 46/50
583242/583242 [==============================] - 42s 73us/step - loss: 627682909.5460 - accuracy: 0.5376 - precision_3: 0.5215 - recall_3: 0.5200 - f1_score: 0.4803
Epoch 47/50
583242/583242 [==============================] - 45s 77us/step - loss: 68210582.5074 - accuracy: 0.5196 - precision_3: 0.5215 - recall_3: 0.5206 - f1_score: 0.4815
Epoch 48/50
583242/583242 [==============================] - 45s 77us/step - loss: 214204.0186 - accuracy: 0.5305 - precision_3: 0.5215 - recall_3: 0.5210 - f1_score: 0.4825
Epoch 49/50
583242/583242 [==============================] - 45s 78us/step - loss: 13658532.3850 - accuracy: 0.5192 - precision_3: 0.5215 - recall_3: 0.5205 - f1_score: 0.4833
Epoch 50/50
583242/583242 [==============================] - 46s 78us/step - loss: 4831245480.3511 - accuracy: 0.5428 - precision_3: 0.5215 - recall_3: 0.5199 - f1_score: 0.4843
history.history:
{'loss': [7.243874482774448, 35.178326722709144, 645.4360819362635, 90.46503608784992, 8018.249065277271, 478.7985253412744, 1509.5105481670173, 319634.3337085587, 33787.17539525017, 408294.25080692035, 355733.60031422914, 636140.3487245855, 51432.53056430048, 14311.819224964695, 139113.19249966185, 49942.68468172626, 56158.08097585451, 173590404.1779784, 560910.191518479, 92991301.52861957, 224641547.00797155, 2398384.9335063477, 3953376.386910872, 248852.07301325936, 396471.25491364015, 230258.95196554466, 139179.62521960872, 4428910.031801376, 7301750.584997825, 46284078.10980314, 171546.47735544844, 5307706.468362324, 536779987.3052286, 169350.72737579432, 721152.4598582971, 19543386.430407725, 33490502.44762412, 110212.46734186029, 2718991727.7730894, 54111064.41281849, 10964253.223963885, 216119988.54820645, 891146.9613421428, 531332.0769188886, 1684308.5545206717, 627682909.5459834, 68210582.5073611, 214204.0185650554, 13658532.384954233, 4831245480.351058], 'accuracy': [0.69504595, 0.5748763, 0.5275546, 0.53507805, 0.52520055, 0.5422792, 0.5332949, 0.5108857, 0.51485145, 0.53012645, 0.5144314, 0.51589394, 0.527402, 0.5273231, 0.50980556, 0.5222515, 0.5123637, 0.53427565, 0.5061895, 0.56010026, 0.5198528, 0.5282884, 0.53082424, 0.5082676, 0.527414, 0.5226321, 0.523606, 0.5280758, 0.5247736, 0.5087099, 0.5191001, 0.52705395, 0.5115064, 0.5226132, 0.5158099, 0.5069817, 0.5056786, 0.5179051, 0.5090854, 0.53599364, 0.5245798, 0.52582806, 0.50302106, 0.5116144, 0.5043841, 0.53761905, 0.5196351, 0.5304848, 0.5191704, 0.5428467], 'precision_3': [0.6683454, 0.6561306, 0.608688, 0.5851064, 0.5721117, 0.56433153, 0.5586946, 0.5533068, 0.5475328, 0.54484916, 0.54224765, 0.5397951, 0.5374531, 0.5366435, 0.5351071, 0.53367674, 0.5323245, 0.53136456, 0.53083014, 0.5306944, 0.5309913, 0.530687, 0.5303162, 0.5297313, 0.5289946, 0.5287141, 0.52841705, 0.5280141, 0.52792954, 0.52733666, 0.52661765, 0.5264924, 0.52594554, 0.5255016, 0.5253291, 0.5245579, 0.5239475, 0.52349854, 0.5229955, 0.52292055, 0.5229105, 0.5229349, 0.5225955, 0.5220944, 0.5216113, 0.52145976, 0.5215262, 0.52146983, 0.5214845, 0.5214586], 'recall_3': [0.7362394, 0.68217105, 0.6254104, 0.59084797, 0.57320213, 0.5682931, 0.56827474, 0.5632752, 0.55674803, 0.551736, 0.5465937, 0.54123414, 0.53832006, 0.536427, 0.5343899, 0.53133655, 0.5305826, 0.5301488, 0.52821636, 0.52742225, 0.52562046, 0.5211995, 0.520953, 0.52066195, 0.51927495, 0.51889503, 0.51808614, 0.5186441, 0.51864123, 0.51804096, 0.5168336, 0.5174676, 0.5176945, 0.51834387, 0.51905054, 0.51921654, 0.51950717, 0.51911193, 0.5190892, 0.5194005, 0.5190072, 0.5189536, 0.51894766, 0.51900214, 0.5194032, 0.52004933, 0.5206087, 0.521024, 0.52048194, 0.51992506], 'f1_score': [0.0014438549, 0.0050003855, 0.033337146, 0.06664761, 0.1169426, 0.13776864, 0.15161264, 0.19350137, 0.23175038, 0.27285114, 0.30597657, 0.33053243, 0.34960368, 0.36564887, 0.37781855, 0.38803303, 0.3964798, 0.40443876, 0.41216537, 0.41951233, 0.426293, 0.43152753, 0.43650293, 0.4406343, 0.44404116, 0.44763136, 0.45084092, 0.45377362, 0.45678458, 0.45890984, 0.46082553, 0.46306086, 0.46482614, 0.46653563, 0.4682829, 0.4694459, 0.47057614, 0.4717735, 0.47290057, 0.47412625, 0.4755914, 0.47701406, 0.47791883, 0.47879353, 0.47934318, 0.48034605, 0.48146793, 0.48248938, 0.48331738, 0.48434168]}
144811/144811 [==============================] - 3s 21us/step
results evaluated:
[1074695.3658104357, 0.5082694292068481, 0.5217286348342896, 0.5187506079673767, 0.48500972986221313]
1615/1615 [==============================] - 0s 18us/step
auto results evaluated:
[1885335.9650154798, 0.900928795337677, 0.521730899810791, 0.5174665451049805, 0.48506560921669006]
predictions:
[[1. 0.]
 [1. 0.]
 [1. 0.]
 ...
 [1. 0.]
 [1. 0.]
 [1. 0.]]
pred_y:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 ...
 [0. 1.]
 [1. 0.]
 [1. 0.]]
/opt/anaconda3/lib/python3.7/site-packages/keras/activations.py:235: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.
  identifier=identifier.__class__.__name__))
Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_37 (Dense)             (None, 40)                480       
_________________________________________________________________
dense_38 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_39 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_40 (Dense)             (None, 40)                1640      
_________________________________________________________________
dense_41 (Dense)             (None, 2)                 82        
=================================================================
Total params: 5,482
Trainable params: 5,482
Non-trainable params: 0
_________________________________________________________________
train_X BEFORE fitting:
[[-7.18308677e-02 -1.83090326e-01  8.93921614e-01 -1.38253894e-03
  -2.29714010e-01  2.63671230e+00 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-4.79774948e-01 -5.67746610e-02  2.87845387e-01 -1.14050112e-01
  -1.51240296e-02 -3.79260187e-01  1.89914415e+00 -4.80601047e-01
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [-7.03172897e-01  3.79667924e-01 -1.16205432e-01 -8.11887362e-02
  -6.59481890e-01 -3.79260187e-01 -5.26552974e-01  2.08072789e+00
  -5.45115797e-01 -3.43478100e-01 -3.94941094e-01]
 [ 1.41854127e-01  1.25501966e-01 -8.23294363e-01  7.37291761e-02
   1.99465951e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
  -5.45115797e-01  2.91139377e+00 -3.94941094e-01]
 [ 8.99464562e-01 -1.08025906e-01  3.88858091e-01 -2.26717684e-01
  -4.26078541e-01 -3.79260187e-01 -5.26552974e-01 -4.80601047e-01
   1.83447261e+00 -3.43478100e-01 -3.94941094e-01]]
train_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
test_X BEFORE fitting:
[[-0.50891381 -0.5332763  -0.82329436  0.0408678   0.62923383 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [ 0.05443754 -0.04748752  0.79290891  0.09720159  0.19946595 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-0.84886721  0.34389524 -0.82329436 -0.06241081  0.62923383  2.6367123
  -0.52655297 -0.48060105 -0.5451158  -0.3434781  -0.39494109]
 [-1.18882061 -0.64300212 -1.95463665  0.13006296  1.27359169 -0.37926019
  -0.52655297 -0.48060105 -0.5451158   2.91139377 -0.39494109]
 [-1.2762372  -0.653427    0.5908835   0.07842366 -0.01512403 -0.37926019
   1.89914415 -0.48060105 -0.5451158  -0.3434781  -0.39494109]]
test_y BEFORE fitting:
[[0. 1.]
 [0. 1.]
 [1. 0.]
 [1. 0.]
 [0. 1.]]
number of unique vals in train_y:
(array([[0., 1.],
       [1., 0.]]), array([288578, 294664]))
number of unique vals in test_y:
(array([[0., 1.],
       [1., 0.]]), array([71196, 73615]))
Epoch 1/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5167 - accuracy: 0.7536 - precision_4: 0.7276 - recall_4: 0.7841 - f1_score: 3.6504e-04
Epoch 2/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5040 - accuracy: 0.7574 - precision_4: 0.7330 - recall_4: 0.7940 - f1_score: 2.4015e-04
Epoch 3/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5037 - accuracy: 0.7576 - precision_4: 0.7330 - recall_4: 0.7966 - f1_score: 1.8876e-04
Epoch 4/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5033 - accuracy: 0.7586 - precision_4: 0.7335 - recall_4: 0.7975 - f1_score: 1.7788e-04
Epoch 5/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5048 - accuracy: 0.7582 - precision_4: 0.7341 - recall_4: 0.7979 - f1_score: 1.6914e-04
Epoch 6/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5002 - accuracy: 0.7590 - precision_4: 0.7344 - recall_4: 0.7978 - f1_score: 1.6531e-04
Epoch 7/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5008 - accuracy: 0.7594 - precision_4: 0.7350 - recall_4: 0.7978 - f1_score: 1.7523e-04
Epoch 8/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5295 - accuracy: 0.7560 - precision_4: 0.7352 - recall_4: 0.7973 - f1_score: 1.9913e-04
Epoch 9/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5000 - accuracy: 0.7594 - precision_4: 0.7353 - recall_4: 0.7969 - f1_score: 2.0931e-04
Epoch 10/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4997 - accuracy: 0.7601 - precision_4: 0.7354 - recall_4: 0.7971 - f1_score: 2.3285e-04
Epoch 11/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5010 - accuracy: 0.7593 - precision_4: 0.7360 - recall_4: 0.7970 - f1_score: 2.3062e-04
Epoch 12/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5013 - accuracy: 0.7596 - precision_4: 0.7361 - recall_4: 0.7969 - f1_score: 2.2372e-04
Epoch 13/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4990 - accuracy: 0.7600 - precision_4: 0.7362 - recall_4: 0.7971 - f1_score: 2.1871e-04
Epoch 14/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5019 - accuracy: 0.7589 - precision_4: 0.7363 - recall_4: 0.7976 - f1_score: 2.2898e-04
Epoch 15/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4996 - accuracy: 0.7600 - precision_4: 0.7361 - recall_4: 0.7978 - f1_score: 2.2518e-04
Epoch 16/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4989 - accuracy: 0.7601 - precision_4: 0.7363 - recall_4: 0.7979 - f1_score: 2.2473e-04
Epoch 17/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4999 - accuracy: 0.7600 - precision_4: 0.7363 - recall_4: 0.7979 - f1_score: 2.3420e-04
Epoch 18/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4988 - accuracy: 0.7603 - precision_4: 0.7363 - recall_4: 0.7979 - f1_score: 2.4178e-04
Epoch 19/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.5024 - accuracy: 0.7593 - precision_4: 0.7364 - recall_4: 0.7979 - f1_score: 2.3952e-04
Epoch 20/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4979 - accuracy: 0.7602 - precision_4: 0.7364 - recall_4: 0.7979 - f1_score: 2.3851e-04
Epoch 21/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4985 - accuracy: 0.7604 - precision_4: 0.7366 - recall_4: 0.7979 - f1_score: 2.4007e-04
Epoch 22/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4987 - accuracy: 0.7602 - precision_4: 0.7366 - recall_4: 0.7979 - f1_score: 2.4013e-04
Epoch 23/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4996 - accuracy: 0.7600 - precision_4: 0.7366 - recall_4: 0.7980 - f1_score: 2.3884e-04
Epoch 24/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4991 - accuracy: 0.7599 - precision_4: 0.7366 - recall_4: 0.7985 - f1_score: 2.4839e-04
Epoch 25/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4992 - accuracy: 0.7602 - precision_4: 0.7366 - recall_4: 0.7985 - f1_score: 2.5784e-04
Epoch 26/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5001 - accuracy: 0.7598 - precision_4: 0.7367 - recall_4: 0.7983 - f1_score: 2.6444e-04
Epoch 27/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4989 - accuracy: 0.7601 - precision_4: 0.7370 - recall_4: 0.7981 - f1_score: 2.6529e-04
Epoch 28/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.4987 - accuracy: 0.7601 - precision_4: 0.7370 - recall_4: 0.7981 - f1_score: 2.6083e-04
Epoch 29/50
583242/583242 [==============================] - 17s 28us/step - loss: 0.4986 - accuracy: 0.7606 - precision_4: 0.7371 - recall_4: 0.7981 - f1_score: 2.6309e-04
Epoch 30/50
583242/583242 [==============================] - 17s 28us/step - loss: 0.4980 - accuracy: 0.7606 - precision_4: 0.7371 - recall_4: 0.7981 - f1_score: 2.6375e-04
Epoch 31/50
583242/583242 [==============================] - 17s 30us/step - loss: 0.4990 - accuracy: 0.7599 - precision_4: 0.7372 - recall_4: 0.7981 - f1_score: 2.6651e-04
Epoch 32/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4977 - accuracy: 0.7603 - precision_4: 0.7372 - recall_4: 0.7981 - f1_score: 2.6842e-04
Epoch 33/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4981 - accuracy: 0.7605 - precision_4: 0.7373 - recall_4: 0.7981 - f1_score: 2.7099e-04
Epoch 34/50
583242/583242 [==============================] - 20s 35us/step - loss: 0.4992 - accuracy: 0.7602 - precision_4: 0.7373 - recall_4: 0.7981 - f1_score: 2.7086e-04
Epoch 35/50
583242/583242 [==============================] - 20s 34us/step - loss: 0.4976 - accuracy: 0.7605 - precision_4: 0.7373 - recall_4: 0.7981 - f1_score: 2.7111e-04
Epoch 36/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4984 - accuracy: 0.7607 - precision_4: 0.7373 - recall_4: 0.7981 - f1_score: 2.8048e-04
Epoch 37/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5010 - accuracy: 0.7601 - precision_4: 0.7373 - recall_4: 0.7985 - f1_score: 2.8366e-04
Epoch 38/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4987 - accuracy: 0.7606 - precision_4: 0.7373 - recall_4: 0.7986 - f1_score: 2.8211e-04
Epoch 39/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4979 - accuracy: 0.7607 - precision_4: 0.7373 - recall_4: 0.7986 - f1_score: 2.7867e-04
Epoch 40/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4979 - accuracy: 0.7609 - precision_4: 0.7373 - recall_4: 0.7986 - f1_score: 2.8215e-04
Epoch 41/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4986 - accuracy: 0.7607 - precision_4: 0.7373 - recall_4: 0.7986 - f1_score: 2.8032e-04
Epoch 42/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4978 - accuracy: 0.7605 - precision_4: 0.7374 - recall_4: 0.7986 - f1_score: 2.7690e-04
Epoch 43/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4978 - accuracy: 0.7604 - precision_4: 0.7374 - recall_4: 0.7986 - f1_score: 2.7468e-04
Epoch 44/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4989 - accuracy: 0.7600 - precision_4: 0.7374 - recall_4: 0.7986 - f1_score: 2.7704e-04
Epoch 45/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4982 - accuracy: 0.7606 - precision_4: 0.7375 - recall_4: 0.7985 - f1_score: 2.7820e-04
Epoch 46/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.5009 - accuracy: 0.7591 - precision_4: 0.7375 - recall_4: 0.7981 - f1_score: 2.8604e-04
Epoch 47/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4985 - accuracy: 0.7599 - precision_4: 0.7375 - recall_4: 0.7981 - f1_score: 2.8440e-04
Epoch 48/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4984 - accuracy: 0.7607 - precision_4: 0.7375 - recall_4: 0.7981 - f1_score: 2.8295e-04
Epoch 49/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4987 - accuracy: 0.7605 - precision_4: 0.7376 - recall_4: 0.7981 - f1_score: 2.8087e-04
Epoch 50/50
583242/583242 [==============================] - 21s 36us/step - loss: 0.4982 - accuracy: 0.7606 - precision_4: 0.7376 - recall_4: 0.7981 - f1_score: 2.8271e-04
history.history:
{'loss': [0.5167249736984115, 0.5040115002387537, 0.5037256389071785, 0.503289496110091, 0.5048304934241924, 0.5002012473807196, 0.5007863031392051, 0.5295429200843647, 0.5000135045077679, 0.49968965648954833, 0.5010146570022718, 0.5012589712582646, 0.49901566958206767, 0.5019144774333337, 0.49959959818260974, 0.4988955301749537, 0.4998976910009354, 0.4987539713721871, 0.5024346084217233, 0.4978974847181617, 0.4984973178082978, 0.49871308655370944, 0.4995810750070212, 0.4991090669784221, 0.49924638719940945, 0.5001456828658205, 0.4989218797619488, 0.49873931486287804, 0.49863051562619104, 0.49797770946022357, 0.4989717989969598, 0.4976915079995944, 0.4980505057410636, 0.49923425558463874, 0.4976115961547847, 0.4984111037098947, 0.5010474108849591, 0.49870415312615135, 0.4979425949622948, 0.4978644011493588, 0.4986047264352922, 0.4978104250200328, 0.4978254931918998, 0.4989379230963485, 0.49819018801640785, 0.5009341274056328, 0.4984915288990203, 0.4983853478436869, 0.49865475708024487, 0.49820977342541173], 'accuracy': [0.75362027, 0.7574403, 0.75757575, 0.75858736, 0.75822383, 0.7589937, 0.75943947, 0.7560498, 0.75936574, 0.7601373, 0.7593469, 0.7595681, 0.76001215, 0.7589131, 0.7600464, 0.760139, 0.759959, 0.7602573, 0.7592886, 0.7601853, 0.76041675, 0.7602093, 0.75998986, 0.759851, 0.76016814, 0.75981325, 0.7601356, 0.7601236, 0.7605779, 0.7606328, 0.75991786, 0.76029164, 0.76047504, 0.76023334, 0.7604922, 0.76069623, 0.76007044, 0.76057625, 0.7606534, 0.7609071, 0.7607083, 0.76054364, 0.7604288, 0.7600224, 0.76061225, 0.7590811, 0.759911, 0.76068425, 0.7605419, 0.7606174], 'precision_4': [0.72763526, 0.73300976, 0.7329787, 0.7335052, 0.73407215, 0.7343533, 0.7350305, 0.7351994, 0.7353044, 0.73535883, 0.7359932, 0.73611844, 0.73620707, 0.7362585, 0.73614836, 0.73626375, 0.7363128, 0.7363283, 0.7363885, 0.7363881, 0.7365527, 0.7366073, 0.7366184, 0.736612, 0.73657054, 0.7367426, 0.73700684, 0.73702204, 0.737063, 0.73709697, 0.737231, 0.7372441, 0.7372861, 0.7373003, 0.73730516, 0.7373046, 0.73730475, 0.73730385, 0.73730904, 0.7373186, 0.7373222, 0.7373631, 0.7373737, 0.7373748, 0.7375275, 0.7375289, 0.7375419, 0.7375452, 0.73758376, 0.7375858], 'recall_4': [0.7841064, 0.79402506, 0.7965939, 0.7974885, 0.79787904, 0.79783434, 0.7978414, 0.79732764, 0.7968943, 0.7971025, 0.7970372, 0.79689497, 0.79706246, 0.7975772, 0.7978341, 0.7978522, 0.7979035, 0.79790634, 0.7979031, 0.79791284, 0.7978687, 0.79787195, 0.797958, 0.7984925, 0.79853064, 0.7982934, 0.79812735, 0.7981104, 0.7981136, 0.79809356, 0.79811364, 0.7981125, 0.79807276, 0.79806143, 0.7980558, 0.79810935, 0.7985497, 0.7985575, 0.7985564, 0.79856664, 0.79856706, 0.79856026, 0.7985573, 0.7985672, 0.79853165, 0.7981265, 0.798074, 0.7980712, 0.79806185, 0.79807067], 'f1_score': [0.00036503878, 0.00024014858, 0.00018876432, 0.00017787825, 0.00016914305, 0.0001653088, 0.00017523035, 0.00019913013, 0.00020930845, 0.000232848, 0.00023061798, 0.00022372026, 0.00021871403, 0.00022897677, 0.00022518272, 0.00022472652, 0.00023420279, 0.00024177878, 0.0002395168, 0.00023850954, 0.00024006986, 0.00024012779, 0.00023883543, 0.00024839374, 0.00025784053, 0.0002644353, 0.00026528715, 0.00026083083, 0.00026308928, 0.0002637502, 0.00026651478, 0.0002684236, 0.0002709915, 0.00027085893, 0.0002711097, 0.00028047874, 0.00028366156, 0.0002821128, 0.0002786689, 0.00028215474, 0.00028031674, 0.00027689995, 0.00027468352, 0.00027703954, 0.00027819723, 0.00028604115, 0.00028440097, 0.00028294866, 0.00028087015, 0.0002827073]}
144811/144811 [==============================] - 2s 17us/step
results evaluated:
[0.49393393718369244, 0.7608883380889893, 0.737628698348999, 0.7982662916183472, 0.00028319808188825846]
1615/1615 [==============================] - 0s 14us/step
auto results evaluated:
[36.31555268609487, 0.900928795337677, 0.7376629114151001, 0.7982838749885559, 0.00033311848528683186]
predictions:
[[0.90515316 0.09484686]
 [0.20776796 0.792232  ]
 [0.93238187 0.06761818]
 ...
 [0.7404154  0.25958464]
 [0.90921575 0.09078424]
 [0.47695598 0.523044  ]]
pred_y:
[[1. 0.]
 [0. 1.]
 [1. 0.]
 ...
 [0. 1.]
 [1. 0.]
 [1. 0.]]
